{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to EcoLogits","text":"<p>EcoLogits tracks the energy consumption and environmental impacts of using generative AI models through APIs. It supports major LLM providers such as OpenAI, Anthropic, Mistral AI and more (see supported providers).</p>"},{"location":"#requirements","title":"Requirements","text":"<p>Python 3.9+</p> <p>EcoLogits relies on key libraries to provide essential functionalities:</p> <ul> <li>Pydantic  for data modeling.</li> <li>Wrapt  for function patching.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Select providers</p> <p> Anthropic Cohere Google Gemini Hugging Face Inference Endpoints LiteLLM Mistral AI OpenAI </p> <p>Run this command</p> <p>For detailed instructions on each provider, refer to the complete list of supported providers and features. It is also possible to install EcoLogits without any provider.</p>"},{"location":"#usage-example","title":"Usage Example","text":"<p>Below is a simple example demonstrating how to use the GPT-3.5-Turbo model from OpenAI with EcoLogits to track environmental impacts.</p> <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(f\"Energy consumption: {response.impacts.energy.value} kWh\")\nprint(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\")\n</code></pre> <p>Environmental impacts are quantified based on four criteria and across two phases:</p> <p>Criteria:</p> <ul> <li>Energy (energy): Final energy consumption in kWh,</li> <li>Global Warming Potential (gwp): Potential impact on global warming in kgCO2eq (commonly known as GHG/carbon emissions),</li> <li>Abiotic Depletion Potential for Elements (adpe): Impact on the depletion of non-living resources such as minerals or metals in kgSbeq,</li> <li>Primary Energy (pe): Total energy consumed from primary sources in MJ.</li> </ul> <p>Phases:</p> <ul> <li>Usage (usage): Represents the phase of energy consumption during model execution,</li> <li>Embodied (embodied): Encompasses resource extraction, manufacturing, and transportation phases associated with the model's lifecycle.</li> </ul> <p>Learn more about environmental impacts assessment in the methodology section.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Mozilla Public License Version 2.0 (MPL-2.0) .</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>EcoLogits is actively developed and maintained by GenAI Impact  non-profit. We extend our gratitude to Data For Good  and Boavizta  for supporting the development of this project. Their contributions of tools, best practices, and expertise in environmental impact assessment have been invaluable.</p>"},{"location":"contributing/","title":"Contribution","text":"<p>Help us improve EcoLogits by contributing! </p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Questions, feature requests and bug reports are all welcome as discussions or issues.</p> <p>When submitting a feature request or bug report, please provide as much detail as possible. For bug reports, please include relevant information about your environment, including the version of EcoLogits and other Python dependencies used in your project.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Getting started and creating a Pull Request is a straightforward process. Since EcoLogits is regularly updated, you can expect to see your contributions incorporated into the project within a matter of days or weeks.</p> <p>For non-trivial changes, please create an issue to discuss your proposal before submitting pull request. This ensures we can review and refine your idea before implementation.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You'll need to meet the following requirements:</p> <ul> <li>Python version above 3.9</li> <li>git</li> <li>make</li> <li>poetry</li> <li>pre-commit</li> </ul>"},{"location":"contributing/#installation-and-setup","title":"Installation and setup","text":"<p>Fork the repository on GitHub and clone your fork locally.</p> <pre><code># Clone your fork and cd into the repo directory\ngit clone git@github.com:&lt;your username&gt;/ecologits.git\ncd ecologits\n\n# Install ecologits development dependencies with poetry\nmake install\n</code></pre>"},{"location":"contributing/#check-out-a-new-branch-and-make-your-changes","title":"Check out a new branch and make your changes","text":"<p>Create a new branch for your changes.</p> <pre><code># Checkout a new branch and make your changes\ngit checkout -b my-new-feature-branch\n# Make your changes and implements tests...\n</code></pre>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<p>Run tests locally to make sure everything is working as expected.</p> <pre><code>make test\n</code></pre> <p>If you have added a new provider you will need to record your tests with VCR.py through pytest-recording.</p> <pre><code>make test-record\n</code></pre> <p>Once your tests are recorded, please check that the newly created cassette files (located in <code>tests/cassettes/...</code>) do not contain any sensible information like API tokens. If so you will need to update the configuration accordingly in <code>conftest.py</code> and run again the command to record tests.</p>"},{"location":"contributing/#build-documentation","title":"Build documentation","text":"<p>If you've made any changes to the documentation (including changes to function signatures, class definitions, or docstrings that will appear in the API documentation), make sure it builds successfully.</p> <pre><code># Build documentation\nmake docs\n# If you have changed the documentation, make sure it builds successfully.\n</code></pre> <p>You can also serve the documentation locally.</p> <pre><code># Serve the documentation at localhost:8000\npoetry run mkdocs serve\n</code></pre>"},{"location":"contributing/#code-formatting-and-pre-commit","title":"Code formatting and pre-commit","text":"<p>Before pushing your work, run the pre-commit hook that will check and lint your code.</p> <pre><code># Run all checks before commit\nmake pre-commit\n</code></pre>"},{"location":"contributing/#commit-and-push-your-changes","title":"Commit and push your changes","text":"<p>Commit your changes, push your branch to GitHub, and create a pull request.</p> <p>Please follow the pull request template and fill in as much information as possible. Link to any relevant issues and include a description of your changes.</p> <p>When your pull request is ready for review, add a comment with the message \"please review\" and we'll take a look as soon as we can.</p>"},{"location":"contributing/#documentation-style","title":"Documentation style","text":"<p>Documentation is written in Markdown and built using Material for MkDocs. API documentation is build from docstrings using mkdocstrings.</p>"},{"location":"contributing/#code-documentation","title":"Code documentation","text":"<p>When contributing to EcoLogits, please make sure that all code is well documented. The following should be documented using properly formatted docstrings.</p> <p>We use Google-style docstrings formatted according to PEP 257 guidelines. (See Example Google Style Python Docstrings for further examples.)</p>"},{"location":"contributing/#documentation-style_1","title":"Documentation style","text":"<p>Documentation should be written in a clear, concise, and approachable tone, making it easy for readers to understand and follow along. Aim for brevity while still providing complete information.</p> <p>Code examples are highly encouraged, but should be kept short, simple and self-contained. Ensure that each example is complete, runnable, and can be easily executed by readers.</p>"},{"location":"contributing/#acknowledgment","title":"Acknowledgment","text":"<p>We'd like to acknowledge that this contribution guide is heavily inspired by the excellent guide from Pydantic. Thanks for the inspiration! </p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#why-are-training-impacts-not-included","title":"Why are training impacts not included?","text":"<p>Even though the training impacts of generative AI models are substantial, we currently do not implement them in our methodologies and tools. EcoLogits is aimed at estimating the impacts of an API request made to a GenAI service. To make the impact assessment complete, we indeed should take into account training impacts. However, given that we focus on services that are used by millions of people, doing billions of requests annually the training impacts are in fact negligible.</p> <p>For example, looking at Llama 3 70B, the estimated training greenhouse gas emissions are \\(1,900\\ tCO2eq\\). This is significant for an AI model but comparing it to running inference on that model for say 100 billion requests annually makes the share of impacts induced by training the model becomes very small. E.g., \\(\\frac{1,900\\ \\text{tCO2eq}}{100\\ \\text{billion requests}} = 1.9e-8\\ \\text{tCO2eq per request}\\) or \\(0.019\\ \\text{gCO2eq per request}\\). This, compared to running a simple request to Llama 3 70B that would yield \\(1\\ \\text{to}\\ 5\\ \\text{gCO2}\\) (calculated with our methodology).</p> <p>It does not mean that we do not plan to integrate training impacts, it is just not a priority right now due to the difference in order of magnitude. It is also worth mentioning that estimating the number of requests that will be ever made in the lifespan of a model is very difficult, both for open-source and proprietary models. You can join the discussion on GitHub #70 .</p>"},{"location":"faq/#whats-the-difference-with-codecarbon","title":"What's the difference with CodeCarbon?","text":"<p>EcoLogits and CodeCarbon  are two different tools that do not aim to address the same use case. CodeCarbon should be used when you control the execution environment of your model. This means that if you deploy models on your laptop, your server or in the cloud it is preferable to use CodeCarbon to get energy consumption and estimate carbon emissions associated with running your model (including training, fine-tuning or inference).</p> <p>On the other hand EcoLogits is designed for scenarios where you do not have access to the execution environment of your GenAI model because it is managed by a third-party provider.  In such cases you can rely on EcoLogits to estimate energy consumption and environmental impacts for inference workloads. Both tools are complementary and can be used together to provide a comprehensive view of environmental impacts across different deployment scenarios.</p>"},{"location":"faq/#how-can-i-estimate-impacts-of-general-use-of-genai-models","title":"How can I estimate impacts of general use of GenAI models?","text":"<p>If you want to estimate the environmental impacts of using generative AI models without coding or making request, we recommend you to use our online webapp EcoLogits Calculator .</p>"},{"location":"faq/#how-do-we-assess-impacts-for-proprietary-models","title":"How do we assess impacts for proprietary models?","text":"<p>Environmental impacts are calculated based on model architecture and parameter count. For proprietary models, we lack transparency from providers, so we estimate parameter counts using available information. For GPT models, we based our estimates on leaked GPT-4 architecture and scaled parameters count for GPT-4-Turbo and GPT-4o based on pricing differences. For other proprietary models like Anthropic's Claude, we assume similar impacts for models released around the same time with similar performance on public benchmarks. Please note that these estimates are based on assumptions and may not be exact. Our methods are open-source and transparent, so you can always see the hypotheses we use.</p>"},{"location":"faq/#how-to-reduce-my-environmental-impact","title":"How to reduce my environmental impact?","text":"<p>First, you may want to assess indirect impacts  and rebound effects  of the project you are building. Does the finality of your product or service is impacting negatively the environment? Does the usage of your product or service drives up consumption and environmental impacts of previously existing technology?</p> <p>Try to be frugal and question your usages or needs of AI:</p> <ul> <li>Do you really need AI to solve your problem?</li> <li>Do you really need GenAI to solve your problem? (you can read this paper )</li> <li>Prefer fine-tuning of small and existing models over generalist models.</li> <li>Evaluate before, during and after the development of your project the environmental impacts with tools like EcoLogits or CodeCarbon  (see more tools )</li> <li>Restrict the use case and limit the usage of your tool or feature to the desired purpose.</li> </ul> <p>Do not buy new GPUs or hardware. Hardware production for data centers is responsible for around 50% of the impacts compared to usage impacts. The share is even more bigger for consumer devices, around 80%.</p> <p>Use cloud instances that are located in low emissions / high energy efficiency data centers (see electricitymaps.com ).</p> <p>Optimize your models for production use cases. You can look at model compression technics such as quantization, pruning or distillation. There are also inference optimization tricks available in some software.</p>"},{"location":"why/","title":"Why use EcoLogits?","text":"<p>Generative AI significantly impacts our environment, consuming electricity and contributing to global greenhouse gas emissions. In 2020, the ICT sector accounted for 2.1% to 3.9% of global emissions, with projections suggesting an increase to 6%-8% by 2025 due to continued growth and adoption Freitag et al., 2021. The advent of GenAI technologies like ChatGPT has further exacerbated this trend, causing a sharp rise in energy, water, and hardware costs for major tech companies. [0, 1].</p>"},{"location":"why/#which-is-bigger-training-or-inference-impacts","title":"Which is bigger: training or inference impacts?","text":"<p>The field of Green AI focuses on evaluating the environmental impacts of AI models. While many studies have concentrated on training impacts [2], they often overlook other critical phases like data collection, storage and processing phases, research experiments and inference. For GenAI, the inference phase can significantly overshadow training impacts when models are deployed at scale [3]. EcoLogits specifically addresses this gap by focusing on the inference impacts of GenAI.</p>"},{"location":"why/#how-to-assess-impacts-properly","title":"How to assess impacts properly?","text":"<p>EcoLogits employs state-of-the-art methodologies based on Life Cycle Assessment and open data to assess environmental impacts across multiple phases and criteria. This includes usage impacts from electricity consumption and embodied impacts from the production and transportation of hardware. Our multi-criteria approach also evaluates carbon emissions, abiotic resource depletion, and primary energy consumption, providing a comprehensive view that informs decisions like model selection, hardware upgrades and cloud deployments.</p>"},{"location":"why/#how-difficult-is-it","title":"How difficult is it?","text":"<p>Assessing environmental impacts can be challenging with external providers due to lack of control over the execution environment. Meaning you can easily estimate usage impact regarding energy consumption with CodeCarbon and also embodied impacts with BoaviztAPI, but these tools become less relevant with external service providers. EcoLogits simplifies this by basing calculations on well-founded assumptions about hardware, model size, and operational practices, making it easier to estimate impacts accurately. For more details, see our methodology section.</p>"},{"location":"why/#easy-to-use","title":"Easy to use","text":"<p>EcoLogits integrates seamlessly into existing GenAI providers, allowing you to assess the environmental impact of each API request with minimal code adjustments:</p> <pre><code>from ecologits import EcoLogits\n\nEcoLogits.init()    \n\n# Then, you can make request to any supported provider.\n</code></pre> <p>See the list of supported providers and more code snippets in the tutorial section.</p>"},{"location":"why/#have-more-questions","title":"Have more questions?","text":"<p>Feel free to ask question in our GitHub discussions forum!</p>"},{"location":"methodology/","title":"Methodology","text":""},{"location":"methodology/#evaluation-methodologies","title":"Evaluation methodologies","text":"<p>The following methodologies are currently available and implemented in EcoLogits:</p> <ul> <li> LLM Inference</li> </ul> <p>Upcoming methodologies (join us to help speed up our progress):</p> <ul> <li> Embeddings</li> <li> Image Generation</li> <li> Multi-Modal</li> </ul>"},{"location":"methodology/#methodological-background","title":"Methodological background","text":"<p>EcoLogits employs the Life Cycle Assessment (LCA) methodology, as defined by ISO 14044, to estimate the environmental impacts of requests made to generative AI inference services. This approach focuses on multiple phases of the lifecycle, specifically raw material extraction, manufacturing, transportation (denoted as embodied impacts), usage and end-of-life. Notably, we do not cover the end-of-life phase due to data limitations on e-waste recycling.</p> <p>Our assessment considers three key environmental criteria:</p> <ul> <li>Global Warming Potential (GWP): Evaluates the impact on global warming in terms of CO2 equivalents.</li> <li>Abiotic Resource Depletion for Elements (ADPe): Assesses the consumption of raw minerals and metals, expressed in antimony equivalents.</li> <li>Primary Energy (PE): Calculates energy consumed from natural sources, expressed in megajoules.</li> </ul> <p>Using a bottom-up modeling approach, we assess and aggregate the environmental impacts of all individual service components. This method differs from top-down approaches by allowing precise allocation of each resource's impact to the overall environmental footprint.</p> <p>Our current focus is on high-performance GPU-accelerated cloud instances, crucial for GenAI inference tasks. While we exclude impacts from training, networking, and end-user devices, we thoroughly evaluate the impacts associated with hosting and running the model inferences.</p> <p>The methodology is grounded in transparency and reproducibility, utilizing open market and technical data to ensure our results are reliable and verifiable.</p>"},{"location":"methodology/#licenses-and-citations","title":"Licenses and citations","text":"<p>All the methodologies are licensed under CC BY-SA 4.0 </p> <p>Please ensure that you adhere to the license terms and properly cite the authors and the GenAI Impact non-profit organization when utilizing this work. Each methodology has an associated paper with specific citation requirements.</p>"},{"location":"methodology/llm_inference/","title":"LLM Inference","text":"<p>Page still under construction</p> <p>This page is still under construction. If you spot any inaccuracies or have questions about the methodology itself, feel free to open an issue on GitHub.</p> <p>Early Publication</p> <p>Beware that this is an early version of the methodology to evaluate the environmental impacts of LLMs at inference. We are still testing and reviewing the methodology internally. Some parts of the methodology may change in the near future.</p>"},{"location":"methodology/llm_inference/#environmental-impacts-of-llm-inference","title":"Environmental Impacts of LLM Inference","text":"Known limitations and hypotheses <ul> <li>Based on a production setup: models are quantized, high-end servers with A100...</li> <li>Current implementation of EcoLogits assumes a fixed and worldwide impact factor for electricity mix.</li> <li>Model architectures are assumed when not dislosed by the provider.</li> <li>Not accounting the impacts of unused cloud resources, data center building, network and end-user devices, model training and data collection...</li> <li>Not tested on multi-modal models for text-to-text generation only.</li> </ul> <p>The environmental impacts of a request, \\(I_{request}\\) to a Large Language Model (LLM) can be divided into two components: the usage impacts, \\(I_{request}^u\\), which account for energy consumption, and the embodied impacts, \\(I_{request}^e\\), which account for resource extraction, hardware manufacturing, and transportation.</p> \\[ \\begin{equation*} \\begin{split} I_{request}&amp;=I_{request}^u  + I_{request}^e \\\\  &amp;= E_{request}*F_{em}+\\frac{\\Delta T}{\\Delta L}*I_{server}^e \\end{split} \\end{equation*} \\] <p>Where \\(E_{request}\\) represents the energy consumption of the IT resources associated with the request. \\(F_{em}\\) denotes the impact factor of electricity consumption, which varies depending on the location and time. Furthermore, \\(I_{server}^e\\) captures the embodied impacts of the IT resources, and \\(\\frac{\\Delta T}{\\Delta L}\\) signifies the hardware utilization factor, calculated as the computation time divided by the lifetime of the hardware.</p>"},{"location":"methodology/llm_inference/#usage-impacts","title":"Usage impacts","text":"<p>To assess the usage impacts of an LLM inference, we first need to estimate the energy consumption of the server, which is equipped with one or more GPUs. We will also take into account the energy consumption of cooling equipment integrated with the data center, using the Power Usage Effectiveness (PUE) metric.</p> <p>Subsequently, we can calculate the environmental impacts by using the \\(F_{em}\\) impact factor of the electricity mix. Ideally, \\(F_{em}\\) should vary with location and time to accurately reflect the local energy mix.</p>"},{"location":"methodology/llm_inference/#modeling-gpu-energy-consumption","title":"Modeling GPU energy consumption","text":"<p>By leveraging the open dataset from the LLM Perf Leaderboard, produced by Hugging Face, we can estimate the energy consumption of the GPU using a parametric model.</p> <p>We fit a linear regression model to the dataset, which models the energy consumption per output token as a function of the number of active parameters in the LLM, denoted as \\(P_{active}\\).</p> What are active parameters? <p>We distinguish between active parameters and total parameter count for Sparse Mixture-of-Experts (SMoE) models. The total parameter count is used to determine the number of required GPUs to load the model into memory. In contrast, the active parameter count is used to estimate the energy consumption of a single GPU. In practice, SMoE models exhibit lower energy consumption per GPU compared to dense models of equivalent size (in terms of total parameters).</p> <ul> <li>For a dense model: \\(P_{active} = P_{total}\\)</li> <li>For a SMoE model: \\(P_{active} =  P_{total} / \\text{number of active experts}\\)</li> </ul> On the LLM Perf Leaderboard dataset filtering <p>We have filtered the dataset to keep relevant data points for the analysis. In particular we have applied the following conditions:</p> <ul> <li>Model number of parameters &gt;= 7B</li> <li>Keep dtype set to float16</li> <li>GPU model is \"NVIDIA A100-SXM4-80GB\"</li> <li>No optimization</li> <li>8bit and 4bit quantization excluding bitsandbytes (bnb)</li> </ul> Figure: Energy consumption (in Wh) per output token vs. number of active parameters (in billions) \\[ \\frac{E_{GPU}}{\\#T_{out}} = \\alpha * P_{active} + \\beta \\] <p>We found that \\(\\alpha = 8.91e-5\\) and \\(\\beta = 1.43e-3\\). Using these values, we can estimate the energy consumption of a simple GPU for the entire request, given the number of output tokens \\(\\#T_{out}\\) and the number of active parameters \\(P_{active}\\):</p> \\[ E_{GPU}(\\#T_{out}, P_{active}) = \\#T_{out} * (\\alpha * P_{active} + \\beta) \\] <p>If the model requires multiple GPUs to be loaded into VRAM, the energy consumption \\(E_{GPU}\\) should be multiplied by the number of GPUs \\(\\#GPU_{required}\\) (see below).</p>"},{"location":"methodology/llm_inference/#modeling-server-energy-consumption","title":"Modeling server energy consumption","text":"<p>To estimate the energy consumption of the entire server, we will use the previously estimated GPU energy model and separately estimate the energy consumption of the server itself (without GPUs), denoted as \\(E_{server\\backslash GPU}\\).</p>"},{"location":"methodology/llm_inference/#server-energy-consumption-without-gpus","title":"Server energy consumption without GPUs","text":"<p>To model the energy consumption of the server without GPUs, we consider a fixed power consumption, \\(W_{server\\backslash GPU}\\), during inference (or generation latency), denoted as \\(\\Delta T\\). We assume that the server hosts multiple GPUs, but not all of them are actively used for the target inference. Therefore, we account for a portion of the energy consumption based on the number of required GPUs, \\(\\#GPU_{required}\\):</p> \\[ E_{server\\backslash GPU}(\\Delta T) = \\Delta T * W_{server\\backslash GPU} * \\frac{\\#GPU_{required}}{\\#GPU_{installed}} \\] <p>For a typical high-end GPU-accelerated cloud instance, we use \\(W_{server\\backslash GPU} = 1\\ kW\\) and \\(\\#GPU_{installed} = 8\\).</p>"},{"location":"methodology/llm_inference/#estimating-the-generation-latency","title":"Estimating the generation latency","text":"<p>The generation latency, \\(\\Delta T\\), is the duration of the inference measured on the server and is independent of networking latency. We estimate the generation latency using the LLM Perf Leaderboard dataset with the previously mentioned filters applied.</p> <p>We fit a linear regression model on the dataset modeling the generation latency per output token given the number of active parameters of the LLM \\(P_{active}\\):</p> Figure: Latency (in s) per output token vs. number of active parameters (in billions) \\[ \\frac{\\Delta T}{\\#T_{out}} = A * P_{active} + B \\] <p>We found \\(A = 8.02e-4\\) and \\(B = 2.23e-2\\). Using these values, we can estimate the generation latency for the entire request given the number of output tokens, \\(\\#T_{out}\\), and the number of active parameters, \\(P_{active}\\). When possible, we also measure the request latency, \\(\\Delta T_{request}\\), and use it as the maximum bound for the generation latency:</p> \\[ \\Delta T(\\#T_{out}, P_{active}) = \\#T_{out} * (A * P_{active} + B) \\] <p>With the request latency, the generation latency is defined as follows:</p> \\[ \\Delta T(\\#T_{out}, P_{active}, \\Delta T_{request}) = \\min[\\#T_{out} * (A * P_{active} + B), \\Delta T_{request}] \\]"},{"location":"methodology/llm_inference/#estimating-the-number-of-active-gpus","title":"Estimating the number of active GPUs","text":"<p>To estimate the number of required GPUs, \\(\\#GPU_{required}\\), to load the model in virtual memory, we divide the required memory to host the LLM for inference, \\(M_{model}\\), by the memory available on one GPU, \\(M_{GPU}\\).</p> <p>The required memory to host the LLM for inference is estimated based on the total number of parameters and the number of bits used for model weights related to quantization. We also apply a memory overhead of \\(1.2\\) (see Transformers Math 101 ):</p> \\[ M_{model}(P_{total},Q)=\\frac{P_{total}*Q}{8}*1.2 \\] <p>We then estimate the number of required GPUs, rounded up:</p> \\[ \\#GPU_{required}(P_{total},Q,M_{GPU}) = \\lceil \\frac{M_{model}(P_{total},Q)}{M_{GPU}}\\rceil \\] <p>To stay consistent with previous assumptions based on LLM Perf Leaderboard data, we use \\(M_{GPU} = 80\\ GB\\) for an NVIDIA A100 80GB GPU.</p>"},{"location":"methodology/llm_inference/#complete-server-energy-consumption","title":"Complete server energy consumption","text":"<p>The total server energy consumption for the request, \\(E_{server}\\), is calculated as follows:</p> \\[ E_{server} = E_{server\\backslash GPU} + \\#GPU_{required} * E_{GPU} \\]"},{"location":"methodology/llm_inference/#modeling-request-energy-consumption","title":"Modeling request energy consumption","text":"<p>To estimate the energy consumption of the request, we multiply the previously computed server energy by the Power Usage Effectiveness (PUE) to account for cooling equipment in the data center:</p> \\[ E_{request} = PUE * E_{server} \\] <p>We typically use a \\(PUE = 1.2\\) for hyperscaler data centers or supercomputers.</p>"},{"location":"methodology/llm_inference/#modeling-request-usage-environmental-impacts","title":"Modeling request usage environmental impacts","text":"<p>To assess the environmental impacts of the request for the usage phase, we multiply the estimated electricity consumption by the impact factor of the electricity mix, \\(F_{em}\\), specific to the target country and time. We currently use a worldwide average multicriteria impact factor from the ADEME Base Empreinte\u00ae:</p> \\[ I^u_{request} = E_{request} * F_{em} \\] Some values of \\(F_{em}\\) per geographical area Area or country GWP (\\(gCO2eq / kWh\\)) ADPe (\\(kgSbeq / kWh\\)) PE (\\(MJ / kWh\\)) \ud83c\udf10 Worldwide \\(590.4\\) \\(7.378 * 10^{-8}\\) \\(9.99\\) \ud83c\uddea\ud83c\uddfa Europe (EEA) \\(509.4\\) \\(6.423 * 10^{-8}\\) \\(12.9\\) \ud83c\uddfa\ud83c\uddf8 USA \\(679.8\\) \\(9.855 * 10^{-8}\\) \\(11.4\\) \ud83c\udde8\ud83c\uddf3 China \\(1,057\\) \\(8.515 * 10^{-8}\\) \\(14.1\\) \ud83c\uddeb\ud83c\uddf7 France \\(81.3\\) \\(4.858 * 10^{-8}\\) \\(11.3\\)"},{"location":"methodology/llm_inference/#embodied-impacts","title":"Embodied impacts","text":"<p>To determine the embodied impacts of an LLM inference, we need to estimate the hardware configuration used to host the model and its lifetime. Embodied impacts account for resource extraction (e.g., minerals and metals), manufacturing, and transportation of the hardware.</p>"},{"location":"methodology/llm_inference/#modeling-server-embodied-impacts","title":"Modeling server embodied impacts","text":"<p>To estimate the embodied impacts of IT hardware, we use the BoaviztAPI tool from the non-profit organization Boavizta. This API embeds a bottom-up multicriteria environment impact estimation engine for embodied and usage phases of IT resources and services. We focus on estimating the embodied impacts of a server and a GPU. BoaviztAPI is an open-source project that relies on open databases and open research on environmental impacts of IT equipment.</p>"},{"location":"methodology/llm_inference/#server-embodied-impacts-without-gpu","title":"Server embodied impacts without GPU","text":"<p>To assess the embodied environmental impacts of a high-end AI server, we use an AWS cloud instance as a reference. We selected the <code>p4de.24xlarge</code> instance, as it corresponds to a server that can be used for LLM inference with eight NVIDIA A100 80GB GPU cards. The embodied impacts of this instance will be used to estimate the embodied impacts of the server without GPUs, denoted as \\(I^e_{server\\backslash GPU}\\).</p> <p>The embodied environmental impacts of the cloud instance are:</p> Server (without GPU) GWP (\\(kgCO2eq\\)) \\(3000\\) ADPe (\\(kgSbeq\\)) \\(0.25\\) PE (\\(MJ\\)) \\(39,000\\) <p>These impacts does not take into account the eight GPUs. (see bellow)</p> Example request to reproduce this calculation <p>On the cloud instance route (/v1/cloud/instance) you can POST the following JSON.</p> <pre><code>{\n    \"provider\": \"aws\",\n    \"instance_type\": \"p4de.24xlarge\"\n}\n</code></pre> <p>Or you can use the demo available demo API with this command using <code>curl</code> and parsing the JSON output with <code>jq</code>.</p> <pre><code>curl -X 'POST' \\\n    'https://api.boavizta.org/v1/cloud/instance?verbose=true&amp;criteria=gwp&amp;criteria=adp&amp;criteria=pe' \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"provider\": \"aws\",\n    \"instance_type\": \"p4de.24xlarge\"\n}' | jq\n</code></pre>"},{"location":"methodology/llm_inference/#gpu-embodied-impacts","title":"GPU embodied impacts","text":"<p>Boavizta is currently developing a methodology to provide multicriteria embodied impacts for GPU cards. For this analysis, we use the embodied impact data they computed for a NVIDIA A100 80GB GPU. These values will be used to estimate the embodied impacts of a single GPU, denoted as \\(I^e_{GPU}\\).</p> NIDIA A100 80GB GWP (\\(kgCO2eq\\)) \\(143\\) ADPe (\\(kgSbeq\\)) \\(5.09 * 10^{-3}\\) PE (\\(MJ\\)) \\(1,828\\) <p>The GPU embodied impacts will be soon available in the BoaviztAPI tool.</p>"},{"location":"methodology/llm_inference/#complete-server-embodied-impacts","title":"Complete server embodied impacts","text":"<p>The final embodied impacts for the server, including the GPUs, are calculated as follows. Note that the embodied impacts of the server without GPUs are scaled by the number of GPUs required to host the model. This allocation is made to account for the fact that the remaining GPUs on the server can be used to host other models or multiple instances of the same model. As we are estimating the impacts of a single LLM inference, we need to exclude the embodied impacts that would be attributed to other services hosted on the same server.</p> \\[ I^e_{server}=\\frac{\\#GPU_{required}}{\\#GPU_{installed}}*I^e_{server\\backslash GPU} + \\#GPU_{required} * I^e_{GPU} \\]"},{"location":"methodology/llm_inference/#modeling-request-embodied-environmental-impacts","title":"Modeling request embodied environmental impacts","text":"<p>To allocate the server embodied impacts to the request, we use an allocation based on the hardware utilization factor, \\(\\frac{\\Delta T}{\\Delta L}\\). In this case, \\(\\Delta L\\) represents the lifetime of the server and GPU, which we fix at 5 years.</p> \\[ I^e_{request}=\\frac{\\Delta T}{\\Delta L} * I^e_{server} \\]"},{"location":"methodology/llm_inference/#conclusion","title":"Conclusion","text":"<p>This paper presents a methodology to assess the environmental impacts of Large Language Model (LLM) inference, considering both usage and embodied impacts. We model server and GPU energy consumption based on various parameters and incorporate PUE and electricity mix impact factors. For embodied impacts, we use the BoaviztAPI tool to estimate environmental impacts of IT hardware. Our methodology offers a comprehensive understanding of the environmental footprint of LLM inference, guiding researchers and practitioners towards more sustainable AI practices. Future work may involve refining the methodology and exploring the impacts of multi-modal models or RAG applications.</p>"},{"location":"methodology/llm_inference/#references","title":"References","text":"<ul> <li>LLM-Perf Leaderboard to estimate GPU energy consumption and latency based on the model architecture and number of output tokens.</li> <li>BoaviztAPI to estimate server embodied impacts and base energy consumption.</li> <li>ADEME Base Empreinte\u00ae for electricity mix impacts per country.</li> </ul>"},{"location":"methodology/llm_inference/#citation","title":"Citation","text":"<p>Please cite GenAI Impact non-profit organization and link to this documentation page. </p> <pre><code>Coming soon...\n</code></pre>"},{"location":"methodology/llm_inference/#license","title":"License","text":"<p>This work is licensed under CC BY-SA 4.0 </p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>_ecologits</li> <li>electricity_mix_repository</li> <li>exceptions</li> <li>impacts<ul> <li>dag</li> <li>llm</li> <li>modeling</li> </ul> </li> <li>log</li> <li>model_repository</li> <li>tracers<ul> <li>anthropic_tracer</li> <li>cohere_tracer</li> <li>google_tracer</li> <li>huggingface_tracer</li> <li>litellm_tracer</li> <li>mistralai_tracer</li> <li>openai_tracer</li> <li>utils</li> </ul> </li> </ul>"},{"location":"reference/_ecologits/","title":"_ecologits","text":""},{"location":"reference/_ecologits/#_ecologits.EcoLogits","title":"<code>EcoLogits</code>","text":"<p>EcoLogits instrumentor to initialize function patching for each provider.</p> <p>By default, the initialization will be done on all available and compatible providers that are supported by the library.</p> <p>Examples:</p> <p>EcoLogits initialization example with OpenAI. <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(f\"Energy consumption: {response.impacts.energy.value} kWh\")\nprint(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\")\n</code></pre></p>"},{"location":"reference/_ecologits/#_ecologits.EcoLogits.init","title":"<code>init(providers=None, electricity_mix_zone='WOR')</code>  <code>staticmethod</code>","text":"<p>Initialization static method. Will attempt to initialize all providers by default.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>Optional[Union[str, list[str]]]</code> <p>list of providers to initialize (all providers by default).</p> <code>None</code> <code>electricity_mix_zone</code> <code>Optional[str]</code> <p>ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).</p> <code>'WOR'</code> Source code in <code>ecologits/_ecologits.py</code> <pre><code>@staticmethod\ndef init(\n    providers: Optional[Union[str, list[str]]] = None,\n    electricity_mix_zone: Optional[str] = \"WOR\",\n) -&gt; None:\n    \"\"\"\n    Initialization static method. Will attempt to initialize all providers by default.\n\n    Args:\n        providers: list of providers to initialize (all providers by default).\n        electricity_mix_zone: ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).\n    \"\"\"\n    if isinstance(providers, str):\n        providers = [providers]\n    if providers is None:\n        providers = list(_INSTRUMENTS.keys())\n\n    init_instruments(providers)\n\n    EcoLogits.config.electricity_mix_zone = electricity_mix_zone\n    EcoLogits.config.providers += providers\n    EcoLogits.config.providers = list(set(EcoLogits.config.providers))\n</code></pre>"},{"location":"reference/electricity_mix_repository/","title":"electricity_mix_repository","text":""},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#exceptions.TracerInitializationError","title":"<code>TracerInitializationError</code>","text":"<p>               Bases: <code>EcoLogitsError</code></p> <p>Tracer is initialized twice</p>"},{"location":"reference/exceptions/#exceptions.ModelingError","title":"<code>ModelingError</code>","text":"<p>               Bases: <code>EcoLogitsError</code></p> <p>Operation or computation not allowed</p>"},{"location":"reference/log/","title":"log","text":""},{"location":"reference/model_repository/","title":"model_repository","text":""},{"location":"reference/impacts/dag/","title":"dag","text":""},{"location":"reference/impacts/llm/","title":"llm","text":""},{"location":"reference/impacts/llm/#impacts.llm.gpu_energy","title":"<code>gpu_energy(model_active_parameter_count, output_token_count, gpu_energy_alpha, gpu_energy_beta)</code>","text":"<p>Compute energy consumption of a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>float</code> <p>Number of active parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>gpu_energy_alpha</code> <code>float</code> <p>Alpha parameter of the GPU linear power consumption profile.</p> required <code>gpu_energy_beta</code> <code>float</code> <p>Beta parameter of the GPU linear power consumption profile.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The energy consumption of a single GPU in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef gpu_energy(\n    model_active_parameter_count: float,\n    output_token_count: float,\n    gpu_energy_alpha: float,\n    gpu_energy_beta: float\n) -&gt; float:\n    \"\"\"\n    Compute energy consumption of a single GPU.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        output_token_count: Number of generated tokens.\n        gpu_energy_alpha: Alpha parameter of the GPU linear power consumption profile.\n        gpu_energy_beta: Beta parameter of the GPU linear power consumption profile.\n\n    Returns:\n        The energy consumption of a single GPU in kWh.\n    \"\"\"\n    return output_token_count * (gpu_energy_alpha * model_active_parameter_count + gpu_energy_beta)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.generation_latency","title":"<code>generation_latency(model_active_parameter_count, output_token_count, gpu_latency_alpha, gpu_latency_beta, request_latency)</code>","text":"<p>Compute the token generation latency in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>float</code> <p>Number of active parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>gpu_latency_alpha</code> <code>float</code> <p>Alpha parameter of the GPU linear latency profile.</p> required <code>gpu_latency_beta</code> <code>float</code> <p>Beta parameter of the GPU linear latency profile.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency (upper bound) in seconds.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The token generation latency in seconds.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef generation_latency(\n    model_active_parameter_count: float,\n    output_token_count: float,\n    gpu_latency_alpha: float,\n    gpu_latency_beta: float,\n    request_latency: float,\n) -&gt; float:\n    \"\"\"\n    Compute the token generation latency in seconds.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        output_token_count: Number of generated tokens.\n        gpu_latency_alpha: Alpha parameter of the GPU linear latency profile.\n        gpu_latency_beta: Beta parameter of the GPU linear latency profile.\n        request_latency: Measured request latency (upper bound) in seconds.\n\n    Returns:\n        The token generation latency in seconds.\n    \"\"\"\n    gpu_latency = output_token_count * (gpu_latency_alpha * model_active_parameter_count + gpu_latency_beta)\n    return min(gpu_latency, request_latency)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.model_required_memory","title":"<code>model_required_memory(model_total_parameter_count, model_quantization_bits)</code>","text":"<p>Compute the required memory to load the model on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model_total_parameter_count</code> <code>float</code> <p>Number of parameters of the model.</p> required <code>model_quantization_bits</code> <code>int</code> <p>Number of bits used to represent the model weights.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The amount of required GPU memory to load the model.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef model_required_memory(\n    model_total_parameter_count: float,\n    model_quantization_bits: int,\n) -&gt; float:\n    \"\"\"\n    Compute the required memory to load the model on GPU.\n\n    Args:\n        model_total_parameter_count: Number of parameters of the model.\n        model_quantization_bits: Number of bits used to represent the model weights.\n\n    Returns:\n        The amount of required GPU memory to load the model.\n    \"\"\"\n    return 1.2 * model_total_parameter_count * model_quantization_bits / 8\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.gpu_required_count","title":"<code>gpu_required_count(model_required_memory, gpu_memory)</code>","text":"<p>Compute the number of required GPU to store the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_required_memory</code> <code>float</code> <p>Required memory to load the model on GPU.</p> required <code>gpu_memory</code> <code>float</code> <p>Amount of memory available on a single GPU.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of required GPUs to load the model.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef gpu_required_count(\n    model_required_memory: float,\n    gpu_memory: float\n) -&gt; int:\n    \"\"\"\n    Compute the number of required GPU to store the model.\n\n    Args:\n        model_required_memory: Required memory to load the model on GPU.\n        gpu_memory: Amount of memory available on a single GPU.\n\n    Returns:\n        The number of required GPUs to load the model.\n    \"\"\"\n    return ceil(model_required_memory / gpu_memory)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_energy","title":"<code>server_energy(generation_latency, server_power, server_gpu_count, gpu_required_count)</code>","text":"<p>Compute the energy consumption of the server.</p> <p>Parameters:</p> Name Type Description Default <code>generation_latency</code> <code>float</code> <p>Token generation latency in seconds.</p> required <code>server_power</code> <code>float</code> <p>Power consumption of the server in kW.</p> required <code>server_gpu_count</code> <code>int</code> <p>Number of available GPUs in the server.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The energy consumption of the server (GPUs are not included) in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_energy(\n    generation_latency: float,\n    server_power: float,\n    server_gpu_count: int,\n    gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the energy consumption of the server.\n\n    Args:\n        generation_latency: Token generation latency in seconds.\n        server_power: Power consumption of the server in kW.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The energy consumption of the server (GPUs are not included) in kWh.\n    \"\"\"\n    return (generation_latency / 3600) * server_power * (gpu_required_count / server_gpu_count)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_energy","title":"<code>request_energy(datacenter_pue, server_energy, gpu_required_count, gpu_energy)</code>","text":"<p>Compute the energy consumption of the request.</p> <p>Parameters:</p> Name Type Description Default <code>datacenter_pue</code> <code>float</code> <p>PUE of the datacenter.</p> required <code>server_energy</code> <code>float</code> <p>Energy consumption of the server in kWh.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <code>gpu_energy</code> <code>float</code> <p>Energy consumption of a single GPU in kWh.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The energy consumption of the request in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_energy(\n    datacenter_pue: float,\n    server_energy: float,\n    gpu_required_count: int,\n    gpu_energy: float\n) -&gt; float:\n    \"\"\"\n    Compute the energy consumption of the request.\n\n    Args:\n        datacenter_pue: PUE of the datacenter.\n        server_energy: Energy consumption of the server in kWh.\n        gpu_required_count: Number of required GPUs to load the model.\n        gpu_energy: Energy consumption of a single GPU in kWh.\n\n    Returns:\n        The energy consumption of the request in kWh.\n    \"\"\"\n    return datacenter_pue * (server_energy + gpu_required_count * gpu_energy)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_gwp","title":"<code>request_usage_gwp(request_energy, if_electricity_mix_gwp)</code>","text":"<p>Compute the Global Warming Potential (GWP) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>float</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The GWP usage impact of the request in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_gwp(\n    request_energy: float,\n    if_electricity_mix_gwp: float\n) -&gt; float:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n\n    Returns:\n        The GWP usage impact of the request in kgCO2eq.\n    \"\"\"\n    return request_energy * if_electricity_mix_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_adpe","title":"<code>request_usage_adpe(request_energy, if_electricity_mix_adpe)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>float</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption in kgSbeq / kWh.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The ADPe usage impact of the request in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_adpe(\n    request_energy: float,\n    if_electricity_mix_adpe: float\n) -&gt; float:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption in kgSbeq / kWh.\n\n    Returns:\n        The ADPe usage impact of the request in kgSbeq.\n    \"\"\"\n    return request_energy * if_electricity_mix_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_pe","title":"<code>request_usage_pe(request_energy, if_electricity_mix_pe)</code>","text":"<p>Compute the Primary Energy (PE) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>float</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The PE usage impact of the request in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_pe(\n    request_energy: float,\n    if_electricity_mix_pe: float\n) -&gt; float:\n    \"\"\"\n    Compute the Primary Energy (PE) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n\n    Returns:\n        The PE usage impact of the request in MJ.\n    \"\"\"\n    return request_energy * if_electricity_mix_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_gwp","title":"<code>server_gpu_embodied_gwp(server_embodied_gwp, server_gpu_count, gpu_embodied_gwp, gpu_required_count)</code>","text":"<p>Compute the Global Warming Potential (GWP) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of the server in kgCO2eq.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of a single GPU in kgCO2eq.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The GWP embodied impact of the server and the GPUs in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_gwp(\n    server_embodied_gwp: float,\n    server_gpu_count: float,\n    gpu_embodied_gwp: float,\n    gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) embodied impact of the server\n\n    Args:\n        server_embodied_gwp: GWP embodied impact of the server in kgCO2eq.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_gwp: GWP embodied impact of a single GPU in kgCO2eq.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The GWP embodied impact of the server and the GPUs in kgCO2eq.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_gwp + gpu_required_count * gpu_embodied_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_adpe","title":"<code>server_gpu_embodied_adpe(server_embodied_adpe, server_gpu_count, gpu_embodied_adpe, gpu_required_count)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of the server in kgSbeq.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of a single GPU in kgSbeq.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The ADPe embodied impact of the server and the GPUs in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_adpe(\n    server_embodied_adpe: float,\n    server_gpu_count: float,\n    gpu_embodied_adpe: float,\n    gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the server\n\n    Args:\n        server_embodied_adpe: ADPe embodied impact of the server in kgSbeq.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_adpe: ADPe embodied impact of a single GPU in kgSbeq.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The ADPe embodied impact of the server and the GPUs in kgSbeq.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_adpe + gpu_required_count * gpu_embodied_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_pe","title":"<code>server_gpu_embodied_pe(server_embodied_pe, server_gpu_count, gpu_embodied_pe, gpu_required_count)</code>","text":"<p>Compute the Primary Energy (PE) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_pe</code> <code>float</code> <p>PE embodied impact of the server in MJ.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_pe</code> <code>float</code> <p>PE embodied impact of a single GPU in MJ.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The PE embodied impact of the server and the GPUs in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_pe(\n    server_embodied_pe: float,\n    server_gpu_count: float,\n    gpu_embodied_pe: float,\n    gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Primary Energy (PE) embodied impact of the server\n\n    Args:\n        server_embodied_pe: PE embodied impact of the server in MJ.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_pe: PE embodied impact of a single GPU in MJ.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The PE embodied impact of the server and the GPUs in MJ.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_pe + gpu_required_count * gpu_embodied_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_gwp","title":"<code>request_embodied_gwp(server_gpu_embodied_gwp, server_lifetime, generation_latency)</code>","text":"<p>Compute the Global Warming Potential (GWP) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of the server and the GPUs in kgCO2eq.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>float</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The GWP embodied impact of the request in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_gwp(\n    server_gpu_embodied_gwp: float,\n    server_lifetime: float,\n    generation_latency: float\n) -&gt; float:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_gwp: GWP embodied impact of the server and the GPUs in kgCO2eq.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The GWP embodied impact of the request in kgCO2eq.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_adpe","title":"<code>request_embodied_adpe(server_gpu_embodied_adpe, server_lifetime, generation_latency)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of the server and the GPUs in kgSbeq.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>float</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The ADPe embodied impact of the request in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_adpe(\n    server_gpu_embodied_adpe: float,\n    server_lifetime: float,\n    generation_latency: float\n) -&gt; float:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_adpe: ADPe embodied impact of the server and the GPUs in kgSbeq.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The ADPe embodied impact of the request in kgSbeq.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_pe","title":"<code>request_embodied_pe(server_gpu_embodied_pe, server_lifetime, generation_latency)</code>","text":"<p>Compute the Primary Energy (PE) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_pe</code> <code>float</code> <p>PE embodied impact of the server and the GPUs in MJ.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>float</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The PE embodied impact of the request in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_pe(\n    server_gpu_embodied_pe: float,\n    server_lifetime: float,\n    generation_latency: float\n) -&gt; float:\n    \"\"\"\n    Compute the Primary Energy (PE) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_pe: PE embodied impact of the server and the GPUs in MJ.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The PE embodied impact of the request in MJ.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.compute_llm_impacts_dag","title":"<code>compute_llm_impacts_dag(model_active_parameter_count, model_total_parameter_count, output_token_count, request_latency, if_electricity_mix_adpe, if_electricity_mix_pe, if_electricity_mix_gwp, model_quantization_bits=MODEL_QUANTIZATION_BITS, gpu_energy_alpha=GPU_ENERGY_ALPHA, gpu_energy_beta=GPU_ENERGY_BETA, gpu_latency_alpha=GPU_LATENCY_ALPHA, gpu_latency_beta=GPU_LATENCY_BETA, gpu_memory=GPU_MEMORY, gpu_embodied_gwp=GPU_EMBODIED_IMPACT_GWP, gpu_embodied_adpe=GPU_EMBODIED_IMPACT_ADPE, gpu_embodied_pe=GPU_EMBODIED_IMPACT_PE, server_gpu_count=SERVER_GPUS, server_power=SERVER_POWER, server_embodied_gwp=SERVER_EMBODIED_IMPACT_GWP, server_embodied_adpe=SERVER_EMBODIED_IMPACT_ADPE, server_embodied_pe=SERVER_EMBODIED_IMPACT_PE, server_lifetime=HARDWARE_LIFESPAN, datacenter_pue=DATACENTER_PUE)</code>","text":"<p>Compute the impacts dag of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>float</code> <p>Number of active parameters of the model.</p> required <code>model_total_parameter_count</code> <code>float</code> <p>Number of parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency in seconds.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <code>model_quantization_bits</code> <code>Optional[int]</code> <p>Number of bits used to represent the model weights.</p> <code>MODEL_QUANTIZATION_BITS</code> <code>gpu_energy_alpha</code> <code>Optional[float]</code> <p>Alpha parameter of the GPU linear power consumption profile.</p> <code>GPU_ENERGY_ALPHA</code> <code>gpu_energy_beta</code> <code>Optional[float]</code> <p>Beta parameter of the GPU linear power consumption profile.</p> <code>GPU_ENERGY_BETA</code> <code>gpu_latency_alpha</code> <code>Optional[float]</code> <p>Alpha parameter of the GPU linear latency profile.</p> <code>GPU_LATENCY_ALPHA</code> <code>gpu_latency_beta</code> <code>Optional[float]</code> <p>Beta parameter of the GPU linear latency profile.</p> <code>GPU_LATENCY_BETA</code> <code>gpu_memory</code> <code>Optional[float]</code> <p>Amount of memory available on a single GPU.</p> <code>GPU_MEMORY</code> <code>gpu_embodied_gwp</code> <code>Optional[float]</code> <p>GWP embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_GWP</code> <code>gpu_embodied_adpe</code> <code>Optional[float]</code> <p>ADPe embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_ADPE</code> <code>gpu_embodied_pe</code> <code>Optional[float]</code> <p>PE embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_PE</code> <code>server_gpu_count</code> <code>Optional[int]</code> <p>Number of available GPUs in the server.</p> <code>SERVER_GPUS</code> <code>server_power</code> <code>Optional[float]</code> <p>Power consumption of the server in kW.</p> <code>SERVER_POWER</code> <code>server_embodied_gwp</code> <code>Optional[float]</code> <p>GWP embodied impact of the server in kgCO2eq.</p> <code>SERVER_EMBODIED_IMPACT_GWP</code> <code>server_embodied_adpe</code> <code>Optional[float]</code> <p>ADPe embodied impact of the server in kgSbeq.</p> <code>SERVER_EMBODIED_IMPACT_ADPE</code> <code>server_embodied_pe</code> <code>Optional[float]</code> <p>PE embodied impact of the server in MJ.</p> <code>SERVER_EMBODIED_IMPACT_PE</code> <code>server_lifetime</code> <code>Optional[float]</code> <p>Lifetime duration of the server in seconds.</p> <code>HARDWARE_LIFESPAN</code> <code>datacenter_pue</code> <code>Optional[float]</code> <p>PUE of the datacenter.</p> <code>DATACENTER_PUE</code> <p>Returns:</p> Type Description <code>dict[str, float]</code> <p>The impacts dag with all intermediate states.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>def compute_llm_impacts_dag(\n    model_active_parameter_count: float,\n    model_total_parameter_count: float,\n    output_token_count: float,\n    request_latency: float,\n    if_electricity_mix_adpe: float,\n    if_electricity_mix_pe: float,\n    if_electricity_mix_gwp: float,\n    model_quantization_bits: Optional[int] = MODEL_QUANTIZATION_BITS,\n    gpu_energy_alpha: Optional[float] = GPU_ENERGY_ALPHA,\n    gpu_energy_beta: Optional[float] = GPU_ENERGY_BETA,\n    gpu_latency_alpha: Optional[float] = GPU_LATENCY_ALPHA,\n    gpu_latency_beta: Optional[float] = GPU_LATENCY_BETA,\n    gpu_memory: Optional[float] = GPU_MEMORY,\n    gpu_embodied_gwp: Optional[float] = GPU_EMBODIED_IMPACT_GWP,\n    gpu_embodied_adpe: Optional[float] = GPU_EMBODIED_IMPACT_ADPE,\n    gpu_embodied_pe: Optional[float] = GPU_EMBODIED_IMPACT_PE,\n    server_gpu_count: Optional[int] = SERVER_GPUS,\n    server_power: Optional[float] = SERVER_POWER,\n    server_embodied_gwp: Optional[float] = SERVER_EMBODIED_IMPACT_GWP,\n    server_embodied_adpe: Optional[float] = SERVER_EMBODIED_IMPACT_ADPE,\n    server_embodied_pe: Optional[float] = SERVER_EMBODIED_IMPACT_PE,\n    server_lifetime: Optional[float] = HARDWARE_LIFESPAN,\n    datacenter_pue: Optional[float] = DATACENTER_PUE,\n) -&gt; dict[str, float]:\n    \"\"\"\n    Compute the impacts dag of an LLM generation request.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        model_total_parameter_count: Number of parameters of the model.\n        output_token_count: Number of generated tokens.\n        request_latency: Measured request latency in seconds.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n        model_quantization_bits: Number of bits used to represent the model weights.\n        gpu_energy_alpha: Alpha parameter of the GPU linear power consumption profile.\n        gpu_energy_beta: Beta parameter of the GPU linear power consumption profile.\n        gpu_latency_alpha: Alpha parameter of the GPU linear latency profile.\n        gpu_latency_beta: Beta parameter of the GPU linear latency profile.\n        gpu_memory: Amount of memory available on a single GPU.\n        gpu_embodied_gwp: GWP embodied impact of a single GPU.\n        gpu_embodied_adpe: ADPe embodied impact of a single GPU.\n        gpu_embodied_pe: PE embodied impact of a single GPU.\n        server_gpu_count: Number of available GPUs in the server.\n        server_power: Power consumption of the server in kW.\n        server_embodied_gwp: GWP embodied impact of the server in kgCO2eq.\n        server_embodied_adpe: ADPe embodied impact of the server in kgSbeq.\n        server_embodied_pe: PE embodied impact of the server in MJ.\n        server_lifetime: Lifetime duration of the server in seconds.\n        datacenter_pue: PUE of the datacenter.\n\n    Returns:\n        The impacts dag with all intermediate states.\n    \"\"\"\n    results = dag.execute(\n        model_active_parameter_count=model_active_parameter_count,\n        model_total_parameter_count=model_total_parameter_count,\n        model_quantization_bits=model_quantization_bits,\n        output_token_count=output_token_count,\n        request_latency=request_latency,\n        if_electricity_mix_gwp=if_electricity_mix_gwp,\n        if_electricity_mix_adpe=if_electricity_mix_adpe,\n        if_electricity_mix_pe=if_electricity_mix_pe,\n        gpu_energy_alpha=gpu_energy_alpha,\n        gpu_energy_beta=gpu_energy_beta,\n        gpu_latency_alpha=gpu_latency_alpha,\n        gpu_latency_beta=gpu_latency_beta,\n        gpu_memory=gpu_memory,\n        gpu_embodied_gwp=gpu_embodied_gwp,\n        gpu_embodied_adpe=gpu_embodied_adpe,\n        gpu_embodied_pe=gpu_embodied_pe,\n        server_gpu_count=server_gpu_count,\n        server_power=server_power,\n        server_embodied_gwp=server_embodied_gwp,\n        server_embodied_adpe=server_embodied_adpe,\n        server_embodied_pe=server_embodied_pe,\n        server_lifetime=server_lifetime,\n        datacenter_pue=datacenter_pue,\n    )\n    return results\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.compute_llm_impacts","title":"<code>compute_llm_impacts(model_active_parameter_count, model_total_parameter_count, output_token_count, if_electricity_mix_adpe, if_electricity_mix_pe, if_electricity_mix_gwp, request_latency=None, **kwargs)</code>","text":"<p>Compute the impacts of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>ValueOrRange</code> <p>Number of active parameters of the model.</p> required <code>model_total_parameter_count</code> <code>ValueOrRange</code> <p>Number of total parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <code>request_latency</code> <code>Optional[float]</code> <p>Measured request latency in seconds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Any other optional parameter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Impacts</code> <p>The impacts of an LLM generation request.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>def compute_llm_impacts(\n    model_active_parameter_count: ValueOrRange,\n    model_total_parameter_count: ValueOrRange,\n    output_token_count: float,\n    if_electricity_mix_adpe: float,\n    if_electricity_mix_pe: float,\n    if_electricity_mix_gwp: float,\n    request_latency: Optional[float] = None,\n    **kwargs: Any\n) -&gt; Impacts:\n    \"\"\"\n    Compute the impacts of an LLM generation request.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        model_total_parameter_count: Number of total parameters of the model.\n        output_token_count: Number of generated tokens.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n        request_latency: Measured request latency in seconds.\n        **kwargs: Any other optional parameter.\n\n    Returns:\n        The impacts of an LLM generation request.\n    \"\"\"\n    if request_latency is None:\n        request_latency = math.inf\n\n    active_params = [model_active_parameter_count]\n    total_params = [model_total_parameter_count]\n\n    if isinstance(model_active_parameter_count, Range) or isinstance(model_total_parameter_count, Range):\n        if isinstance(model_active_parameter_count, Range):\n            active_params = [model_active_parameter_count.min, model_active_parameter_count.max]\n        else:\n            active_params = [model_active_parameter_count, model_active_parameter_count]\n        if isinstance(model_total_parameter_count, Range):\n            total_params = [model_total_parameter_count.min, model_total_parameter_count.max]\n        else:\n            total_params = [model_total_parameter_count, model_total_parameter_count]\n\n    results = {}\n    fields = [\"request_energy\", \"request_usage_gwp\", \"request_usage_adpe\", \"request_usage_pe\",\n              \"request_embodied_gwp\", \"request_embodied_adpe\", \"request_embodied_pe\"]\n    for act_param, tot_param in zip(active_params, total_params):\n        res = compute_llm_impacts_dag(\n            model_active_parameter_count=act_param,\n            model_total_parameter_count=tot_param,\n            output_token_count=output_token_count,\n            request_latency=request_latency,\n            if_electricity_mix_adpe=if_electricity_mix_adpe,\n            if_electricity_mix_pe=if_electricity_mix_pe,\n            if_electricity_mix_gwp=if_electricity_mix_gwp,\n            **kwargs\n        )\n        for field in fields:\n            if field in results:\n                results[field] = Range(min=results[field], max=res[field])\n            else:\n                results[field] = res[field]\n\n    energy = Energy(value=results[\"request_energy\"])\n    gwp_usage = GWP(value=results[\"request_usage_gwp\"])\n    adpe_usage = ADPe(value=results[\"request_usage_adpe\"])\n    pe_usage = PE(value=results[\"request_usage_pe\"])\n    gwp_embodied = GWP(value=results[\"request_embodied_gwp\"])\n    adpe_embodied = ADPe(value=results[\"request_embodied_adpe\"])\n    pe_embodied = PE(value=results[\"request_embodied_pe\"])\n    return Impacts(\n        energy=energy,\n        gwp=gwp_usage + gwp_embodied,\n        adpe=adpe_usage + adpe_embodied,\n        pe=pe_usage + pe_embodied,\n        usage=Usage(\n            energy=energy,\n            gwp=gwp_usage,\n            adpe=adpe_usage,\n            pe=pe_usage\n        ),\n        embodied=Embodied(\n            gwp=gwp_embodied,\n            adpe=adpe_embodied,\n            pe=pe_embodied\n        )\n    )\n</code></pre>"},{"location":"reference/impacts/modeling/","title":"modeling","text":""},{"location":"reference/impacts/modeling/#impacts.modeling.Range","title":"<code>Range</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>RangeValue data model to represent intervals.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>float</code> <p>Lower bound of the interval.</p> <code>max</code> <code>float</code> <p>Upper bound of the interval.</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Impact","title":"<code>Impact</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base impact data model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Impact type.</p> <code>name</code> <code>str</code> <p>Impact name.</p> <code>value</code> <code>ValueOrRange</code> <p>Impact value.</p> <code>unit</code> <code>str</code> <p>Impact unit.</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Energy","title":"<code>Energy</code>","text":"<p>               Bases: <code>Impact</code></p> <p>Energy consumption.</p> Info <p>Final energy consumption \"measured from the plug\".</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>energy</p> <code>name</code> <code>str</code> <p>Energy</p> <code>value</code> <code>str</code> <p>Energy value</p> <code>unit</code> <code>str</code> <p>Kilowatt-hour (kWh)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.GWP","title":"<code>GWP</code>","text":"<p>               Bases: <code>Impact</code></p> <p>Global Warming Potential (GWP) impact.</p> Info <p>Also, commonly known as GHG/carbon emissions.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>GWP</p> <code>name</code> <code>str</code> <p>Global Warming Potential</p> <code>value</code> <code>str</code> <p>GWP value</p> <code>unit</code> <code>str</code> <p>Kilogram Carbon Dioxide Equivalent (kgCO2eq)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.ADPe","title":"<code>ADPe</code>","text":"<p>               Bases: <code>Impact</code></p> <p>Abiotic Depletion Potential for Elements (ADPe) impact.</p> Info <p>Impact on the depletion of non-living resources such as minerals or metals.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>ADPe</p> <code>name</code> <code>str</code> <p>Abiotic Depletion Potential (elements)</p> <code>value</code> <code>str</code> <p>ADPe value</p> <code>unit</code> <code>str</code> <p>Kilogram Antimony Equivalent (kgSbeq)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.PE","title":"<code>PE</code>","text":"<p>               Bases: <code>Impact</code></p> <p>Primary Energy (PE) impact.</p> Info <p>Total energy consumed from primary sources.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>PE</p> <code>name</code> <code>str</code> <p>Primary Energy</p> <code>value</code> <code>str</code> <p>PE value</p> <code>unit</code> <code>str</code> <p>Megajoule (MJ)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Phase","title":"<code>Phase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base impact phase data model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Phase type.</p> <code>name</code> <code>str</code> <p>Phase name.</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Usage","title":"<code>Usage</code>","text":"<p>               Bases: <code>Phase</code></p> <p>Usage impacts data model.</p> Info <p>Represents the phase of energy consumption during model execution.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>usage</p> <code>name</code> <code>str</code> <p>Usage</p> <code>energy</code> <code>Energy</code> <p>Energy consumption</p> <code>gwp</code> <code>GWP</code> <p>Global Warming Potential (GWP) usage impact</p> <code>adpe</code> <code>ADPe</code> <p>Abiotic Depletion Potential for Elements (ADPe) usage impact</p> <code>pe</code> <code>PE</code> <p>Primary Energy (PE) usage impact</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Embodied","title":"<code>Embodied</code>","text":"<p>               Bases: <code>Phase</code></p> <p>Embodied impacts data model.</p> Info <p>Encompasses resource extraction, manufacturing, and transportation phases associated with the model's lifecycle.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>embodied</p> <code>name</code> <code>str</code> <p>Embodied</p> <code>gwp</code> <code>GWP</code> <p>Global Warming Potential (GWP) embodied impact</p> <code>adpe</code> <code>ADPe</code> <p>Abiotic Depletion Potential for Elements (ADPe) embodied impact</p> <code>pe</code> <code>PE</code> <p>Primary Energy (PE) embodied impact</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Impacts","title":"<code>Impacts</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Impacts data model.</p> <p>Attributes:</p> Name Type Description <code>energy</code> <code>Energy</code> <p>Total energy consumption</p> <code>gwp</code> <code>GWP</code> <p>Total Global Warming Potential (GWP) impact</p> <code>adpe</code> <code>ADPe</code> <p>Total Abiotic Depletion Potential for Elements (ADPe) impact</p> <code>pe</code> <code>PE</code> <p>Total Primary Energy (PE) impact</p> <code>usage</code> <code>Usage</code> <p>Impacts for the usage phase</p> <code>embodied</code> <code>Embodied</code> <p>Impacts for the embodied phase</p>"},{"location":"reference/tracers/anthropic_tracer/","title":"anthropic_tracer","text":""},{"location":"reference/tracers/cohere_tracer/","title":"cohere_tracer","text":""},{"location":"reference/tracers/google_tracer/","title":"google_tracer","text":""},{"location":"reference/tracers/huggingface_tracer/","title":"huggingface_tracer","text":""},{"location":"reference/tracers/litellm_tracer/","title":"litellm_tracer","text":""},{"location":"reference/tracers/mistralai_tracer/","title":"mistralai_tracer","text":""},{"location":"reference/tracers/openai_tracer/","title":"openai_tracer","text":""},{"location":"reference/tracers/utils/","title":"utils","text":""},{"location":"reference/tracers/utils/#tracers.utils.llm_impacts","title":"<code>llm_impacts(provider, model_name, output_token_count, request_latency, electricity_mix_zone='WOR')</code>","text":"<p>High-level function to compute the impacts of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider.</p> required <code>model_name</code> <code>str</code> <p>Name of the LLM used.</p> required <code>output_token_count</code> <code>int</code> <p>Number of generated tokens.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency in seconds.</p> required <code>electricity_mix_zone</code> <code>Optional[str]</code> <p>ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).</p> <code>'WOR'</code> <p>Returns:</p> Type Description <code>Optional[Impacts]</code> <p>The impacts of an LLM generation request.</p> Source code in <code>ecologits/tracers/utils.py</code> <pre><code>def llm_impacts(\n    provider: str,\n    model_name: str,\n    output_token_count: int,\n    request_latency: float,\n    electricity_mix_zone: Optional[str] = \"WOR\",\n) -&gt; Optional[Impacts]:\n    \"\"\"\n    High-level function to compute the impacts of an LLM generation request.\n\n    Args:\n        provider: Name of the provider.\n        model_name: Name of the LLM used.\n        output_token_count: Number of generated tokens.\n        request_latency: Measured request latency in seconds.\n        electricity_mix_zone: ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).\n\n    Returns:\n        The impacts of an LLM generation request.\n    \"\"\"\n\n    model = models.find_model(provider=provider, model_name=model_name)\n    if model is None:\n        logger.debug(f\"Could not find model `{model_name}` for {provider} provider.\")\n        return None\n    model_active_params = model.active_parameters \\\n                          or Range(min=model.active_parameters_range[0], max=model.active_parameters_range[1])\n    model_total_params = model.total_parameters \\\n                         or Range(min=model.total_parameters_range[0], max=model.total_parameters_range[1])\n\n    electricity_mix = electricity_mixes.find_electricity_mix(zone=electricity_mix_zone)\n    if electricity_mix is None:\n        logger.debug(f\"Could not find electricity mix `{electricity_mix_zone}` in the ADEME database\")\n        return None\n    if_electricity_mix_adpe=electricity_mix.adpe\n    if_electricity_mix_pe=electricity_mix.pe\n    if_electricity_mix_gwp=electricity_mix.gwp\n\n    return compute_llm_impacts(\n        model_active_parameter_count=model_active_params,\n        model_total_parameter_count=model_total_params,\n        output_token_count=output_token_count,\n        request_latency=request_latency,\n        if_electricity_mix_adpe=if_electricity_mix_adpe,\n        if_electricity_mix_pe=if_electricity_mix_pe,\n        if_electricity_mix_gwp=if_electricity_mix_gwp,\n    )\n</code></pre>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The  EcoLogits library tracks the energy consumption and environmental impacts of generative AI models accessed through APIs and their official client libraries. </p> <p>It achieves this by patching the Python client libraries, ensuring that each API request is wrapped with an impact calculation function. This function computes the environmental impact based on several request features, such as the chosen model, the number of tokens generated, and the request's latency. The resulting data is then encapsulated in an <code>Impacts</code> object, which is added to the response, containing the environmental impacts for a specific request.</p> <ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>ecologits</code> with <code>pip</code> and get up and running in minutes.</p> <p> Getting started</p> </li> <li> <p> Environmental impacts</p> <p>Understand what environmental impacts and phases are reported.  </p> <p> Tutorial</p> </li> <li> <p> Supported providers</p> <p>List of providers and tutorials on how to make requests.</p> <p> Providers</p> </li> <li> <p> Methodology</p> <p>Understand how we estimate environmental impacts.</p> <p> Methodology</p> </li> </ul>"},{"location":"tutorial/#initialization-of-ecologits","title":"Initialization of EcoLogits","text":"<p>To use EcoLogits in your projects, you will need to initialize the client tracers that are used internally to intercept and enrich responses. The default initialization will use default parameters and enable tracking of all available providers. To change that behaviour, read along on how to configure EcoLogits.</p> <pre><code>from ecologits import EcoLogits\n\n# Default initialization method\nEcoLogits.init()\n</code></pre>"},{"location":"tutorial/#configure-providers","title":"Configure providers","text":"<p>You can select which provider to enable with EcoLogits using the <code>providers</code> parameter.</p> <p>Default behavior is to enable all available providers.</p> Select a providers to enable<pre><code>from ecologits import EcoLogits\n\n# Examples on how to enable one or multiple providers\nEcoLogits.init(providers=\"openai\")\nEcoLogits.init(providers=[\"anthropic\", \"mistralai\"])\n</code></pre> Disabling a provider at runtime is not supported <p>It is currently not possible to dynamically activate and deactivate a provider at runtime. Each time that <code>EcoLogits</code> is re-initialized with another providers, the latter will be added to the list of already initialized providers. If you think that un-initializing a provider could be necessary for your use case, please open an issue .\"</p>"},{"location":"tutorial/#configure-electricity-mix","title":"Configure electricity mix","text":"<p>You can change the electricity mix  for server-side computation depending on a specific location. EcoLogits will automatically change the default impact factors for electricity consumption according to the selected zone. </p> <p>Available zones are listed in the electricity_mixes.csv  file and are based on the ISO 3166-1 alpha-3  convention with some extras like <code>WOR</code> for World or <code>EEE</code> for Europe. </p> <p>Electricity mixes for each geographic zone are sourced from the ADEME Base Empreinte\u00ae  database and are based on yearly averages.</p> <p>Default electricity mix zone is <code>WOR</code> for World.</p> Select a different electricity mix<pre><code>from ecologits import EcoLogits\n\n# Select the electricity mix of France\nEcoLogits.init(electricity_mix_zone=\"FRA\")\n</code></pre>"},{"location":"tutorial/impacts/","title":"Environmental Impacts","text":"<p>Environmental impacts are reported for each request in the <code>Impacts</code> pydantic model and features multiple criteria such as the energy and global warming potential per phase (usage or embodied) as well as the total impacts.</p> <p>To learn more on how we estimate the environmental impacts and what are our hypotheses go to the methodology section.</p> Structure of Impacts model<pre><code>from ecologits.impacts.modeling import *\n\nImpacts(\n    energy=Energy(), # (1)!\n    gwp=GWP(),\n    adpe=ADPe(),\n    pe=PE(),\n    usage=Usage( # (2)!\n        energy=Energy(),\n        gwp=GWP(),\n        adpe=ADPe(),\n        pe=PE(),\n    ),\n    embodied=Embodied( # (3)!\n        gwp=GWP(),\n        adpe=ADPe(),\n        pe=PE(),\n    )\n)\n</code></pre> <ol> <li>Total impacts for all phases.</li> <li>Usage impacts for the electricity consumption impacts. Note that the energy is equal to the \"total\" energy impact.</li> <li>Embodied impacts for resource extract, manufacturing and transportation of hardware components allocated to the request. </li> </ol> <p>You can extract an impact with:</p> <pre><code>&gt;&gt;&gt; response.impacts.usage.gwp.value  # (1)!\n0.34    # Expressed in kgCO2eq.\n</code></pre> <ol> <li>Assuming you have made an inference and get the response in an <code>response</code> object.</li> </ol> <p>Or you could get value range impact instead:</p> <pre><code>&gt;&gt;&gt; response.impacts.usage.gwp.value\nRange(min=0.16, max=0.48) # Expressed in kgCO2eq (1)\n</code></pre> <ol> <li><code>Range</code> are used to define intervals.</li> </ol>"},{"location":"tutorial/impacts/#criteria","title":"Criteria","text":"<p>To evaluate the impact of human activities on the planet or on the climate we use criteria that usually focus on a specific issue such as GHG emissions for global warming, water consumption and pollution or the depletion of natural resources. We currently support three environmental impact criteria in addition with the direct energy consumption. </p> <p>Monitoring multiple criteria is useful to avoid pollution shifting, which is defined as the transfer of pollution from one medium to another. It is a common pitfall to optimize only one criterion like GHG emissions (e.g. buying new hardware that is more energy efficient), that can lead to higher impacts on minerals and metals depletion for example (see encyclopedia.com ).</p>"},{"location":"tutorial/impacts/#energy","title":"Energy","text":"<p>The <code>Energy</code> criterion refers to the direct electricity consumption of GPUs, server and other equipments from the data center. As defined the energy criteria is not an environmental impact, but it is used to estimate other impacts in the usage phase. This criterion is expressed in kilowatt-hour (kWh).</p> Energy model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>energy</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Energy</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>Energy value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilowatt-hour (kWh)</p> </li> </ul>"},{"location":"tutorial/impacts/#global-warming-potential-gwp","title":"Global Warming Potential (GWP)","text":"<p>The Global Warming Potential (<code>GWP</code>) criterion is an index measuring how much heat is absorbed by greenhouse gases in the atmosphere compared to carbon dioxide. This criterion is expressed in kilogram of carbon dioxide equivalent (kgCO2eq).</p> <p>Learn more: wikipedia.org </p> GWP model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>GWP</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Global Warming Potential</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>GWP value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilogram Carbon Dioxide Equivalent (kgCO2eq)</p> </li> </ul>"},{"location":"tutorial/impacts/#abiotic-depletion-potential-for-elements-adpe","title":"Abiotic Depletion Potential for Elements (ADPe)","text":"<p>The Abiotic Depletion Potential \u2013 elements (<code>ADPe</code>) criterion represents the reduction of non-renewable and non-living (abiotic) resources such as metals and minerals. This criterion is expressed in kilogram of antimony equivalent (kgSbeq).</p> <p>Learn more: sciencedirect.com </p> ADPe model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>ADPe</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Abiotic Depletion Potential (elements)</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>ADPe value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilogram Antimony Equivalent (kgSbeq)</p> </li> </ul>"},{"location":"tutorial/impacts/#primary-energy-pe","title":"Primary Energy (PE)","text":"<p>The Primary Energy (<code>PE</code>) criterion represents the amount of energy consumed from natural sources such as raw fuels and other forms of energy, including waste. This criterion is expressed in megajoule (MJ). </p> <p>Learn more: wikipedia.org </p> PE model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>PE</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Primary Energy</p> </li> <li> <code>value</code>               (<code>str</code>)           \u2013            <p>PE value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Megajoule (MJ)</p> </li> </ul>"},{"location":"tutorial/impacts/#phases","title":"Phases","text":"<p>Inspired from the Life Cycle Assessment methodology we classify impacts is two phases (usage and embodied). The usage phase is about the environmental impacts related to the energy consumption while using an AI model. The embodied phase encompasses upstream impacts such as resource extraction, manufacturing, and transportation. We currently do not support the third phase which is end-of-life due to a lack of open research and transparency on that matter.</p> <p>Learn more: wikipedia.org </p> <p>Another pitfall in environmental impact assessment is to only look at the usage phase and ignore upstream and downstream impacts. This can lead to higher overall impacts on the entire life cycle. If you replace old hardware by newer that is more energy efficient, you will get a reduction of impacts on the usage phase, but it will increase the upstream impacts as well.  </p>"},{"location":"tutorial/impacts/#usage","title":"Usage","text":"<p>The <code>Usage</code> phase accounts for the environmental impacts while using AI models. We report all criteria in addition to direct energy consumption for this phase. </p> <p>Note that we use the worldwide average electricity mix impact factor by default. </p> Usage model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>usage</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Usage</p> </li> <li> <code>energy</code>               (<code>Energy</code>)           \u2013            <p>Energy consumption</p> </li> <li> <code>gwp</code>               (<code>GWP</code>)           \u2013            <p>Global Warming Potential (GWP) usage impact</p> </li> <li> <code>adpe</code>               (<code>ADPe</code>)           \u2013            <p>Abiotic Depletion Potential for Elements (ADPe) usage impact</p> </li> <li> <code>pe</code>               (<code>PE</code>)           \u2013            <p>Primary Energy (PE) usage impact</p> </li> </ul>"},{"location":"tutorial/impacts/#embodied","title":"Embodied","text":"<p>The Embodied phase accounts for the upstream environmental impacts such as resource extraction, manufacturing and transportation allocated to the request. We report all criteria (excluding energy consumption) for this phase.</p> Embodied model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>embodied</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Embodied</p> </li> <li> <code>gwp</code>               (<code>GWP</code>)           \u2013            <p>Global Warming Potential (GWP) embodied impact</p> </li> <li> <code>adpe</code>               (<code>ADPe</code>)           \u2013            <p>Abiotic Depletion Potential for Elements (ADPe) embodied impact</p> </li> <li> <code>pe</code>               (<code>PE</code>)           \u2013            <p>Primary Energy (PE) embodied impact</p> </li> </ul>"},{"location":"tutorial/impacts/#impact-factors","title":"Impact Factors","text":"<p>We use impact factors to quantify environmental harm from human activities, measuring the ratio of greenhouse gases, resource consumption, and other criteria resulting from activities like energy consumption, industrial processes, transportation, waster management and more.</p>"},{"location":"tutorial/impacts/#electricity-mix","title":"Electricity Mix","text":"<p>When initializing <code>EcoLogits</code>, you can choose a specific electricity mix zone from the ADEME Base Empreinte\u00ae  database.</p> Select a different electricity mix<pre><code>from ecologits import EcoLogits\n\n# Select the electricity mix of France\nEcoLogits.init(electricity_mix_zone=\"FRA\")\n</code></pre> <p>By default, the <code>WOR</code> World electricity mix is used, whose values are:</p> Impact criteria Value Unit GWP \\(5.904e-1\\) kgCO2eq / kWh ADPe \\(7.378e-7\\) kgSbeq / kWh PE \\(9.988\\) MJ / kWh"},{"location":"tutorial/providers/","title":"Supported providers","text":""},{"location":"tutorial/providers/#list-of-all-providers","title":"List of all providers","text":"Provider name Extra for installation Guide Anthropic <code>anthropic</code> Guide for Anthropic  Cohere <code>cohere</code> Guide for Cohere  Google Gemini <code>google-generativeai</code> Guide for Google Gemini  Hugging Face Hub <code>huggingface-hub</code> Guide for Hugging Face Hub  LiteLLM <code>litellm</code> Guide for LiteLLM  Mistral AI <code>mistralai</code> Guide for Mistral AI  OpenAI <code>openai</code> Guide for OpenAI  Azure OpenAI <code>openai</code> Guide for Azure OpenAI"},{"location":"tutorial/providers/#chat-completions","title":"Chat Completions","text":"Provider Completions Completions (stream) Completions (async) Completions (async + stream) Anthropic Cohere Google Gemini HuggingFace Hub LiteLLM Mistral AI OpenAI Azure OpenAI <p>Partial support for Anthropic streams, see full documentation: Anthropic provider.</p>"},{"location":"tutorial/providers/anthropic/","title":"Anthropic","text":"<p>This guide focuses on the integration of  EcoLogits with the Anthropic official python client .</p> <p>Official links:</p> <ul> <li>Repository:  anthropics/anthropic-sdk-python</li> <li>Documentation:  docs.anthropic.com</li> </ul>"},{"location":"tutorial/providers/anthropic/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Anthropic client, please use the <code>anthropic</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[anthropic]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Anthropic's Python client.</p>"},{"location":"tutorial/providers/anthropic/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/anthropic/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from anthropic import Anthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Anthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nresponse = client.messages.create(\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    model=\"claude-3-haiku-20240307\",\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom anthropic import AsyncAnthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncAnthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.messages.create(\n        max_tokens=100,\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        model=\"claude-3-haiku-20240307\",\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/anthropic/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated in the last chunk for the entire request.</p> SyncAsync <pre><code>from anthropic import Anthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Anthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nwith client.messages.stream(\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    model=\"claude-3-haiku-20240307\",\n) as stream:\n    for text in stream.text_stream:\n        pass\n    # Get estimated environmental impacts of the inference\n    print(stream.impacts)\n</code></pre> <pre><code>import asyncio\nfrom anthropic import AsyncAnthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncAnthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    async with client.messages.stream(\n        max_tokens=100,\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        model=\"claude-3-haiku-20240307\",\n    ) as stream:\n        async for text in stream.text_stream:\n            pass\n        # Get estimated environmental impacts of the inference\n        print(stream.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/cohere/","title":"Cohere","text":"<p>This guide focuses on the integration of  EcoLogits with the Cohere official python client .</p> <p>Official links:</p> <ul> <li>Repository:  mistralai/client-python</li> <li>Documentation:  docs.cohere.com</li> </ul>"},{"location":"tutorial/providers/cohere/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Cohere client, please use the <code>cohere</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[cohere]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Cohere's Python client.</p>"},{"location":"tutorial/providers/cohere/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/cohere/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from cohere import Client\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Client(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nresponse = client.chat(\n    message=\"Tell me a funny joke!\", \n    max_tokens=100\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom cohere import AsyncClient\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncClient(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat(\n        message=\"Tell me a funny joke!\", \n        max_tokens=100\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/cohere/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated in the last chunk for the entire request.</p> SyncAsync <pre><code>from cohere import Client\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Client(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nstream = client.chat_stream(\n    message=\"Tell me a funny joke!\", \n    max_tokens=100\n)\n\nfor chunk in stream:\n    if chunk.event_type == \"stream-end\":\n        # Get estimated environmental impacts of the inference\n        print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom cohere import AsyncClient\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncClient(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    stream = await client.chat_stream(\n        message=\"Tell me a funny joke!\", \n        max_tokens=100\n    )\n\n    async for chunk in stream:\n        if chunk.event_type == \"stream-end\":\n            # Get estimated environmental impacts of the inference\n            print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/google/","title":"Google Gemini","text":"<p>This guide focuses on the integration of  EcoLogits with the Google Gemini official python client .</p> <p>Official links:</p> <ul> <li>Repository:  google-gemini/generative-ai-python</li> <li>Documentation:  ai.google.dev</li> </ul>"},{"location":"tutorial/providers/google/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Google Gemini client, please use the <code>google-generativeai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[google-generativeai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Google Gemini Python client.</p>"},{"location":"tutorial/providers/google/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/google/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini\ngenai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\nresponse = model.generate_content(\"Write a story about a magic backpack.\")\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in async mode\nasync def main() -&gt; None:\n    genai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    response = await model.generate_content_async(\n        \"Write a story about a magic backpack.\"\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/google/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in streaming mode\ngenai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\nstream = model.generate_content(\n    \"Write a story about a magic backpack.\", \n    stream=True\n)\n\n# Get cumulative estimated environmental impacts of the inference\nfor chunk in stream:\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in streaming and async mode\nasync def main() -&gt; None:\n    genai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    stream = await model.generate_content_async(\n        \"Write a story about a magic backpack.\", \n        stream=True\n    )\n\n    # Get cumulative estimated environmental impacts of the inference\n    async for chunk in stream:\n        print(chunk.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/huggingface_hub/","title":"Hugging Face Hub","text":"<p>This guide focuses on the integration of  EcoLogits with the Hugging Face Hub official python client .</p> <p>Official links:</p> <ul> <li>Repository:  huggingface/huggingface_hub</li> <li>Documentation:  huggingface.co</li> </ul>"},{"location":"tutorial/providers/huggingface_hub/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Hugging Face Hub client, please use the <code>huggingface-hub</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[huggingface-hub]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Hugging Face Hub's Python client.</p>"},{"location":"tutorial/providers/huggingface_hub/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/huggingface_hub/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom huggingface_hub import InferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\nresponse = client.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    max_tokens=15\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom huggingface_hub import AsyncInferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncInferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\n\nasync def main() -&gt; None:\n    response = await client.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        max_tokens=15\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/huggingface_hub/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom huggingface_hub import InferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\nstream = client.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    max_tokens=15,\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom huggingface_hub import AsyncInferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncInferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\n\nasync def main() -&gt; None:\n    stream = await client.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        max_tokens=15,\n        stream=True\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/litellm/","title":"LiteLLM","text":"<p>This guide focuses on the integration of  EcoLogits with the LiteLLM official Python client .</p> <p>Official links:</p> <ul> <li>Repository:  BerriAI/litellm</li> <li>Documentation:  litellm.vercel.app</li> </ul>"},{"location":"tutorial/providers/litellm/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with LiteLLM, please use the <code>litellm</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[litellm]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with LiteLLM's Python client.</p>"},{"location":"tutorial/providers/litellm/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/litellm/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data. Make sure you have the api key of the provider used in an .env file. Make sure you call the litellm generation function as \"litellm.completion\" and not just \"completion\".</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport litellm\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nresponse = litellm.completion(\n    model=\"gpt-4o-2024-05-13\",\n    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nimport litellm\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nasync def main() -&gt; None:\n    response = await litellm.acompletion(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/litellm/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport litellm\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nstream = litellm.completion(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello World!\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nimport litellm\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nasync def main() -&gt; None:\n    stream = await litellm.acompletion(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/mistralai/","title":"Mistral AI","text":"<p>This guide focuses on the integration of  EcoLogits with the Mistral AI official python client .</p> <p>Official links:</p> <ul> <li>Repository:  mistralai/client-python</li> <li>Documentation:  docs.mistral.ai</li> </ul>"},{"location":"tutorial/providers/mistralai/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Mistral AI client, please use the <code>mistralai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[mistralai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Mistral AI's Python client.</p>"},{"location":"tutorial/providers/mistralai/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/mistralai/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom mistralai.client import MistralClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = MistralClient(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nresponse = client.chat(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ],\n    model=\"mistral-tiny\"\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom mistralai.async_client import MistralAsyncClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = MistralAsyncClient(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ],\n        model=\"mistral-tiny\"\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/mistralai/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom mistralai.client import MistralClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = MistralClient(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nstream = client.chat_stream(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ],\n    model=\"mistral-tiny\"\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom mistralai.async_client import MistralAsyncClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = MistralAsyncClient(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ],\n        model=\"mistral-tiny\"\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        if hasattr(chunk, \"impacts\"):\n            print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/openai/","title":"OpenAI","text":"<p>This guide focuses on the integration of  EcoLogits with the OpenAI official python client .</p> <p>Official links:</p> <ul> <li>Repository:  openai/openai-python</li> <li>Documentation:  platform.openai.com</li> </ul>"},{"location":"tutorial/providers/openai/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the OpenAI client, please use the <code>openai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[openai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with OpenAI's Python client.</p>"},{"location":"tutorial/providers/openai/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/openai/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom openai import AsyncOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncOpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/openai/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nstream = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello World!\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom openai import AsyncOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncOpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    stream = await client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/openai/#compatibility-with-azure-openai","title":"Compatibility with Azure OpenAI","text":"<p>EcoLogits is also compatible with Azure OpenAI .</p> <pre><code>import os\nfrom ecologits import EcoLogits\nfrom openai import AzureOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AzureOpenAI(\n    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n    api_version=\"2024-02-01\"\n)\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-35-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre>"}]}