{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to EcoLogits","text":"<p>EcoLogits tracks the energy consumption and environmental impacts of using generative AI models through APIs. It supports major LLM providers such as OpenAI, Anthropic, Mistral AI and more (see supported providers).</p> <p>EcoLogits was created and is actively maintained by the GenAI Impact  non-profit.</p>"},{"location":"#requirements","title":"Requirements","text":"<p>Python 3.9+</p> <p>EcoLogits relies on key libraries to provide essential functionalities:</p> <ul> <li>Pydantic  for data modeling.</li> <li>Wrapt  for function patching.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Select providers</p> <p> Anthropic Cohere Google Gemini Hugging Face Inference Endpoints LiteLLM Mistral AI OpenAI </p> <p>Run this command</p> <p>For detailed instructions on each provider, refer to the complete list of supported providers and features. It is also possible to install EcoLogits without any provider.</p>"},{"location":"#usage-example","title":"Usage Example","text":"<p>Below is a simple example demonstrating how to use the GPT-4o-mini model from OpenAI with EcoLogits to track environmental impacts.</p> <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nresponse = client.chat.completions.create( # (1)!\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(f\"Energy consumption: {response.impacts.energy.value} kWh\") # (2)!\nprint(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\") # (3)!\n\n# Get potential warnings\nif response.impacts.has_warnings:\n    for w in response.impacts.warnings:\n        print(w) # (4)!\n\n# Get potential errors\nif response.impacts.has_errors:\n    for w in response.impacts.errors:\n        print(w) # (5)!\n</code></pre> <ol> <li>You don't need to change your code when making a request! </li> <li>Total estimated energy consumption for the request in kilowatt-hour (kWh). You can expect an interval, see exemple here. </li> <li>Total estimated greenhouse gas emissions for the request in kilogram of CO2 equivalent (kgCO2eq). You can expect an interval, see exemple here. </li> <li>For <code>gpt-4o-mini</code>, you can expect two warnings: <code>model-arch-not-released</code> and <code>model-arch-multimodal</code>.</li> <li>On this example you shouldn't get any errors.</li> </ol>"},{"location":"#impacts-output-in-a-nutshell","title":"Impacts output in a nutshell","text":"<p><code>ImpactsOutput</code> object is returned for each request made to supported clients. It gathers, environmental impacts and potential warnings and errors.</p> <p>EcoLogits aims to give a comprehensive view of the environmental footprint of generative AI models at inference. Impacts are reported in total, but also per life cycle phases:</p> <ul> <li>Usage: related to the impacts of the energy consumption during model execution.</li> <li>Embodied: related to resource extraction, manufacturing and transportation of the hardware.</li> </ul> <p>And, multiple criteria:</p> <ul> <li>Energy: related to the final electricity consumption in kWh. </li> <li>Global Warming Potential (GWP): related to climate change, commonly known as GHG emissions in kgCO2eq.</li> <li>Abiotic Depletion Potential for Elements (ADPe): related to the depletion of minerals and metals in kgSbeq.</li> <li>Primary Energy (PE): related to the energy consumed from primary sources like oil, gas or coal in MJ. </li> </ul> <p>For detailed instructions on how to use the library, see our tutorial section. If you want more details on the underlying impacts calculations, see our methodology section.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the Mozilla Public License Version 2.0 (MPL-2.0) .</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We extend our gratitude to Data For Good  and Boavizta  for supporting the development of this project. Their contributions of tools, best practices, and expertise in environmental impact assessment have been invaluable.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Help us improve EcoLogits by contributing! </p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Questions, feature requests and bug reports are all welcome as discussions or issues.</p> <p>When submitting a feature request or bug report, please provide as much detail as possible. For bug reports, please include relevant information about your environment, including the version of EcoLogits and other Python dependencies used in your project.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Getting started and creating a Pull Request is a straightforward process. Since EcoLogits is regularly updated, you can expect to see your contributions incorporated into the project within a matter of days or weeks.</p> <p>For non-trivial changes, please create an issue to discuss your proposal before submitting pull request. This ensures we can review and refine your idea before implementation.</p>"},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<p>You'll need to meet the following requirements:</p> <ul> <li>Python version above 3.9</li> <li>git</li> <li>make</li> <li>poetry</li> <li>pre-commit</li> </ul>"},{"location":"contributing/#installation-and-setup","title":"Installation and setup","text":"<p>Fork the repository on GitHub and clone your fork locally.</p> <pre><code># Clone your fork and cd into the repo directory\ngit clone git@github.com:&lt;your username&gt;/ecologits.git\ncd ecologits\n\n# Install ecologits development dependencies with poetry\nmake install\n</code></pre>"},{"location":"contributing/#check-out-a-new-branch-and-make-your-changes","title":"Check out a new branch and make your changes","text":"<p>Create a new branch for your changes.</p> <pre><code># Checkout a new branch and make your changes\ngit checkout -b my-new-feature-branch\n# Make your changes and implements tests...\n</code></pre>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<p>Run tests locally to make sure everything is working as expected.</p> <pre><code>make test\n</code></pre> <p>If you have added a new provider you will need to record your tests with VCR.py through pytest-recording.</p> <pre><code>make test-record\n</code></pre> <p>Once your tests are recorded, please check that the newly created cassette files (located in <code>tests/cassettes/...</code>) do not contain any sensible information like API tokens. If so you will need to update the configuration accordingly in <code>conftest.py</code> and run again the command to record tests.</p>"},{"location":"contributing/#build-documentation","title":"Build documentation","text":"<p>If you've made any changes to the documentation (including changes to function signatures, class definitions, or docstrings that will appear in the API documentation), make sure it builds successfully.</p> <pre><code># Build documentation\nmake docs\n# If you have changed the documentation, make sure it builds successfully.\n</code></pre> <p>You can also serve the documentation locally.</p> <pre><code># Serve the documentation at localhost:8000\npoetry run mkdocs serve\n</code></pre>"},{"location":"contributing/#code-formatting-and-pre-commit","title":"Code formatting and pre-commit","text":"<p>Before pushing your work, run the pre-commit hook that will check and lint your code.</p> <pre><code># Run all checks before commit\nmake pre-commit\n</code></pre>"},{"location":"contributing/#commit-and-push-your-changes","title":"Commit and push your changes","text":"<p>Commit your changes, push your branch to GitHub, and create a pull request.</p> <p>Please follow the pull request template and fill in as much information as possible. Link to any relevant issues and include a description of your changes.</p> <p>When your pull request is ready for review, add a comment with the message \"please review\" and we'll take a look as soon as we can.</p>"},{"location":"contributing/#documentation-style","title":"Documentation style","text":"<p>Documentation is written in Markdown and built using Material for MkDocs. API documentation is build from docstrings using mkdocstrings.</p>"},{"location":"contributing/#code-documentation","title":"Code documentation","text":"<p>When contributing to EcoLogits, please make sure that all code is well documented. The following should be documented using properly formatted docstrings.</p> <p>We use Google-style docstrings formatted according to PEP 257 guidelines. (See Example Google Style Python Docstrings for further examples.)</p>"},{"location":"contributing/#documentation-style_1","title":"Documentation style","text":"<p>Documentation should be written in a clear, concise, and approachable tone, making it easy for readers to understand and follow along. Aim for brevity while still providing complete information.</p> <p>Code examples are highly encouraged, but should be kept short, simple and self-contained. Ensure that each example is complete, runnable, and can be easily executed by readers.</p>"},{"location":"contributing/#acknowledgment","title":"Acknowledgment","text":"<p>We'd like to acknowledge that this contribution guide is heavily inspired by the excellent guide from Pydantic. Thanks for the inspiration! </p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#why-are-training-impacts-not-included","title":"Why are training impacts not included?","text":"<p>Even though the training impacts of generative AI models are substantial, we currently do not implement them in our methodologies and tools. EcoLogits is aimed at estimating the impacts of an API request made to a GenAI service. To make the impact assessment complete, we indeed should take into account training impacts. However, given that we focus on services that are used by millions of people, doing billions of requests annually the training impacts are in fact negligible.</p> <p>For example, looking at Llama 3 70B, the estimated training greenhouse gas emissions are \\(1,900\\ tCO2eq\\). This is significant for an AI model but comparing it to running inference on that model for say 100 billion requests annually makes the share of impacts induced by training the model becomes very small. E.g., \\(\\frac{1,900\\ \\text{tCO2eq}}{100\\ \\text{billion requests}} = 1.9e-8\\ \\text{tCO2eq per request}\\) or \\(0.019\\ \\text{gCO2eq per request}\\). This, compared to running a simple request to Llama 3 70B that would yield \\(1\\ \\text{to}\\ 5\\ \\text{gCO2}\\) (calculated with our methodology).</p> <p>It does not mean that we do not plan to integrate training impacts, it is just not a priority right now due to the difference in order of magnitude. It is also worth mentioning that estimating the number of requests that will be ever made in the lifespan of a model is very difficult, both for open-source and proprietary models. You can join the discussion on GitHub #70 .</p>"},{"location":"faq/#whats-the-difference-with-codecarbon","title":"What's the difference with CodeCarbon?","text":"<p>EcoLogits and CodeCarbon  are two different tools that do not aim to address the same use case. CodeCarbon should be used when you control the execution environment of your model. This means that if you deploy models on your laptop, your server or in the cloud it is preferable to use CodeCarbon to get energy consumption and estimate carbon emissions associated with running your model (including training, fine-tuning or inference).</p> <p>On the other hand EcoLogits is designed for scenarios where you do not have access to the execution environment of your GenAI model because it is managed by a third-party provider.  In such cases you can rely on EcoLogits to estimate energy consumption and environmental impacts for inference workloads. Both tools are complementary and can be used together to provide a comprehensive view of environmental impacts across different deployment scenarios.</p>"},{"location":"faq/#how-can-i-estimate-impacts-of-general-use-of-genai-models","title":"How can I estimate impacts of general use of GenAI models?","text":"<p>If you want to estimate the environmental impacts of using generative AI models without coding or making request, we recommend you to use our online webapp EcoLogits Calculator .</p>"},{"location":"faq/#how-do-we-assess-impacts-for-proprietary-models","title":"How do we assess impacts for proprietary models?","text":"<p>Environmental impacts are calculated based on model architecture and parameter count. For proprietary models, we lack transparency from providers, so we estimate parameter counts using available information. For GPT models, we based our estimates on leaked GPT-4 architecture and scaled parameters count for GPT-4-Turbo and GPT-4o based on pricing differences. For other proprietary models like Anthropic's Claude, we assume similar impacts for models released around the same time with similar performance on public benchmarks. Please note that these estimates are based on assumptions and may not be exact. Our methods are open-source and transparent, so you can always see the hypotheses we use.</p>"},{"location":"faq/#how-to-reduce-my-environmental-impact","title":"How to reduce my environmental impact?","text":"<p>First, you may want to assess indirect impacts  and rebound effects  of the project you are building. Does the finality of your product or service is impacting negatively the environment? Does the usage of your product or service drives up consumption and environmental impacts of previously existing technology?</p> <p>Try to be frugal and question your usages or needs of AI:</p> <ul> <li>Do you really need AI to solve your problem?</li> <li>Do you really need GenAI to solve your problem? (you can read this paper )</li> <li>Prefer fine-tuning of small and existing models over generalist models.</li> <li>Evaluate before, during and after the development of your project the environmental impacts with tools like EcoLogits or CodeCarbon  (see more tools )</li> <li>Restrict the use case and limit the usage of your tool or feature to the desired purpose.</li> </ul> <p>Do not buy new GPUs or hardware. Hardware production for data centers is responsible for around 50% of the impacts compared to usage impacts. The share is even more bigger for consumer devices, around 80%.</p> <p>Use cloud instances that are located in low emissions / high energy efficiency data centers (see electricitymaps.com ).</p> <p>Optimize your models for production use cases. You can look at model compression technics such as quantization, pruning or distillation. There are also inference optimization tricks available in some software.</p>"},{"location":"why/","title":"Why use EcoLogits?","text":"<p>Generative AI significantly impacts our environment, consuming electricity and contributing to global greenhouse gas emissions. In 2020, the ICT sector accounted for 2.1% to 3.9% of global emissions, with projections suggesting an increase to 6%-8% by 2025 due to continued growth and adoption Freitag et al., 2021. The advent of GenAI technologies like ChatGPT has further exacerbated this trend, causing a sharp rise in energy, water, and hardware costs for major tech companies. [0, 1].</p>"},{"location":"why/#which-is-bigger-training-or-inference-impacts","title":"Which is bigger: training or inference impacts?","text":"<p>The field of Green AI focuses on evaluating the environmental impacts of AI models. While many studies have concentrated on training impacts [2], they often overlook other critical phases like data collection, storage and processing phases, research experiments and inference. For GenAI, the inference phase can significantly overshadow training impacts when models are deployed at scale [3]. EcoLogits specifically addresses this gap by focusing on the inference impacts of GenAI.</p>"},{"location":"why/#how-to-assess-impacts-properly","title":"How to assess impacts properly?","text":"<p>EcoLogits employs state-of-the-art methodologies based on Life Cycle Assessment and open data to assess environmental impacts across multiple phases and criteria. This includes usage impacts from electricity consumption and embodied impacts from the production and transportation of hardware. Our multi-criteria approach also evaluates carbon emissions, abiotic resource depletion, and primary energy consumption, providing a comprehensive view that informs decisions like model selection, hardware upgrades and cloud deployments.</p>"},{"location":"why/#how-difficult-is-it","title":"How difficult is it?","text":"<p>Assessing environmental impacts can be challenging with external providers due to lack of control over the execution environment. Meaning you can easily estimate usage impact regarding energy consumption with CodeCarbon and also embodied impacts with BoaviztAPI, but these tools become less relevant with external service providers. EcoLogits simplifies this by basing calculations on well-founded assumptions about hardware, model size, and operational practices, making it easier to estimate impacts accurately. For more details, see our methodology section.</p>"},{"location":"why/#easy-to-use","title":"Easy to use","text":"<p>EcoLogits integrates seamlessly into existing GenAI providers, allowing you to assess the environmental impact of each API request with minimal code adjustments:</p> <pre><code>from ecologits import EcoLogits\n\nEcoLogits.init()    \n\n# Then, you can make request to any supported provider.\n</code></pre> <p>See the list of supported providers and more code snippets in the tutorial section.</p>"},{"location":"why/#have-more-questions","title":"Have more questions?","text":"<p>Feel free to ask question in our GitHub discussions forum!</p>"},{"location":"methodology/","title":"Methodology","text":""},{"location":"methodology/#evaluation-methodologies","title":"Evaluation methodologies","text":"<p>The following methodologies are currently available and implemented in EcoLogits:</p> <ul> <li> LLM Inference</li> </ul> <p>Upcoming methodologies (join us to help speed up our progress):</p> <ul> <li> Embeddings</li> <li> Image Generation</li> <li> Multi-Modal</li> </ul>"},{"location":"methodology/#methodological-background","title":"Methodological background","text":"<p>EcoLogits employs the Life Cycle Assessment (LCA) methodology, as defined by ISO 14044, to estimate the environmental impacts of requests made to generative AI inference services. This approach focuses on multiple phases of the lifecycle, specifically raw material extraction, manufacturing, transportation (denoted as embodied impacts), usage and end-of-life. Notably, we do not cover the end-of-life phase due to data limitations on e-waste recycling.</p> <p>Our assessment considers three key environmental criteria:</p> <ul> <li>Global Warming Potential (GWP): Evaluates the impact on global warming in terms of CO2 equivalents.</li> <li>Abiotic Resource Depletion for Elements (ADPe): Assesses the consumption of raw minerals and metals, expressed in antimony equivalents.</li> <li>Primary Energy (PE): Calculates energy consumed from natural sources, expressed in megajoules.</li> </ul> <p>Using a bottom-up modeling approach, we assess and aggregate the environmental impacts of all individual service components within scope. This method differs from top-down approaches by allowing precise allocation of each resource's impact to the overall environmental footprint. The key advantage of bottom-up modeling is that our methodology can be customized for each provider that share information.</p> <p>Our method computes high-confidence approximation intervals, providing a range of values within which we are confident enough that the true consumption lies.</p> <p>The methodology is grounded in transparency and reproducibility, utilizing open market and technical data to ensure our results are reliable and verifiable.</p>"},{"location":"methodology/#scope-of-the-methodology","title":"Scope of the methodology","text":"<p>Our methodology focuses on assessing the environmental impacts of GenAI inference tasks. That is why we exclude impacts from training, networking and end-used devices, we thoroughly evaluate the impacts associated with hosting and running the model inferences.</p> Boundaries of our impact assessment methodology. <p>Because evaluating the environmental footprint of GenAI services is hard we make some assumptions to simplify the assessment. In the following section we will describe general assumptions that we use, if you want to learn more about the specifics look at the according methodology page.  </p>"},{"location":"methodology/#assumptions-and-limitations","title":"Assumptions and limitations","text":"<p>Estimating the environmental impacts of generative AI services at inference can be really challenging because of the lack of open data and transparent information from the key players (AI/cloud providers, hardware manufacturers, environmental impact databases, etc.) In the LLM inference methodology we explain at high-level all the assumptions and limitations of our bottom-up approach.</p> <p>Regarding the assumptions we make on proprietary models, we have a dedicated section for increased transparency and explainability.</p>"},{"location":"methodology/#licenses-and-citations","title":"Licenses and citations","text":"<p>All the methodologies are licensed under CC BY-SA 4.0 </p> <p>Please ensure that you adhere to the license terms and properly cite the authors and the GenAI Impact non-profit organization when utilizing this work. Each methodology has an associated paper with specific citation requirements.</p>"},{"location":"methodology/llm_inference/","title":"Environmental Impacts of LLM Inference","text":"<p> v1.0 </p>"},{"location":"methodology/llm_inference/#introduction","title":"Introduction","text":"<p>The environmental impacts of a request, \\(I_{\\text{request}}\\) to a Large Language Model (LLM) can be divided into two components: the usage impacts, \\(I_{\\text{request}}^{\\text{u}}\\), which account for energy consumption, and the embodied impacts, \\(I_{\\text{request}}^{\\text{e}}\\), which account for resource extraction, hardware manufacturing, and transportation:</p> \\[ \\begin{equation*} \\begin{split} I_{\\text{request}}&amp;=I_{\\text{request}}^{\\text{u}}  + I_{\\text{request}}^{\\text{e}} \\\\  &amp;= E_{\\text{request}} \\times F_{\\text{em}}+\\frac{\\Delta T}{\\Delta L} \\times I_{\\text{server}}^{\\text{e}},  \\end{split} \\end{equation*} \\] <p>where \\(E_{\\text{request}}\\) represents the energy consumption of the IT resources associated with the request. \\(F_{\\text{em}}\\) denotes the impact factor of electricity consumption, which varies depending on the location and time. Furthermore, \\(I_{\\text{server}}^{\\text{e}}\\) captures the embodied impacts of the IT resources, and \\(\\frac{\\Delta T}{\\Delta L}\\) signifies the hardware utilization factor, calculated as the computation time divided by the lifetime of the hardware.</p>"},{"location":"methodology/llm_inference/#usage-impacts","title":"Usage impacts","text":"<p>To assess the usage impacts of an LLM inference, we first need to estimate the energy consumption of the server, which is equipped with one or more GPUs. We will also take into account the energy consumption of cooling equipment integrated with the data center, using the Power Usage Effectiveness (PUE) metric.</p> <p>Subsequently, we can calculate the environmental impacts by using the \\(F_{\\text{em}}\\) impact factor of the electricity mix. Ideally, \\(F_{\\text{em}}\\) should vary with location and time to accurately reflect the local energy mix.</p>"},{"location":"methodology/llm_inference/#modeling-gpu-energy-consumption","title":"Modeling GPU energy consumption","text":"<p>By leveraging the open dataset from the LLM Perf Leaderboard, produced by Hugging Face, we can estimate the energy consumption of the GPU using a parametric model.</p> <p>We fit a linear regression model to the dataset, which models the energy consumption per output token as a function of the number of active parameters in the LLM, denoted as \\(P_{\\text{active}}\\).</p> What are active parameters? <p>We distinguish between active parameters and total parameter count for Sparse Mixture-of-Experts (SMoE) models. The total parameter count is used to determine the number of required GPUs to load the model into memory. In contrast, the active parameter count is used to estimate the energy consumption of a single GPU. In practice, SMoE models exhibit lower energy consumption per GPU compared to dense models of equivalent size (in terms of total parameters).</p> <ul> <li>For a dense model: \\(P_{\\text{active}} = P_{\\text{total}}\\)</li> <li>For a SMoE model: \\(P_{\\text{active}} =  P_{\\text{total}} / \\text{number of active experts}\\)</li> </ul> On the LLM Perf Leaderboard dataset filtering <p>We have filtered the dataset to keep relevant data points for the analysis. In particular we have applied the following conditions:</p> <ul> <li>Model number of parameters &gt;= 7B</li> <li>Keep dtype set to float16</li> <li>GPU model is \"NVIDIA A100-SXM4-80GB\"</li> <li>No optimization</li> <li>8bit and 4bit quantization excluding bitsandbytes (bnb)</li> </ul> Figure: Energy consumption (in Wh) per output token vs. number of active parameters (in billions) What is a 95% confidence interval? <p>The standard deviation \\(\\delta\\) of a linear regression measures \"how close are the datapoints to the fitted line\". A priori, the larger it is, the worse is the approximation. Consider a linear regression \\(Y(x) \\approx \\alpha x + \\beta\\). Given some assumptions, we can say that \\(Y(x) \\in [\\alpha x + \\beta - 1.96\\delta, \\alpha x + \\beta + 1.96\\delta]\\) with probability 95% (see [1] and [2] for more details).</p> <p>In our methodology, in order to take into account approximation errors as much as possible, we provide the 95% confidence interval of our linear approximation. The computed linear regression gives a confidence interval of</p> \\[ \\frac{E_{\\text{GPU}}}{\\#T_{\\text{out}}} = \\alpha \\times P_{\\text{active}} + \\beta \\pm 1.96 \\sigma,  \\] <p>with \\(\\alpha = 8.91e-5\\), \\(\\beta = 1.43e-3\\) and \\(\\sigma = 5.19e-4\\). </p> <p>Using these values, we can estimate the energy consumption of a simple GPU for the entire request, given the number of output tokens \\(\\#T_{\\text{out}}\\) and the number of active parameters \\(P_{\\text{active}}\\):</p> \\[ E_{\\text{GPU}}(\\#T_{\\text{out}}, P_{\\text{active}}) = \\#T_{\\text{out}} \\times (\\alpha \\times P_{\\text{active}} + \\beta \\pm 1.96 \\sigma). \\] <p>If the model requires multiple GPUs to be loaded into VRAM, the energy consumption \\(E_{\\text{GPU}}\\) should be multiplied by the number of required GPUs, \\(\\text{GPU}\\) (see below).</p>"},{"location":"methodology/llm_inference/#modeling-server-energy-consumption","title":"Modeling server energy consumption","text":"<p>To estimate the energy consumption of the entire server, we will use the previously estimated GPU energy model and separately estimate the energy consumption of the server itself (without GPUs), denoted as \\(E_{\\text{server} \\backslash \\text{GPU}}\\).</p>"},{"location":"methodology/llm_inference/#server-energy-consumption-without-gpus","title":"Server energy consumption without GPUs","text":"<p>To model the energy consumption of the server without GPUs, we consider a fixed power consumption, \\(W_{\\text{server} \\backslash \\text{GPU}}\\), during inference (or generation latency), denoted as \\(\\Delta T\\). We assume that the server hosts multiple GPUs, but not all of them are actively used for the target inference. Therefore, we account for a portion of the energy consumption based on the number of required GPUs, \\(\\text{GPU}\\):</p> \\[ E_{\\text{server} \\backslash \\text{GPU}}(\\Delta T) = \\Delta T \\times W_{\\text{server} \\backslash \\text{GPU}} \\times \\frac{\\text{GPU}}{\\#\\text{GPU}_{\\text{installed}}}. \\] <p>For a typical high-end GPU-accelerated cloud instance, we use \\(W_{\\text{server} \\backslash \\text{GPU}} = 1\\) kW and \\(\\#\\text{GPU}_{\\text{installed}} = 8\\).</p>"},{"location":"methodology/llm_inference/#estimating-the-generation-latency","title":"Estimating the generation latency","text":"<p>The generation latency, \\(\\Delta T\\), is the duration of the inference measured on the server and is independent of networking latency. We estimate the generation latency using the LLM Perf Leaderboard dataset with the previously mentioned filters applied.</p> <p>We fit a linear regression model on the dataset modeling the generation latency per output token given the number of active parameters of the LLM \\(P_{\\text{active}}\\):</p> Figure: Latency (in s) per output token vs. number of active parameters (in billions) <p>Again, we propagate 95% confidence intervals through our computations. The fit gives an interval of</p> \\[ \\frac{\\Delta T}{\\#T_{\\text{out}}} = \\alpha \\times P_{\\text{active}} + \\beta \\pm 1.96\\delta,  \\] <p>with \\(\\alpha = 8.02e-4\\), \\(\\beta = 2.23e-2\\) and \\(\\delta = 7.00e-6\\). Using these values, we can estimate the generation latency for the entire request given the number of output tokens, \\(\\#T_{\\text{out}}\\), and the number of active parameters, \\(P_{\\text{active}}\\). When possible, we also measure the request latency, \\(\\Delta T_{\\text{request}}\\), and use it as the maximum bound for the generation latency:</p> \\[ \\Delta T(\\#T_{\\text{out}}, P_{\\text{active}}) = \\#T_{\\text{out}} \\times (\\alpha \\times P_{\\text{active}} + \\beta \\pm 1.96\\delta). \\] <p>With the request latency, the generation latency is defined as follows:</p> \\[ \\Delta T(\\#T_{\\text{out}}, P_{\\text{active}}, \\Delta T_{\\text{request}}) = \\min \\left\\{ \\#T_{\\text{out}} \\times (\\alpha \\times P_{\\text{active}} + \\beta \\pm 1.96 \\delta), \\Delta T_{\\text{request}} \\right\\}. \\]"},{"location":"methodology/llm_inference/#estimating-the-number-of-active-gpus","title":"Estimating the number of active GPUs","text":"<p>To estimate the number of required GPUs, \\(\\text{GPU}\\), to load the model in virtual memory, we divide the required memory to host the LLM for inference, noted \\(M_{\\text{model}}\\), by the memory available on one GPU, noted \\(M_{\\text{GPU}}\\).</p> <p>The required memory to host the LLM for inference is estimated based on the total number of parameters and the number of bits used for model weights related to quantization. We also apply a memory overhead of \\(1.2\\) (see Transformers Math 101 ):</p> \\[ M_{\\text{model}}(P_{\\text{total}},Q)=1.2 \\times \\frac{P_{\\text{total}} \\times Q}{8}. \\] <p>We then estimate the number of required GPUs, rounded up:</p> \\[ \\text{GPU}(P_{\\text{total}},Q,M_{\\text{GPU}}) = \\left\\lceil \\frac{M_{\\text{model}}(P_{\\text{total}},Q)}{M_{\\text{GPU}}} \\right\\rceil. \\] <p>To stay consistent with previous assumptions based on LLM Perf Leaderboard data, we use \\(M_{\\text{GPU}} = 80\\) GB for an NVIDIA A100 80GB GPU.</p>"},{"location":"methodology/llm_inference/#complete-server-energy-consumption","title":"Complete server energy consumption","text":"<p>The total server energy consumption for the request, \\(E_{\\text{server}}\\), is computed as follows:</p> \\[ E_{\\text{server}} = E_{\\text{server} \\backslash \\text{GPU}} + \\text{GPU} \\times E_{\\text{GPU}}. \\]"},{"location":"methodology/llm_inference/#modeling-request-energy-consumption","title":"Modeling request energy consumption","text":"<p>To estimate the energy consumption of the request, we multiply the previously computed server energy by the Power Usage Effectiveness (PUE) to account for cooling equipment in the data center:</p> \\[ E_{\\text{request}} = \\text{PUE} \\times E_{\\text{server}}. \\] <p>We typically use a \\(\\text{PUE} = 1.2\\) for hyperscaler data centers or supercomputers.</p>"},{"location":"methodology/llm_inference/#modeling-request-usage-environmental-impacts","title":"Modeling request usage environmental impacts","text":"<p>To assess the environmental impacts of the request for the usage phase, we multiply the estimated electricity consumption by the impact factor of the electricity mix, \\(F_{\\text{em}}\\), specific to the target country and time. Unless otherwise stated, we currently use a worldwide average multicriteria impact factor from the ADEME Base Empreinte\u00ae:</p> \\[ I^\\text{u}_{\\text{request}} = E_{\\text{request}} \\times F_{\\text{em}}. \\] <p>Note that the user can still chose another electricity mix from the ADEME Base Empreinte\u00ae.</p> Some values of \\(F_{\\text{em}}\\) per geographical area Area or country GWP (gCO2eq / kWh) ADPe (kgSbeq / kWh) PE (MJ / kWh) \ud83c\udf10 Worldwide \\(590.4\\) \\(7.378 \\times 10^{-8}\\) \\(9.99\\) \ud83c\uddea\ud83c\uddfa Europe (EEA) \\(509.4\\) \\(6.423 \\times 10^{-8}\\) \\(12.9\\) \ud83c\uddfa\ud83c\uddf8 USA \\(679.8\\) \\(9.855 \\times 10^{-8}\\) \\(11.4\\) \ud83c\udde8\ud83c\uddf3 China \\(1,057\\) \\(8.515 \\times 10^{-8}\\) \\(14.1\\) \ud83c\uddeb\ud83c\uddf7 France \\(81.3\\) \\(4.858 \\times 10^{-8}\\) \\(11.3\\)"},{"location":"methodology/llm_inference/#embodied-impacts","title":"Embodied impacts","text":"<p>To determine the embodied impacts of an LLM inference, we need to estimate the hardware configuration used to host the model and its lifetime. Embodied impacts account for resource extraction (e.g., minerals and metals), manufacturing, and transportation of the hardware.</p>"},{"location":"methodology/llm_inference/#modeling-server-embodied-impacts","title":"Modeling server embodied impacts","text":"<p>To estimate the embodied impacts of IT hardware, we use the BoaviztAPI tool from the non-profit organization Boavizta. This API embeds a bottom-up multicriteria environment impact estimation engine for embodied and usage phases of IT resources and services. We focus on estimating the embodied impacts of a server and a GPU. BoaviztAPI is an open-source project that relies on open databases and open research on environmental impacts of IT equipment.</p>"},{"location":"methodology/llm_inference/#server-embodied-impacts-without-gpu","title":"Server embodied impacts without GPU","text":"<p>To assess the embodied environmental impacts of a high-end AI server, we use an AWS cloud instance as a reference. We selected the <code>p4de.24xlarge</code> instance, as it corresponds to a server that can be used for LLM inference with eight NVIDIA A100 80GB GPU cards. The embodied impacts of this instance will be used to estimate the embodied impacts of the server without GPUs, denoted as \\(I^{\\text{e}}_{\\text{server} \\backslash \\text{GPU}}\\).</p> <p>The embodied environmental impacts of the cloud instance are:</p> Server (without GPU) GWP (kgCO2eq) \\(3000\\) ADPe (kgSbeq) \\(0.25\\) PE (MJ) \\(39,000\\) <p>These impacts does not take into account the eight GPUs. (see below)</p> Example request to reproduce this calculation <p>On the cloud instance route (/v1/cloud/instance) you can POST the following JSON.</p> <pre><code>{\n    \"provider\": \"aws\",\n    \"instance_type\": \"p4de.24xlarge\"\n}\n</code></pre> <p>Or you can use the demo available demo API with this command using <code>curl</code> and parsing the JSON output with <code>jq</code>.</p> <pre><code>curl -X 'POST' \\\n    'https://api.boavizta.org/v1/cloud/instance?verbose=true&amp;criteria=gwp&amp;criteria=adp&amp;criteria=pe' \\\n    -H 'accept: application/json' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n    \"provider\": \"aws\",\n    \"instance_type\": \"p4de.24xlarge\"\n}' | jq\n</code></pre>"},{"location":"methodology/llm_inference/#gpu-embodied-impacts","title":"GPU embodied impacts","text":"<p>Boavizta is currently developing a methodology to provide multicriteria embodied impacts for GPU cards. For this analysis, we use the embodied impact data they computed for a NVIDIA A100 80GB GPU. These values will be used to estimate the embodied impacts of a single GPU, denoted as \\(I^{\\text{e}}_{\\text{GPU}}\\).</p> NIDIA A100 80GB GWP (kgCO2eq) \\(143\\) ADPe (kgSbeq) \\(5.09 \\times 10^{-3}\\) PE (MJ) \\(1,828\\) <p>The GPU embodied impacts will be soon available in the BoaviztAPI tool.</p>"},{"location":"methodology/llm_inference/#complete-server-embodied-impacts","title":"Complete server embodied impacts","text":"<p>The final embodied impacts for the server, including the GPUs, are calculated as follows. Note that the embodied impacts of the server without GPUs are scaled by the number of GPUs required to host the model. This allocation is made to account for the fact that the remaining GPUs on the server can be used to host other models or multiple instances of the same model. As we are estimating the impacts of a single LLM inference, we need to exclude the embodied impacts that would be attributed to other services hosted on the same server:</p> \\[ I^{\\text{e}}_{\\text{server}}=\\frac{\\text{GPU}}{\\#\\text{GPU}_{\\text{installed}}} \\times I^{\\text{e}}_{\\text{server} \\backslash \\text{GPU}} + \\text{GPU} \\times I^{\\text{e}}_{\\text{GPU}}. \\]"},{"location":"methodology/llm_inference/#modeling-request-embodied-environmental-impacts","title":"Modeling request embodied environmental impacts","text":"<p>To allocate the server embodied impacts to the request, we use an allocation based on the hardware utilization factor, \\(\\frac{\\Delta T}{\\Delta L}\\). In this case, \\(\\Delta L\\) represents the lifetime of the server and GPU, which we fix at 5 years:</p> \\[ I^{\\text{e}}_{\\text{request}}=\\frac{\\Delta T}{\\Delta L} \\times I^{\\text{e}}_{\\text{server}}. \\]"},{"location":"methodology/llm_inference/#assumptions-and-limitations","title":"Assumptions and limitations","text":"<p>To be able to estimate environmental impacts of LLMs at inference we took the approach of modeling the key components that compose the service. In this section we will list major assumptions we make when modeling environmental impacts as well as known limitations. When possible we will try to quantify the potential inaccuracies.</p>"},{"location":"methodology/llm_inference/#on-models","title":"On models","text":"<p>Two major information we are looking for is the required infrastructure to host the AI model, such as the number of GPUs as well as the energy consumption that results from doing an inference on the model.</p> <p>Assuming the required infrastructure for open models can be relatively straightforward because the model size is known. But for proprietary models this can very be challenging given that some AI provider do not disclose any technical information on that matter. That's why we rely on estimations of parameters count for closed models, to learn more read the dedicated section.</p> <p>Assuming the energy consumption for AI models is done through benchmarking open models. We tend to rely on external sources for benchmarking, but we conduct our own experiments as well. Because of our limited capacity and the technical complexity to host very big AI models we extrapolate the consumption of smaller models to bigger models.</p> <p>Assumptions:</p> <ul> <li>Models are deployed with pytorch backend.</li> <li>Models are quantized to 4 bits.</li> </ul> <p>Limitations:</p> <ul> <li>We do not account for other inference optimizations such as flash attention, batching or parallelism.</li> <li>We do not benchmark models bigger than 70 billion parameters.</li> <li>We do not have benchmarks for multi-GPU deployments.</li> <li>We do not account for the multiple modalities of a model (only text-to-text generation).</li> </ul>"},{"location":"methodology/llm_inference/#on-benchmarking-data","title":"On benchmarking data","text":"<p>We use linear regression models to approximate energy consumption per token and latency per token as a function of the number of active parameters in the LLM. We represent the linear model as \\(Y = a \\times X + b + \\epsilon\\), where \\(Y\\) is the predicted value, \\(X\\) is the input variable, \\(a\\) and \\(b\\) are the regression coefficients, and \\(\\epsilon\\) is the error term. We assume that the errors (\\(\\epsilon\\)) follow a normal distribution with a mean of zero and a constant variance, represented as \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\). This enables us to use the 95% confidence interval, calculated using the standard deviation of the errors (\\(\\sigma\\)) and the 97.5th percentile point of the standard normal distribution (approximately 1.96).</p>"},{"location":"methodology/llm_inference/#on-hardware","title":"On hardware","text":"<p>We estimate the required infrastructure to run the service in terms of hardware. We consider that the service is hosted in the cloud on servers equipped with high-end GPUs. </p> <p>Assumptions:</p> <ul> <li>Models are deployed on NVIDIA A100 GPUs with 80GB of memory.</li> <li>Base servers are similar to p4de.24xlarge AWS cloud instances.</li> </ul> <p>Limitations:</p> <ul> <li>We do not account for TPUs or other type of accelerators.</li> <li>We do not account for networking or storage primitives.</li> <li>We do not account for infrastructure overheads or utilization factors.</li> </ul>"},{"location":"methodology/llm_inference/#on-data-centers","title":"On data centers","text":"<p>The type of services we model rely on high-end hardware that we consider is hosted by cloud service providers. Thus, we model data centers impacts as well and especially the overhead for cooling equipments.</p> <p>We consider the Power Usage Effectiveness (PUE) metric from data centers. These values can be quite complicated to get from the providers themselves. A good amount of data is available for providers that build their own data centers (such as hyperscalers). But part of the AI workloads are also located in non-hyperscale data centers or in co-located data centers. That's why we prefer to rely on a global average for PUE that can be overridden for providers that disclose more precise data.</p> <p>Assumptions:</p> <ul> <li>PUE = 1.2 (arbitrary value, valid for hyperscalers)</li> </ul> <p>Limitations:</p> <ul> <li>We do not know precisely where are located the data centers that run AI models.</li> <li>We do not account for the specific infrastructure or way to cooldown servers in data centers.</li> <li>We do not account for the local electricity generation (private power plants) specific to the data center.</li> <li>We do not account for the overhead of the cloud provider for internal services like backing up or monitoring.</li> </ul>"},{"location":"methodology/llm_inference/#on-impact-factors","title":"On impact factors","text":"<p>To transform physical values such as energy consumption into environmental impacts we use impact factors. These can be hard to estimate and precise and up-to-date data is rarely open to use.</p> <p>Assumptions:</p> <ul> <li>Electricity mix are taken from the ADEME Base Empreinte database and averaged per country.</li> </ul> <p>Limitations:</p> <ul> <li>We do not account for local electricity generation for data center or regional electricity mixes the smallest supported zone is a country.</li> </ul>"},{"location":"methodology/llm_inference/#on-embodied-impacts","title":"On embodied impacts","text":"<p>We aim at covering the largest scope possible when assessing the environmental impacts. That is why we rely extensively on the work done by Boavizta non-profit. Unfortunately, assessing the environmental impacts of resources extraction, hardware manufacturing and transportation is very challenging mainly due to a lack of transparency from all the organizations that are involved. Estimations of the inaccuracies are currently not supported within Boavizta's methodology and tool (BoaviztAPI).</p>"},{"location":"methodology/llm_inference/#references","title":"References","text":"<ul> <li>LLM-Perf Leaderboard to estimate GPU energy consumption and latency based on the model architecture and number of output tokens.</li> <li>BoaviztAPI to estimate server embodied impacts and base energy consumption.</li> <li>ADEME Base Empreinte\u00ae for electricity mix impacts per country.</li> </ul>"},{"location":"methodology/llm_inference/#citation","title":"Citation","text":"<p>Please cite GenAI Impact non-profit organization and link to this documentation page. </p> <pre><code>Coming soon...\n</code></pre>"},{"location":"methodology/llm_inference/#license","title":"License","text":"<p>This work is licensed under CC BY-SA 4.0 </p>"},{"location":"methodology/proprietary_models/","title":"About proprietary models","text":"<p>EcoLogits is designed to assess the environmental footprint of generative AI models, including both open and proprietary models, during the inference phase. Our methodology is based on open science, data and models to ensure transparency and explainability. However, evaluating proprietary models hosted by private AI providers presents unique challenges due to limited access to their technical details.</p> <p>This page explains our approach to modeling the environmental impacts of proprietary models and the potential sources of inaccuracy. We'll describe our methods for estimating model architectures and provide examples of how we make these estimations. Understanding these assumptions is crucial for users to interpret our results accurately and use EcoLogits with confidence.</p>"},{"location":"methodology/proprietary_models/#extrapolation-from-open-models","title":"Extrapolation from open models","text":"<p>Our methodology uses a bottom-up approach to model environmental impacts. This means we account for the physical infrastructure and resource consumption required to host generative AI services. Part of this modeling focuses on the AI models themselves, including:</p> <ul> <li>Model architecture (e.g., number of parameters)</li> <li>Deployment strategy and optimizations (e.g., backend types, quantization techniques)</li> </ul> <p>For open models, obtaining this information is straightforward as the models are publicly available. However, for proprietary models, we must rely on extrapolations from open model characteristics. For example, to assess the energy consumption of a request, we use GPU energy consumption benchmarks from open models and apply them to proprietary models accordingly.</p>"},{"location":"methodology/proprietary_models/#methodology-to-estimate-the-model-architecture","title":"Methodology to estimate the model architecture","text":"<p>For LLMs, key architectural aspects include whether the model is dense or a mixture of experts (MoE), and the model size (number of parameters). To estimate these characteristics for proprietary models, we leverage various information sources such as leaked data, evaluation benchmarks, and pricing information. This process is manual and may be updated as new information becomes available. Below are some examples of how we determine specific model architectures.</p>"},{"location":"methodology/proprietary_models/#from-leaked-data","title":"From leaked data","text":"<p>The model architecture of GPT-4 was leaked<sup>1</sup> and estimated to be a sparse mixture of experts model with a total of 1.8 trillion parameters and 16 experts of 111 billion parameters each. The number of active experts during inference could be around 2. We used this information to add GPT-4 to EcoLogits with a range of active experts between 2 and 8 (~half of the models), so active parameters are between 220B and 880B.</p>"},{"location":"methodology/proprietary_models/#from-evaluation-benchmarks","title":"From evaluation benchmarks","text":"<p>At the time of release Claude 3 Opus from Anthropic had about the same performance on general benchmarks such as MMLU or HelloSwag<sup>2</sup>. That is why we considered that Claude 3 Opus was about the same size of GPT-4. The model size is estimated to be about 2 trillion parameters<sup>3</sup>, and we also consider that it is a sparse mixture of experts, just like GPT-4, with between 250B and 1000B active parameters.</p>"},{"location":"methodology/proprietary_models/#from-pricing-information","title":"From pricing information","text":"<p>When OpenAI released GPT-4-Turbo we considered the model to be a distilled version of GPT-4 which justified the price drop<sup>4</sup>. We thus estimated that GPT-4-Turbo was a scaled down (2x when comparing output tokens price) architecture of GPT-4 and used the price difference to conclude that GPT-4-Turbo was a sparse mixture of experts of 880B total parameters and between 110B and 440B active parameters.</p>"},{"location":"methodology/proprietary_models/#transparency-and-updates","title":"Transparency and updates","text":"<p>To maintain transparency, we publish an additional document  detailing our estimations for model architectures. This document is regularly updated to stay in sync with new releases of EcoLogits.</p> <p>The full list of supported models is also available on GitHub .</p> <ol> <li> <p>Tweet from Yam Peleg (https://archive.ph/2RQ8X)\u00a0\u21a9</p> </li> <li> <p>Claude 3 family (https://www.anthropic.com/news/claude-3-family)\u00a0\u21a9</p> </li> <li> <p>LifeArchitect memo on Claude 3 Opus (https://lifearchitect.substack.com/p/the-memo-special-edition-claude-3)\u00a0\u21a9</p> </li> <li> <p>OpenAI GPT-4-Turbo official release (https://openai.com/index/new-models-and-developer-products-announced-at-devday/)\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>_ecologits</li> <li>electricity_mix_repository</li> <li>exceptions</li> <li>impacts<ul> <li>dag</li> <li>llm</li> <li>modeling</li> </ul> </li> <li>log</li> <li>model_repository</li> <li>status_messages</li> <li>tracers<ul> <li>anthropic_tracer</li> <li>cohere_tracer</li> <li>google_tracer</li> <li>huggingface_tracer</li> <li>litellm_tracer</li> <li>mistralai_tracer_v0</li> <li>mistralai_tracer_v1</li> <li>openai_tracer</li> <li>utils</li> </ul> </li> <li>utils<ul> <li>range_value</li> </ul> </li> </ul>"},{"location":"reference/_ecologits/","title":"_ecologits","text":""},{"location":"reference/_ecologits/#_ecologits.EcoLogits","title":"<code>EcoLogits</code>","text":"<p>EcoLogits instrumentor to initialize function patching for each provider.</p> <p>By default, the initialization will be done on all available and compatible providers that are supported by the library.</p> <p>Examples:</p> <p>EcoLogits initialization example with OpenAI. <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(f\"Energy consumption: {response.impacts.energy.value} kWh\")\nprint(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\")\n</code></pre></p>"},{"location":"reference/_ecologits/#_ecologits.EcoLogits.init","title":"<code>init(providers=None, electricity_mix_zone='WOR')</code>  <code>staticmethod</code>","text":"<p>Initialization static method. Will attempt to initialize all providers by default.</p> <p>Parameters:</p> Name Type Description Default <code>providers</code> <code>Optional[Union[str, list[str]]]</code> <p>list of providers to initialize (all providers by default).</p> <code>None</code> <code>electricity_mix_zone</code> <code>str</code> <p>ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).</p> <code>'WOR'</code> Source code in <code>ecologits/_ecologits.py</code> <pre><code>@staticmethod\ndef init(\n    providers: Optional[Union[str, list[str]]] = None,\n    electricity_mix_zone: str = \"WOR\",\n) -&gt; None:\n    \"\"\"\n    Initialization static method. Will attempt to initialize all providers by default.\n\n    Args:\n        providers: list of providers to initialize (all providers by default).\n        electricity_mix_zone: ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).\n    \"\"\"\n    if isinstance(providers, str):\n        providers = [providers]\n    if providers is None:\n        providers = list(_INSTRUMENTS.keys())\n\n    init_instruments(providers)\n\n    EcoLogits.config.electricity_mix_zone = electricity_mix_zone\n    EcoLogits.config.providers += providers\n    EcoLogits.config.providers = list(set(EcoLogits.config.providers))\n</code></pre>"},{"location":"reference/electricity_mix_repository/","title":"electricity_mix_repository","text":""},{"location":"reference/exceptions/","title":"exceptions","text":""},{"location":"reference/exceptions/#exceptions.TracerInitializationError","title":"<code>TracerInitializationError</code>","text":"<p>               Bases: <code>EcoLogitsError</code></p> <p>Tracer is initialized twice</p>"},{"location":"reference/exceptions/#exceptions.ModelingError","title":"<code>ModelingError</code>","text":"<p>               Bases: <code>EcoLogitsError</code></p> <p>Operation or computation not allowed</p>"},{"location":"reference/log/","title":"log","text":""},{"location":"reference/model_repository/","title":"model_repository","text":""},{"location":"reference/status_messages/","title":"status_messages","text":""},{"location":"reference/status_messages/#status_messages.WarningMessage","title":"<code>WarningMessage</code>","text":"<p>               Bases: <code>_StatusMessage</code></p> <p>Warning message.</p> <p>Attributes:</p> Name Type Description <code>code</code> <code>str</code> <p>Warning code.</p> <code>message</code> <code>str</code> <p>Warning message.</p>"},{"location":"reference/status_messages/#status_messages.ErrorMessage","title":"<code>ErrorMessage</code>","text":"<p>               Bases: <code>_StatusMessage</code></p> <p>Error message.</p> <p>Attributes:</p> Name Type Description <code>code</code> <code>str</code> <p>Error code.</p> <code>message</code> <code>str</code> <p>Error message.</p>"},{"location":"reference/impacts/dag/","title":"dag","text":""},{"location":"reference/impacts/llm/","title":"llm","text":""},{"location":"reference/impacts/llm/#impacts.llm.gpu_energy","title":"<code>gpu_energy(model_active_parameter_count, output_token_count, gpu_energy_alpha, gpu_energy_beta, gpu_energy_stdev)</code>","text":"<p>Compute energy consumption of a single GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>float</code> <p>Number of active parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>gpu_energy_alpha</code> <code>float</code> <p>Alpha parameter of the GPU linear power consumption profile.</p> required <code>gpu_energy_beta</code> <code>float</code> <p>Beta parameter of the GPU linear power consumption profile.</p> required <code>gpu_energy_stdev</code> <code>float</code> <p>Standard deviation of the GPU linear power consumption profile.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The 95% confidence interval of energy consumption of a single GPU in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef gpu_energy(\n        model_active_parameter_count: float,\n        output_token_count: float,\n        gpu_energy_alpha: float,\n        gpu_energy_beta: float,\n        gpu_energy_stdev: float\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute energy consumption of a single GPU.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        output_token_count: Number of generated tokens.\n        gpu_energy_alpha: Alpha parameter of the GPU linear power consumption profile.\n        gpu_energy_beta: Beta parameter of the GPU linear power consumption profile.\n        gpu_energy_stdev: Standard deviation of the GPU linear power consumption profile.\n\n    Returns:\n        The 95% confidence interval of energy consumption of a single GPU in kWh.\n    \"\"\"\n    gpu_energy_per_token_mean = gpu_energy_alpha * model_active_parameter_count + gpu_energy_beta\n    gpu_energy_min = output_token_count * (gpu_energy_per_token_mean - 1.96 * gpu_energy_stdev)\n    gpu_energy_max = output_token_count * (gpu_energy_per_token_mean + 1.96 * gpu_energy_stdev)\n    return RangeValue(min=max(0, gpu_energy_min), max=gpu_energy_max)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.generation_latency","title":"<code>generation_latency(model_active_parameter_count, output_token_count, gpu_latency_alpha, gpu_latency_beta, gpu_latency_stdev, request_latency)</code>","text":"<p>Compute the token generation latency in seconds.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>float</code> <p>Number of active parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>gpu_latency_alpha</code> <code>float</code> <p>Alpha parameter of the GPU linear latency profile.</p> required <code>gpu_latency_beta</code> <code>float</code> <p>Beta parameter of the GPU linear latency profile.</p> required <code>gpu_latency_stdev</code> <code>float</code> <p>Standard deviation of the GPU linear latency profile.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency (upper bound) in seconds.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The token generation latency in seconds.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef generation_latency(\n        model_active_parameter_count: float,\n        output_token_count: float,\n        gpu_latency_alpha: float,\n        gpu_latency_beta: float,\n        gpu_latency_stdev: float,\n        request_latency: float,\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the token generation latency in seconds.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        output_token_count: Number of generated tokens.\n        gpu_latency_alpha: Alpha parameter of the GPU linear latency profile.\n        gpu_latency_beta: Beta parameter of the GPU linear latency profile.\n        gpu_latency_stdev: Standard deviation of the GPU linear latency profile.\n        request_latency: Measured request latency (upper bound) in seconds.\n\n    Returns:\n        The token generation latency in seconds.\n    \"\"\"\n    gpu_latency_per_token_mean = gpu_latency_alpha * model_active_parameter_count + gpu_latency_beta\n    gpu_latency_min = output_token_count * (gpu_latency_per_token_mean - 1.96 * gpu_latency_stdev)\n    gpu_latency_max = output_token_count * (gpu_latency_per_token_mean + 1.96 * gpu_latency_stdev)\n    gpu_latency_interval = RangeValue(min=max(0, gpu_latency_min), max=gpu_latency_max)\n    if gpu_latency_interval &lt; request_latency:\n        return gpu_latency_interval\n    return request_latency\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.model_required_memory","title":"<code>model_required_memory(model_total_parameter_count, model_quantization_bits)</code>","text":"<p>Compute the required memory to load the model on GPU.</p> <p>Parameters:</p> Name Type Description Default <code>model_total_parameter_count</code> <code>float</code> <p>Number of parameters of the model.</p> required <code>model_quantization_bits</code> <code>int</code> <p>Number of bits used to represent the model weights.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The amount of required GPU memory to load the model.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef model_required_memory(\n        model_total_parameter_count: float,\n        model_quantization_bits: int,\n) -&gt; float:\n    \"\"\"\n    Compute the required memory to load the model on GPU.\n\n    Args:\n        model_total_parameter_count: Number of parameters of the model.\n        model_quantization_bits: Number of bits used to represent the model weights.\n\n    Returns:\n        The amount of required GPU memory to load the model.\n    \"\"\"\n    return 1.2 * model_total_parameter_count * model_quantization_bits / 8\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.gpu_required_count","title":"<code>gpu_required_count(model_required_memory, gpu_memory)</code>","text":"<p>Compute the number of required GPU to store the model.</p> <p>Parameters:</p> Name Type Description Default <code>model_required_memory</code> <code>float</code> <p>Required memory to load the model on GPU.</p> required <code>gpu_memory</code> <code>float</code> <p>Amount of memory available on a single GPU.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of required GPUs to load the model.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef gpu_required_count(\n        model_required_memory: float,\n        gpu_memory: float\n) -&gt; int:\n    \"\"\"\n    Compute the number of required GPU to store the model.\n\n    Args:\n        model_required_memory: Required memory to load the model on GPU.\n        gpu_memory: Amount of memory available on a single GPU.\n\n    Returns:\n        The number of required GPUs to load the model.\n    \"\"\"\n    return ceil(model_required_memory / gpu_memory)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_energy","title":"<code>server_energy(generation_latency, server_power, server_gpu_count, gpu_required_count)</code>","text":"<p>Compute the energy consumption of the server.</p> <p>Parameters:</p> Name Type Description Default <code>generation_latency</code> <code>float</code> <p>Token generation latency in seconds.</p> required <code>server_power</code> <code>float</code> <p>Power consumption of the server in kW.</p> required <code>server_gpu_count</code> <code>int</code> <p>Number of available GPUs in the server.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The energy consumption of the server (GPUs are not included) in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_energy(\n        generation_latency: float,\n        server_power: float,\n        server_gpu_count: int,\n        gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the energy consumption of the server.\n\n    Args:\n        generation_latency: Token generation latency in seconds.\n        server_power: Power consumption of the server in kW.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The energy consumption of the server (GPUs are not included) in kWh.\n    \"\"\"\n    return (generation_latency / 3600) * server_power * (gpu_required_count / server_gpu_count)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_energy","title":"<code>request_energy(datacenter_pue, server_energy, gpu_required_count, gpu_energy)</code>","text":"<p>Compute the energy consumption of the request.</p> <p>Parameters:</p> Name Type Description Default <code>datacenter_pue</code> <code>float</code> <p>PUE of the datacenter.</p> required <code>server_energy</code> <code>float</code> <p>Energy consumption of the server in kWh.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <code>gpu_energy</code> <code>ValueOrRange</code> <p>Energy consumption of a single GPU in kWh.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The energy consumption of the request in kWh.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_energy(\n        datacenter_pue: float,\n        server_energy: float,\n        gpu_required_count: int,\n        gpu_energy: ValueOrRange\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the energy consumption of the request.\n\n    Args:\n        datacenter_pue: PUE of the datacenter.\n        server_energy: Energy consumption of the server in kWh.\n        gpu_required_count: Number of required GPUs to load the model.\n        gpu_energy: Energy consumption of a single GPU in kWh.\n\n    Returns:\n        The energy consumption of the request in kWh.\n    \"\"\"\n    return datacenter_pue * (server_energy + gpu_required_count * gpu_energy)\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_gwp","title":"<code>request_usage_gwp(request_energy, if_electricity_mix_gwp)</code>","text":"<p>Compute the Global Warming Potential (GWP) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>ValueOrRange</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The GWP usage impact of the request in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_gwp(\n        request_energy: ValueOrRange,\n        if_electricity_mix_gwp: float\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n\n    Returns:\n        The GWP usage impact of the request in kgCO2eq.\n    \"\"\"\n    return request_energy * if_electricity_mix_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_adpe","title":"<code>request_usage_adpe(request_energy, if_electricity_mix_adpe)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>ValueOrRange</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption in kgSbeq / kWh.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The ADPe usage impact of the request in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_adpe(\n        request_energy: ValueOrRange,\n        if_electricity_mix_adpe: float\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption in kgSbeq / kWh.\n\n    Returns:\n        The ADPe usage impact of the request in kgSbeq.\n    \"\"\"\n    return request_energy * if_electricity_mix_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_usage_pe","title":"<code>request_usage_pe(request_energy, if_electricity_mix_pe)</code>","text":"<p>Compute the Primary Energy (PE) usage impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>request_energy</code> <code>ValueOrRange</code> <p>Energy consumption of the request in kWh.</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The PE usage impact of the request in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_usage_pe(\n        request_energy: ValueOrRange,\n        if_electricity_mix_pe: float\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Primary Energy (PE) usage impact of the request.\n\n    Args:\n        request_energy: Energy consumption of the request in kWh.\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n\n    Returns:\n        The PE usage impact of the request in MJ.\n    \"\"\"\n    return request_energy * if_electricity_mix_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_gwp","title":"<code>server_gpu_embodied_gwp(server_embodied_gwp, server_gpu_count, gpu_embodied_gwp, gpu_required_count)</code>","text":"<p>Compute the Global Warming Potential (GWP) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of the server in kgCO2eq.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of a single GPU in kgCO2eq.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The GWP embodied impact of the server and the GPUs in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_gwp(\n        server_embodied_gwp: float,\n        server_gpu_count: float,\n        gpu_embodied_gwp: float,\n        gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) embodied impact of the server\n\n    Args:\n        server_embodied_gwp: GWP embodied impact of the server in kgCO2eq.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_gwp: GWP embodied impact of a single GPU in kgCO2eq.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The GWP embodied impact of the server and the GPUs in kgCO2eq.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_gwp + gpu_required_count * gpu_embodied_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_adpe","title":"<code>server_gpu_embodied_adpe(server_embodied_adpe, server_gpu_count, gpu_embodied_adpe, gpu_required_count)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of the server in kgSbeq.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of a single GPU in kgSbeq.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The ADPe embodied impact of the server and the GPUs in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_adpe(\n        server_embodied_adpe: float,\n        server_gpu_count: float,\n        gpu_embodied_adpe: float,\n        gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the server\n\n    Args:\n        server_embodied_adpe: ADPe embodied impact of the server in kgSbeq.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_adpe: ADPe embodied impact of a single GPU in kgSbeq.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The ADPe embodied impact of the server and the GPUs in kgSbeq.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_adpe + gpu_required_count * gpu_embodied_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.server_gpu_embodied_pe","title":"<code>server_gpu_embodied_pe(server_embodied_pe, server_gpu_count, gpu_embodied_pe, gpu_required_count)</code>","text":"<p>Compute the Primary Energy (PE) embodied impact of the server</p> <p>Parameters:</p> Name Type Description Default <code>server_embodied_pe</code> <code>float</code> <p>PE embodied impact of the server in MJ.</p> required <code>server_gpu_count</code> <code>float</code> <p>Number of available GPUs in the server.</p> required <code>gpu_embodied_pe</code> <code>float</code> <p>PE embodied impact of a single GPU in MJ.</p> required <code>gpu_required_count</code> <code>int</code> <p>Number of required GPUs to load the model.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The PE embodied impact of the server and the GPUs in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef server_gpu_embodied_pe(\n        server_embodied_pe: float,\n        server_gpu_count: float,\n        gpu_embodied_pe: float,\n        gpu_required_count: int\n) -&gt; float:\n    \"\"\"\n    Compute the Primary Energy (PE) embodied impact of the server\n\n    Args:\n        server_embodied_pe: PE embodied impact of the server in MJ.\n        server_gpu_count: Number of available GPUs in the server.\n        gpu_embodied_pe: PE embodied impact of a single GPU in MJ.\n        gpu_required_count: Number of required GPUs to load the model.\n\n    Returns:\n        The PE embodied impact of the server and the GPUs in MJ.\n    \"\"\"\n    return (gpu_required_count / server_gpu_count) * server_embodied_pe + gpu_required_count * gpu_embodied_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_gwp","title":"<code>request_embodied_gwp(server_gpu_embodied_gwp, server_lifetime, generation_latency)</code>","text":"<p>Compute the Global Warming Potential (GWP) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_gwp</code> <code>float</code> <p>GWP embodied impact of the server and the GPUs in kgCO2eq.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>ValueOrRange</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The GWP embodied impact of the request in kgCO2eq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_gwp(\n        server_gpu_embodied_gwp: float,\n        server_lifetime: float,\n        generation_latency: ValueOrRange\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Global Warming Potential (GWP) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_gwp: GWP embodied impact of the server and the GPUs in kgCO2eq.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The GWP embodied impact of the request in kgCO2eq.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_gwp\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_adpe","title":"<code>request_embodied_adpe(server_gpu_embodied_adpe, server_lifetime, generation_latency)</code>","text":"<p>Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_adpe</code> <code>float</code> <p>ADPe embodied impact of the server and the GPUs in kgSbeq.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>ValueOrRange</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The ADPe embodied impact of the request in kgSbeq.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_adpe(\n        server_gpu_embodied_adpe: float,\n        server_lifetime: float,\n        generation_latency: ValueOrRange\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Abiotic Depletion Potential for Elements (ADPe) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_adpe: ADPe embodied impact of the server and the GPUs in kgSbeq.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The ADPe embodied impact of the request in kgSbeq.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_adpe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.request_embodied_pe","title":"<code>request_embodied_pe(server_gpu_embodied_pe, server_lifetime, generation_latency)</code>","text":"<p>Compute the Primary Energy (PE) embodied impact of the request.</p> <p>Parameters:</p> Name Type Description Default <code>server_gpu_embodied_pe</code> <code>float</code> <p>PE embodied impact of the server and the GPUs in MJ.</p> required <code>server_lifetime</code> <code>float</code> <p>Lifetime duration of the server in seconds.</p> required <code>generation_latency</code> <code>ValueOrRange</code> <p>Token generation latency in seconds.</p> required <p>Returns:</p> Type Description <code>ValueOrRange</code> <p>The PE embodied impact of the request in MJ.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>@dag.asset\ndef request_embodied_pe(\n        server_gpu_embodied_pe: float,\n        server_lifetime: float,\n        generation_latency: ValueOrRange\n) -&gt; ValueOrRange:\n    \"\"\"\n    Compute the Primary Energy (PE) embodied impact of the request.\n\n    Args:\n        server_gpu_embodied_pe: PE embodied impact of the server and the GPUs in MJ.\n        server_lifetime: Lifetime duration of the server in seconds.\n        generation_latency: Token generation latency in seconds.\n\n    Returns:\n        The PE embodied impact of the request in MJ.\n    \"\"\"\n    return (generation_latency / server_lifetime) * server_gpu_embodied_pe\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.compute_llm_impacts_dag","title":"<code>compute_llm_impacts_dag(model_active_parameter_count, model_total_parameter_count, output_token_count, request_latency, if_electricity_mix_adpe, if_electricity_mix_pe, if_electricity_mix_gwp, model_quantization_bits=MODEL_QUANTIZATION_BITS, gpu_energy_alpha=GPU_ENERGY_ALPHA, gpu_energy_beta=GPU_ENERGY_BETA, gpu_energy_stdev=GPU_ENERGY_STDEV, gpu_latency_alpha=GPU_LATENCY_ALPHA, gpu_latency_beta=GPU_LATENCY_BETA, gpu_latency_stdev=GPU_LATENCY_STDEV, gpu_memory=GPU_MEMORY, gpu_embodied_gwp=GPU_EMBODIED_IMPACT_GWP, gpu_embodied_adpe=GPU_EMBODIED_IMPACT_ADPE, gpu_embodied_pe=GPU_EMBODIED_IMPACT_PE, server_gpu_count=SERVER_GPUS, server_power=SERVER_POWER, server_embodied_gwp=SERVER_EMBODIED_IMPACT_GWP, server_embodied_adpe=SERVER_EMBODIED_IMPACT_ADPE, server_embodied_pe=SERVER_EMBODIED_IMPACT_PE, server_lifetime=HARDWARE_LIFESPAN, datacenter_pue=DATACENTER_PUE)</code>","text":"<p>Compute the impacts dag of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>ValueOrRange</code> <p>Number of active parameters of the model.</p> required <code>model_total_parameter_count</code> <code>ValueOrRange</code> <p>Number of parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency in seconds.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <code>model_quantization_bits</code> <code>Optional[int]</code> <p>Number of bits used to represent the model weights.</p> <code>MODEL_QUANTIZATION_BITS</code> <code>gpu_energy_alpha</code> <code>Optional[float]</code> <p>Alpha parameter of the GPU linear power consumption profile.</p> <code>GPU_ENERGY_ALPHA</code> <code>gpu_energy_beta</code> <code>Optional[float]</code> <p>Beta parameter of the GPU linear power consumption profile.</p> <code>GPU_ENERGY_BETA</code> <code>gpu_energy_stdev</code> <code>Optional[float]</code> <p>Standard deviation of the GPU linear power consumption profile.</p> <code>GPU_ENERGY_STDEV</code> <code>gpu_latency_alpha</code> <code>Optional[float]</code> <p>Alpha parameter of the GPU linear latency profile.</p> <code>GPU_LATENCY_ALPHA</code> <code>gpu_latency_beta</code> <code>Optional[float]</code> <p>Beta parameter of the GPU linear latency profile.</p> <code>GPU_LATENCY_BETA</code> <code>gpu_latency_stdev</code> <code>Optional[float]</code> <p>Standard deviation of the GPU linear latency profile.</p> <code>GPU_LATENCY_STDEV</code> <code>gpu_memory</code> <code>Optional[float]</code> <p>Amount of memory available on a single GPU.</p> <code>GPU_MEMORY</code> <code>gpu_embodied_gwp</code> <code>Optional[float]</code> <p>GWP embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_GWP</code> <code>gpu_embodied_adpe</code> <code>Optional[float]</code> <p>ADPe embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_ADPE</code> <code>gpu_embodied_pe</code> <code>Optional[float]</code> <p>PE embodied impact of a single GPU.</p> <code>GPU_EMBODIED_IMPACT_PE</code> <code>server_gpu_count</code> <code>Optional[int]</code> <p>Number of available GPUs in the server.</p> <code>SERVER_GPUS</code> <code>server_power</code> <code>Optional[float]</code> <p>Power consumption of the server in kW.</p> <code>SERVER_POWER</code> <code>server_embodied_gwp</code> <code>Optional[float]</code> <p>GWP embodied impact of the server in kgCO2eq.</p> <code>SERVER_EMBODIED_IMPACT_GWP</code> <code>server_embodied_adpe</code> <code>Optional[float]</code> <p>ADPe embodied impact of the server in kgSbeq.</p> <code>SERVER_EMBODIED_IMPACT_ADPE</code> <code>server_embodied_pe</code> <code>Optional[float]</code> <p>PE embodied impact of the server in MJ.</p> <code>SERVER_EMBODIED_IMPACT_PE</code> <code>server_lifetime</code> <code>Optional[float]</code> <p>Lifetime duration of the server in seconds.</p> <code>HARDWARE_LIFESPAN</code> <code>datacenter_pue</code> <code>Optional[float]</code> <p>PUE of the datacenter.</p> <code>DATACENTER_PUE</code> <p>Returns:</p> Type Description <code>dict[str, ValueOrRange]</code> <p>The impacts dag with all intermediate states.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>def compute_llm_impacts_dag(\n        model_active_parameter_count: ValueOrRange,\n        model_total_parameter_count: ValueOrRange,\n        output_token_count: float,\n        request_latency: float,\n        if_electricity_mix_adpe: float,\n        if_electricity_mix_pe: float,\n        if_electricity_mix_gwp: float,\n        model_quantization_bits: Optional[int] = MODEL_QUANTIZATION_BITS,\n        gpu_energy_alpha: Optional[float] = GPU_ENERGY_ALPHA,\n        gpu_energy_beta: Optional[float] = GPU_ENERGY_BETA,\n        gpu_energy_stdev: Optional[float] = GPU_ENERGY_STDEV,\n        gpu_latency_alpha: Optional[float] = GPU_LATENCY_ALPHA,\n        gpu_latency_beta: Optional[float] = GPU_LATENCY_BETA,\n        gpu_latency_stdev: Optional[float] = GPU_LATENCY_STDEV,\n        gpu_memory: Optional[float] = GPU_MEMORY,\n        gpu_embodied_gwp: Optional[float] = GPU_EMBODIED_IMPACT_GWP,\n        gpu_embodied_adpe: Optional[float] = GPU_EMBODIED_IMPACT_ADPE,\n        gpu_embodied_pe: Optional[float] = GPU_EMBODIED_IMPACT_PE,\n        server_gpu_count: Optional[int] = SERVER_GPUS,\n        server_power: Optional[float] = SERVER_POWER,\n        server_embodied_gwp: Optional[float] = SERVER_EMBODIED_IMPACT_GWP,\n        server_embodied_adpe: Optional[float] = SERVER_EMBODIED_IMPACT_ADPE,\n        server_embodied_pe: Optional[float] = SERVER_EMBODIED_IMPACT_PE,\n        server_lifetime: Optional[float] = HARDWARE_LIFESPAN,\n        datacenter_pue: Optional[float] = DATACENTER_PUE,\n) -&gt; dict[str, ValueOrRange]:\n    \"\"\"\n    Compute the impacts dag of an LLM generation request.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        model_total_parameter_count: Number of parameters of the model.\n        output_token_count: Number of generated tokens.\n        request_latency: Measured request latency in seconds.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n        model_quantization_bits: Number of bits used to represent the model weights.\n        gpu_energy_alpha: Alpha parameter of the GPU linear power consumption profile.\n        gpu_energy_beta: Beta parameter of the GPU linear power consumption profile.\n        gpu_energy_stdev: Standard deviation of the GPU linear power consumption profile.\n        gpu_latency_alpha: Alpha parameter of the GPU linear latency profile.\n        gpu_latency_beta: Beta parameter of the GPU linear latency profile.\n        gpu_latency_stdev: Standard deviation of the GPU linear latency profile.\n        gpu_memory: Amount of memory available on a single GPU.\n        gpu_embodied_gwp: GWP embodied impact of a single GPU.\n        gpu_embodied_adpe: ADPe embodied impact of a single GPU.\n        gpu_embodied_pe: PE embodied impact of a single GPU.\n        server_gpu_count: Number of available GPUs in the server.\n        server_power: Power consumption of the server in kW.\n        server_embodied_gwp: GWP embodied impact of the server in kgCO2eq.\n        server_embodied_adpe: ADPe embodied impact of the server in kgSbeq.\n        server_embodied_pe: PE embodied impact of the server in MJ.\n        server_lifetime: Lifetime duration of the server in seconds.\n        datacenter_pue: PUE of the datacenter.\n\n    Returns:\n        The impacts dag with all intermediate states.\n    \"\"\"\n    results = dag.execute(\n        model_active_parameter_count=model_active_parameter_count,\n        model_total_parameter_count=model_total_parameter_count,\n        model_quantization_bits=model_quantization_bits,\n        output_token_count=output_token_count,\n        request_latency=request_latency,\n        if_electricity_mix_gwp=if_electricity_mix_gwp,\n        if_electricity_mix_adpe=if_electricity_mix_adpe,\n        if_electricity_mix_pe=if_electricity_mix_pe,\n        gpu_energy_alpha=gpu_energy_alpha,\n        gpu_energy_beta=gpu_energy_beta,\n        gpu_energy_stdev=gpu_energy_stdev,\n        gpu_latency_alpha=gpu_latency_alpha,\n        gpu_latency_beta=gpu_latency_beta,\n        gpu_latency_stdev=gpu_latency_stdev,\n        gpu_memory=gpu_memory,\n        gpu_embodied_gwp=gpu_embodied_gwp,\n        gpu_embodied_adpe=gpu_embodied_adpe,\n        gpu_embodied_pe=gpu_embodied_pe,\n        server_gpu_count=server_gpu_count,\n        server_power=server_power,\n        server_embodied_gwp=server_embodied_gwp,\n        server_embodied_adpe=server_embodied_adpe,\n        server_embodied_pe=server_embodied_pe,\n        server_lifetime=server_lifetime,\n        datacenter_pue=datacenter_pue,\n    )\n    return results\n</code></pre>"},{"location":"reference/impacts/llm/#impacts.llm.compute_llm_impacts","title":"<code>compute_llm_impacts(model_active_parameter_count, model_total_parameter_count, output_token_count, if_electricity_mix_adpe, if_electricity_mix_pe, if_electricity_mix_gwp, request_latency=None, **kwargs)</code>","text":"<p>Compute the impacts of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>model_active_parameter_count</code> <code>ValueOrRange</code> <p>Number of active parameters of the model.</p> required <code>model_total_parameter_count</code> <code>ValueOrRange</code> <p>Number of total parameters of the model.</p> required <code>output_token_count</code> <code>float</code> <p>Number of generated tokens.</p> required <code>if_electricity_mix_adpe</code> <code>float</code> <p>ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).</p> required <code>if_electricity_mix_pe</code> <code>float</code> <p>PE impact factor of electricity consumption in MJ / kWh.</p> required <code>if_electricity_mix_gwp</code> <code>float</code> <p>GWP impact factor of electricity consumption in kgCO2eq / kWh.</p> required <code>request_latency</code> <code>Optional[float]</code> <p>Measured request latency in seconds.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Any other optional parameter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Impacts</code> <p>The impacts of an LLM generation request.</p> Source code in <code>ecologits/impacts/llm.py</code> <pre><code>def compute_llm_impacts( # noqa: PLR0912\n        model_active_parameter_count: ValueOrRange,\n        model_total_parameter_count: ValueOrRange,\n        output_token_count: float,\n        if_electricity_mix_adpe: float,\n        if_electricity_mix_pe: float,\n        if_electricity_mix_gwp: float,\n        request_latency: Optional[float] = None,\n        **kwargs: Any\n) -&gt; Impacts:\n    \"\"\"\n    Compute the impacts of an LLM generation request.\n\n    Args:\n        model_active_parameter_count: Number of active parameters of the model.\n        model_total_parameter_count: Number of total parameters of the model.\n        output_token_count: Number of generated tokens.\n        if_electricity_mix_adpe: ADPe impact factor of electricity consumption of kgSbeq / kWh (Antimony).\n        if_electricity_mix_pe: PE impact factor of electricity consumption in MJ / kWh.\n        if_electricity_mix_gwp: GWP impact factor of electricity consumption in kgCO2eq / kWh.\n        request_latency: Measured request latency in seconds.\n        **kwargs: Any other optional parameter.\n\n    Returns:\n        The impacts of an LLM generation request.\n    \"\"\"\n    if request_latency is None:\n        request_latency = math.inf\n\n    active_params = [model_active_parameter_count]\n    total_params = [model_total_parameter_count]\n\n    if isinstance(model_active_parameter_count, RangeValue) or isinstance(model_total_parameter_count, RangeValue):\n        if isinstance(model_active_parameter_count, RangeValue):\n            active_params = [model_active_parameter_count.min, model_active_parameter_count.max]\n        else:\n            active_params = [model_active_parameter_count, model_active_parameter_count]\n        if isinstance(model_total_parameter_count, RangeValue):\n            total_params = [model_total_parameter_count.min, model_total_parameter_count.max]\n        else:\n            total_params = [model_total_parameter_count, model_total_parameter_count]\n\n    results: dict[str, Union[RangeValue, float, int]] = {}\n    fields = [\"request_energy\", \"request_usage_gwp\", \"request_usage_adpe\", \"request_usage_pe\",\n              \"request_embodied_gwp\", \"request_embodied_adpe\", \"request_embodied_pe\"]\n\n    for act_param, tot_param in zip(active_params, total_params):\n        res = compute_llm_impacts_dag(\n            model_active_parameter_count=act_param,\n            model_total_parameter_count=tot_param,\n            output_token_count=output_token_count,\n            request_latency=request_latency,\n            if_electricity_mix_adpe=if_electricity_mix_adpe,\n            if_electricity_mix_pe=if_electricity_mix_pe,\n            if_electricity_mix_gwp=if_electricity_mix_gwp,\n            **kwargs\n        )\n        for field in fields:\n            if field in results:\n                min_result = results[field]\n                max_result = res[field]\n                if isinstance(min_result, RangeValue) and isinstance(max_result, RangeValue):\n                    min_value = cast(Union[float, int], min_result.min)\n                    max_value = cast(Union[float, int], max_result.max)\n                    results[field] = RangeValue(min=min_value, max=max_value)\n                elif isinstance(min_result, (float, int)):\n                    min_value = cast(Union[float, int], min_result)\n                    max_value = cast(Union[float, int], max_result)\n                    results[field] = RangeValue(min=min_value, max=max_value)\n                else:\n                    raise TypeError(\n                        \"Unexpected behaviour. With different parameters, DAG does not return the same type\"\n                    )\n            else:\n                results[field] = res[field]\n\n    energy = Energy(value=results[\"request_energy\"])\n    gwp_usage = GWP(value=results[\"request_usage_gwp\"])\n    adpe_usage = ADPe(value=results[\"request_usage_adpe\"])\n    pe_usage = PE(value=results[\"request_usage_pe\"])\n    gwp_embodied = GWP(value=results[\"request_embodied_gwp\"])\n    adpe_embodied = ADPe(value=results[\"request_embodied_adpe\"])\n    pe_embodied = PE(value=results[\"request_embodied_pe\"])\n    return Impacts(\n        energy=energy,\n        gwp=gwp_usage + gwp_embodied,\n        adpe=adpe_usage + adpe_embodied,\n        pe=pe_usage + pe_embodied,\n        usage=Usage(\n            energy=energy,\n            gwp=gwp_usage,\n            adpe=adpe_usage,\n            pe=pe_usage\n        ),\n        embodied=Embodied(\n            gwp=gwp_embodied,\n            adpe=adpe_embodied,\n            pe=pe_embodied\n        )\n    )\n</code></pre>"},{"location":"reference/impacts/modeling/","title":"modeling","text":""},{"location":"reference/impacts/modeling/#impacts.modeling.BaseImpact","title":"<code>BaseImpact</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base impact data model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Impact type.</p> <code>name</code> <code>str</code> <p>Impact name.</p> <code>value</code> <code>ValueOrRange</code> <p>Impact value.</p> <code>unit</code> <code>str</code> <p>Impact unit.</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Energy","title":"<code>Energy</code>","text":"<p>               Bases: <code>BaseImpact</code></p> <p>Energy consumption.</p> Info <p>Final energy consumption \"measured from the plug\".</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>energy</p> <code>name</code> <code>str</code> <p>Energy</p> <code>value</code> <code>ValueOrRange</code> <p>Energy value</p> <code>unit</code> <code>str</code> <p>Kilowatt-hour (kWh)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.GWP","title":"<code>GWP</code>","text":"<p>               Bases: <code>BaseImpact</code></p> <p>Global Warming Potential (GWP) impact.</p> Info <p>Also, commonly known as GHG/carbon emissions.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>GWP</p> <code>name</code> <code>str</code> <p>Global Warming Potential</p> <code>value</code> <code>ValueOrRange</code> <p>GWP value</p> <code>unit</code> <code>str</code> <p>Kilogram Carbon Dioxide Equivalent (kgCO2eq)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.ADPe","title":"<code>ADPe</code>","text":"<p>               Bases: <code>BaseImpact</code></p> <p>Abiotic Depletion Potential for Elements (ADPe) impact.</p> Info <p>Impact on the depletion of non-living resources such as minerals or metals.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>ADPe</p> <code>name</code> <code>str</code> <p>Abiotic Depletion Potential (elements)</p> <code>value</code> <code>ValueOrRange</code> <p>ADPe value</p> <code>unit</code> <code>str</code> <p>Kilogram Antimony Equivalent (kgSbeq)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.PE","title":"<code>PE</code>","text":"<p>               Bases: <code>BaseImpact</code></p> <p>Primary Energy (PE) impact.</p> Info <p>Total energy consumed from primary sources.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>PE</p> <code>name</code> <code>str</code> <p>Primary Energy</p> <code>value</code> <code>ValueOrRange</code> <p>PE value</p> <code>unit</code> <code>str</code> <p>Megajoule (MJ)</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Phase","title":"<code>Phase</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base impact phase data model.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Phase type.</p> <code>name</code> <code>str</code> <p>Phase name.</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Usage","title":"<code>Usage</code>","text":"<p>               Bases: <code>Phase</code></p> <p>Usage impacts data model.</p> Info <p>Represents the phase of energy consumption during model execution.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>usage</p> <code>name</code> <code>str</code> <p>Usage</p> <code>energy</code> <code>Energy</code> <p>Energy consumption</p> <code>gwp</code> <code>GWP</code> <p>Global Warming Potential (GWP) usage impact</p> <code>adpe</code> <code>ADPe</code> <p>Abiotic Depletion Potential for Elements (ADPe) usage impact</p> <code>pe</code> <code>PE</code> <p>Primary Energy (PE) usage impact</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Embodied","title":"<code>Embodied</code>","text":"<p>               Bases: <code>Phase</code></p> <p>Embodied impacts data model.</p> Info <p>Encompasses resource extraction, manufacturing, and transportation phases associated with the model's lifecycle.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>embodied</p> <code>name</code> <code>str</code> <p>Embodied</p> <code>gwp</code> <code>GWP</code> <p>Global Warming Potential (GWP) embodied impact</p> <code>adpe</code> <code>ADPe</code> <p>Abiotic Depletion Potential for Elements (ADPe) embodied impact</p> <code>pe</code> <code>PE</code> <p>Primary Energy (PE) embodied impact</p>"},{"location":"reference/impacts/modeling/#impacts.modeling.Impacts","title":"<code>Impacts</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Impacts data model.</p> <p>Attributes:</p> Name Type Description <code>energy</code> <code>Energy</code> <p>Total energy consumption</p> <code>gwp</code> <code>GWP</code> <p>Total Global Warming Potential (GWP) impact</p> <code>adpe</code> <code>ADPe</code> <p>Total Abiotic Depletion Potential for Elements (ADPe) impact</p> <code>pe</code> <code>PE</code> <p>Total Primary Energy (PE) impact</p> <code>usage</code> <code>Usage</code> <p>Impacts for the usage phase</p> <code>embodied</code> <code>Embodied</code> <p>Impacts for the embodied phase</p>"},{"location":"reference/tracers/anthropic_tracer/","title":"anthropic_tracer","text":""},{"location":"reference/tracers/cohere_tracer/","title":"cohere_tracer","text":""},{"location":"reference/tracers/google_tracer/","title":"google_tracer","text":""},{"location":"reference/tracers/huggingface_tracer/","title":"huggingface_tracer","text":""},{"location":"reference/tracers/litellm_tracer/","title":"litellm_tracer","text":""},{"location":"reference/tracers/litellm_tracer/#tracers.litellm_tracer.litellm_match_model","title":"<code>litellm_match_model(model_name)</code>","text":"<p>Match according provider and model from a litellm model_name.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model as used in litellm.</p> required <p>Returns:</p> Type Description <code>Optional[tuple[str, str]]</code> <p>A tuple (provider, model_name) matching a record of the ModelRepository.</p> Source code in <code>ecologits/tracers/litellm_tracer.py</code> <pre><code>def litellm_match_model(model_name: str) -&gt; Optional[tuple[str, str]]:\n    \"\"\"\n    Match according provider and model from a litellm model_name.\n\n    Args:\n        model_name: Name of the model as used in litellm.\n\n    Returns:\n        A tuple (provider, model_name) matching a record of the ModelRepository.\n    \"\"\"\n    candidate = process.extractOne(\n        query=model_name,\n        choices=_model_choices,\n        scorer=fuzz.token_sort_ratio,\n        score_cutoff=51\n    )\n    if candidate is not None:\n        provider, model_name = candidate[0].split(\"/\", 1)\n        return provider, model_name\n    return None\n</code></pre>"},{"location":"reference/tracers/mistralai_tracer_v0/","title":"mistralai_tracer_v0","text":""},{"location":"reference/tracers/mistralai_tracer_v1/","title":"mistralai_tracer_v1","text":""},{"location":"reference/tracers/openai_tracer/","title":"openai_tracer","text":""},{"location":"reference/tracers/utils/","title":"utils","text":""},{"location":"reference/tracers/utils/#tracers.utils.ImpactsOutput","title":"<code>ImpactsOutput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Impacts output data model.</p> <p>Attributes:</p> Name Type Description <code>energy</code> <code>Optional[Energy]</code> <p>Total energy consumption</p> <code>gwp</code> <code>Optional[GWP]</code> <p>Total Global Warming Potential (GWP) impact</p> <code>adpe</code> <code>Optional[ADPe]</code> <p>Total Abiotic Depletion Potential for Elements (ADPe) impact</p> <code>pe</code> <code>Optional[PE]</code> <p>Total Primary Energy (PE) impact</p> <code>usage</code> <code>Optional[Usage]</code> <p>Impacts for the usage phase</p> <code>embodied</code> <code>Optional[Embodied]</code> <p>Impacts for the embodied phase</p> <code>warnings</code> <code>Optional[list[WarningMessage]]</code> <p>List of warnings</p> <code>errors</code> <code>Optional[list[ErrorMessage]]</code> <p>List of errors</p>"},{"location":"reference/tracers/utils/#tracers.utils.llm_impacts","title":"<code>llm_impacts(provider, model_name, output_token_count, request_latency, electricity_mix_zone='WOR')</code>","text":"<p>High-level function to compute the impacts of an LLM generation request.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str</code> <p>Name of the provider.</p> required <code>model_name</code> <code>str</code> <p>Name of the LLM used.</p> required <code>output_token_count</code> <code>int</code> <p>Number of generated tokens.</p> required <code>request_latency</code> <code>float</code> <p>Measured request latency in seconds.</p> required <code>electricity_mix_zone</code> <code>str</code> <p>ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).</p> <code>'WOR'</code> <p>Returns:</p> Type Description <code>ImpactsOutput</code> <p>The impacts of an LLM generation request.</p> Source code in <code>ecologits/tracers/utils.py</code> <pre><code>def llm_impacts(\n    provider: str,\n    model_name: str,\n    output_token_count: int,\n    request_latency: float,\n    electricity_mix_zone: str = \"WOR\",\n) -&gt; ImpactsOutput:\n    \"\"\"\n    High-level function to compute the impacts of an LLM generation request.\n\n    Args:\n        provider: Name of the provider.\n        model_name: Name of the LLM used.\n        output_token_count: Number of generated tokens.\n        request_latency: Measured request latency in seconds.\n        electricity_mix_zone: ISO 3166-1 alpha-3 code of the electricity mix zone (WOR by default).\n\n    Returns:\n        The impacts of an LLM generation request.\n    \"\"\"\n\n    model = models.find_model(provider=provider, model_name=model_name)\n    if model is None:\n        error = ModelNotRegisteredError(message=f\"Could not find model `{model_name}` for {provider} provider.\")\n        logger.warning_once(str(error))\n        return ImpactsOutput(errors=[error])\n\n    if isinstance(model.architecture.parameters, ParametersMoE):\n        model_total_params = model.architecture.parameters.total\n        model_active_params = model.architecture.parameters.active\n    else:\n        model_total_params = model.architecture.parameters\n        model_active_params = model.architecture.parameters\n\n    electricity_mix = electricity_mixes.find_electricity_mix(zone=electricity_mix_zone)\n    if electricity_mix is None:\n        error = ZoneNotRegisteredError(message=f\"Could not find electricity mix for `{electricity_mix_zone}` zone.\")\n        logger.warning_once(str(error))\n        return ImpactsOutput(errors=[error])\n\n    if_electricity_mix_adpe=electricity_mix.adpe\n    if_electricity_mix_pe=electricity_mix.pe\n    if_electricity_mix_gwp=electricity_mix.gwp\n    impacts = compute_llm_impacts(\n        model_active_parameter_count=model_active_params,\n        model_total_parameter_count=model_total_params,\n        output_token_count=output_token_count,\n        request_latency=request_latency,\n        if_electricity_mix_adpe=if_electricity_mix_adpe,\n        if_electricity_mix_pe=if_electricity_mix_pe,\n        if_electricity_mix_gwp=if_electricity_mix_gwp,\n    )\n    impacts = ImpactsOutput.model_validate(impacts.model_dump())\n\n    if model.has_warnings:\n        for w in model.warnings:\n            logger.warning_once(str(w))\n            impacts.add_warning(w)\n\n    return impacts\n</code></pre>"},{"location":"reference/utils/range_value/","title":"range_value","text":""},{"location":"reference/utils/range_value/#utils.range_value.RangeValue","title":"<code>RangeValue</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>RangeValue data model to represent intervals.</p> <p>Attributes:</p> Name Type Description <code>min</code> <code>Union[float, int]</code> <p>Lower bound of the interval.</p> <code>max</code> <code>Union[float, int]</code> <p>Upper bound of the interval.</p>"},{"location":"tutorial/","title":"Tutorial","text":"<p>The  EcoLogits library tracks the energy consumption and environmental impacts of generative AI models accessed through APIs and their official client libraries. </p> <p>It achieves this by patching the Python client libraries, ensuring that each API request is wrapped with an impact calculation function. This function computes the environmental impact based on several request features, such as the chosen model, the number of tokens generated, and the request's latency. The resulting data is then encapsulated in an <code>ImpactsOutput</code> object, which is added to the response, containing the environmental impacts for a specific request.</p> <ul> <li> <p> Set up in 5 minutes</p> <p>Install <code>ecologits</code> with <code>pip</code> and get up and running in minutes.</p> <p> Getting started</p> </li> <li> <p> Environmental impacts</p> <p>Understand what environmental impacts and phases are reported.  </p> <p> Tutorial</p> </li> <li> <p> Supported providers</p> <p>List of providers and tutorials on how to make requests.</p> <p> Providers</p> </li> <li> <p> Methodology</p> <p>Understand how we estimate environmental impacts.</p> <p> Methodology</p> </li> </ul>"},{"location":"tutorial/#initialization-of-ecologits","title":"Initialization of EcoLogits","text":"<p>To use EcoLogits in your projects, you will need to initialize the client tracers that are used internally to intercept and enrich responses. The default initialization will use default parameters and enable tracking of all available providers. To change that behaviour, read along on how to configure EcoLogits.</p> <pre><code>from ecologits import EcoLogits\n\n# Default initialization method\nEcoLogits.init()\n</code></pre>"},{"location":"tutorial/#configure-providers","title":"Configure providers","text":"<p>You can select which provider to enable with EcoLogits using the <code>providers</code> parameter.</p> <p>Default behavior is to enable all available providers.</p> Select a providers to enable<pre><code>from ecologits import EcoLogits\n\n# Examples on how to enable one or multiple providers\nEcoLogits.init(providers=\"openai\")\nEcoLogits.init(providers=[\"anthropic\", \"mistralai\"])\n</code></pre> Disabling a provider at runtime is not supported <p>It is currently not possible to dynamically activate and deactivate a provider at runtime. Each time that <code>EcoLogits</code> is re-initialized with another providers, the latter will be added to the list of already initialized providers. If you think that un-initializing a provider could be necessary for your use case, please open an issue .\"</p>"},{"location":"tutorial/#configure-electricity-mix","title":"Configure electricity mix","text":"<p>You can change the electricity mix  for server-side computation depending on a specific location. EcoLogits will automatically change the default impact factors for electricity consumption according to the selected zone. </p> <p>Available zones are listed in the electricity_mixes.csv  file and are based on the ISO 3166-1 alpha-3  convention with some extras like <code>WOR</code> for World or <code>EEE</code> for Europe. </p> <p>Electricity mixes for each geographic zone are sourced from the ADEME Base Empreinte\u00ae  database and are based on yearly averages.</p> <p>Default electricity mix zone is <code>WOR</code> for World.</p> Select a different electricity mix<pre><code>from ecologits import EcoLogits\n\n# Select the electricity mix of France\nEcoLogits.init(electricity_mix_zone=\"FRA\")\n</code></pre>"},{"location":"tutorial/impacts/","title":"Environmental Impacts","text":"<p>Environmental impacts are reported for each request in the <code>ImpactsOutput</code> that features multiple impact criteria such as energy consumption or the global warming potential per phase (usage or embodied) as well as the total impacts. It also contains potential warnings and errors that can occur during the calculation.</p> <p>To learn more on how we estimate the environmental impacts and what are our hypotheses go to the methodology section.</p>"},{"location":"tutorial/impacts/#impacts-output","title":"Impacts Output","text":"<p>The <code>ImpactsOutput</code> is structured the following way:</p> <pre><code>from ecologits.tracers.utils import ImpactsOutput\nfrom ecologits.impacts.modeling import ADPe, Embodied, Energy, GWP, PE, Usage\n\n\nImpactsOutput(\n    energy=Energy(),    # Total energy consumed (electricity)\n    gwp=GWP(),          # Total global warming potential (or GHG emissions)\n    adpe=ADPe(),        # Total abiotic resource depletion\n    pe=PE(),            # Total energy consumed from primary sources\n    usage=Usage( # (1)!\n        energy=Energy(),\n        gwp=GWP(),\n        adpe=ADPe(),\n        pe=PE(),\n    ),\n    embodied=Embodied( # (2)!\n        gwp=GWP(),\n        adpe=ADPe(),\n        pe=PE(),\n    ),\n    warnings=None, # (3)!\n    errors=None\n)\n</code></pre> <ol> <li>Usage impacts for the electricity consumption impacts. Note that the energy is equal to the \"total\" energy impact.</li> <li>Embodied impacts for resource extract, manufacturing and transportation of hardware components allocated to the request. </li> <li>List of <code>WarningMessage</code> and <code>ErrorMessage</code>.</li> </ol>"},{"location":"tutorial/impacts/#example-of-an-impact-value","title":"Example of an Impact Value","text":"<p>The impact objects named <code>Energy</code>, <code>GWP</code>, <code>ADPe</code> or <code>PE</code> all share the following structure:</p> <pre><code>from ecologits.impacts.modeling import BaseImpact\nfrom ecologits.utils.range_value import RangeValue\n\nclass GWP(BaseImpact):  # (1)!\n    type: str = \"GWP\"\n    name: str = \"Global Warming Potential\"\n    unit: str = \"kgCO2eq\"\n    value: float | RangeValue = 0.34 \n</code></pre> <p>You can retrieve the GWP impact value from a request with the following:</p> <pre><code>&gt;&gt;&gt; response.impacts.gwp.value #  (1)!\n0.34    # Total GHG emissions in kgCO2eq.\n\n&gt;&gt;&gt; response.impacts.usage.gwp.value \n0.23    # or for the usage phase only (2)\n</code></pre> <ol> <li>Assuming you have made an inference and get the response in an <code>response</code> object.</li> <li>Impacts of the usage phase corresponds to the impacts of electricity consumption. Learn more about usage phase in the next section.</li> </ol>"},{"location":"tutorial/impacts/#example-with-a-rangevalue","title":"Example with a <code>RangeValue</code>","text":"<p>Impact values can also be represented as intervals with the <code>RangeValue</code> object. They are used to give an estimate range of possible values between a <code>min</code> and a <code>max</code>. </p> <p>About <code>RangeValue</code> intervals</p> <p>This range of values corresponds a high-confidence approximation interval, within which we are confident enough that the true consumption lies. This interval is defined by several approximations, such as the model size (if unknown) and the statistical regressions that we perform for estimating quantities. For more information, see the methodology.</p> <p>Example of an impact with a <code>RangeValue</code>:</p> <pre><code>&gt;&gt;&gt; response.impacts.gwp.value\nRangeValue(min=0.16, max=0.48) # in kgCO2eq (1)\n</code></pre> <ol> <li><code>RangeValue</code> are used to define intervals. It corresponds to the 95% confidence interval of our approximation.</li> </ol>"},{"location":"tutorial/impacts/#impact-criteria","title":"Impact Criteria","text":"<p>To evaluate the impact of human activities on the planet or on the climate we use criteria that usually focus on a specific issue such as GHG emissions for global warming, water consumption and pollution or the depletion of natural resources. We currently support three environmental impact criteria in addition with the direct energy consumption. </p> <p>Monitoring multiple criteria is useful to avoid pollution shifting, which is defined as the transfer of pollution from one medium to another. It is a common pitfall to optimize only one criterion like GHG emissions (e.g. buying new hardware that is more energy efficient), that can lead to higher impacts on minerals and metals depletion for example (see encyclopedia.com ).</p>"},{"location":"tutorial/impacts/#energy","title":"Energy","text":"<p>The <code>Energy</code> criterion refers to the direct electricity consumption of GPUs, server and other equipments from the data center. As defined the energy criteria is not an environmental impact, but it is used to estimate other impacts in the usage phase. This criterion is expressed in kilowatt-hour (kWh).</p> Energy model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>energy</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Energy</p> </li> <li> <code>value</code>               (<code>ValueOrRange</code>)           \u2013            <p>Energy value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilowatt-hour (kWh)</p> </li> </ul>"},{"location":"tutorial/impacts/#global-warming-potential-gwp","title":"Global Warming Potential (GWP)","text":"<p>The Global Warming Potential (<code>GWP</code>) criterion is an index measuring how much heat is absorbed by greenhouse gases in the atmosphere compared to carbon dioxide. This criterion is expressed in kilogram of carbon dioxide equivalent (kgCO2eq).</p> <p>Learn more: wikipedia.org </p> GWP model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>GWP</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Global Warming Potential</p> </li> <li> <code>value</code>               (<code>ValueOrRange</code>)           \u2013            <p>GWP value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilogram Carbon Dioxide Equivalent (kgCO2eq)</p> </li> </ul>"},{"location":"tutorial/impacts/#abiotic-depletion-potential-for-elements-adpe","title":"Abiotic Depletion Potential for Elements (ADPe)","text":"<p>The Abiotic Depletion Potential \u2013 elements (<code>ADPe</code>) criterion represents the reduction of non-renewable and non-living (abiotic) resources such as metals and minerals. This criterion is expressed in kilogram of antimony equivalent (kgSbeq).</p> <p>Learn more: sciencedirect.com </p> ADPe model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>ADPe</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Abiotic Depletion Potential (elements)</p> </li> <li> <code>value</code>               (<code>ValueOrRange</code>)           \u2013            <p>ADPe value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Kilogram Antimony Equivalent (kgSbeq)</p> </li> </ul>"},{"location":"tutorial/impacts/#primary-energy-pe","title":"Primary Energy (PE)","text":"<p>The Primary Energy (<code>PE</code>) criterion represents the amount of energy consumed from natural sources such as raw fuels and other forms of energy, including waste. This criterion is expressed in megajoule (MJ). </p> <p>Learn more: wikipedia.org </p> PE model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>PE</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Primary Energy</p> </li> <li> <code>value</code>               (<code>ValueOrRange</code>)           \u2013            <p>PE value</p> </li> <li> <code>unit</code>               (<code>str</code>)           \u2013            <p>Megajoule (MJ)</p> </li> </ul>"},{"location":"tutorial/impacts/#impact-phases","title":"Impact Phases","text":"<p>Inspired from the Life Cycle Assessment methodology we classify impacts is two phases (usage and embodied). The usage phase is about the environmental impacts related to the energy consumption while using an AI model. The embodied phase encompasses upstream impacts such as resource extraction, manufacturing, and transportation. We currently do not support the third phase which is end-of-life due to a lack of open research and transparency on that matter.</p> <p>Learn more: wikipedia.org </p> <p>Another pitfall in environmental impact assessment is to only look at the usage phase and ignore upstream and downstream impacts. This can lead to higher overall impacts on the entire life cycle. If you replace old hardware by newer that is more energy efficient, you will get a reduction of impacts on the usage phase, but it will increase the upstream impacts as well.  </p>"},{"location":"tutorial/impacts/#usage","title":"Usage","text":"<p>The <code>Usage</code> phase accounts for the environmental impacts while using AI models. We report all criteria in addition to direct energy consumption for this phase. </p> <p>Note that we use the worldwide average electricity mix impact factor by default. </p> Usage model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>usage</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Usage</p> </li> <li> <code>energy</code>               (<code>Energy</code>)           \u2013            <p>Energy consumption</p> </li> <li> <code>gwp</code>               (<code>GWP</code>)           \u2013            <p>Global Warming Potential (GWP) usage impact</p> </li> <li> <code>adpe</code>               (<code>ADPe</code>)           \u2013            <p>Abiotic Depletion Potential for Elements (ADPe) usage impact</p> </li> <li> <code>pe</code>               (<code>PE</code>)           \u2013            <p>Primary Energy (PE) usage impact</p> </li> </ul>"},{"location":"tutorial/impacts/#embodied","title":"Embodied","text":"<p>The Embodied phase accounts for the upstream environmental impacts such as resource extraction, manufacturing and transportation allocated to the request. We report all criteria (excluding energy consumption) for this phase.</p> Embodied model attributes <p>Attributes:</p> <ul> <li> <code>type</code>               (<code>str</code>)           \u2013            <p>embodied</p> </li> <li> <code>name</code>               (<code>str</code>)           \u2013            <p>Embodied</p> </li> <li> <code>gwp</code>               (<code>GWP</code>)           \u2013            <p>Global Warming Potential (GWP) embodied impact</p> </li> <li> <code>adpe</code>               (<code>ADPe</code>)           \u2013            <p>Abiotic Depletion Potential for Elements (ADPe) embodied impact</p> </li> <li> <code>pe</code>               (<code>PE</code>)           \u2013            <p>Primary Energy (PE) embodied impact</p> </li> </ul>"},{"location":"tutorial/impacts/#impact-factors","title":"Impact Factors","text":"<p>We use impact factors to quantify environmental harm from human activities, measuring the ratio of greenhouse gases, resource consumption, and other criteria resulting from activities like energy consumption, industrial processes, transportation, waster management and more.</p>"},{"location":"tutorial/impacts/#electricity-mix","title":"Electricity Mix","text":"<p>When initializing <code>EcoLogits</code>, you can choose a specific electricity mix zone from the ADEME Base Empreinte\u00ae  database.</p> Select a different electricity mix<pre><code>from ecologits import EcoLogits\n\n# Select the electricity mix of France\nEcoLogits.init(electricity_mix_zone=\"FRA\")\n</code></pre> <p>By default, the <code>WOR</code> World electricity mix is used, whose values are:</p> Impact criteria Value Unit GWP \\(5.904e-1\\) kgCO2eq / kWh ADPe \\(7.378e-7\\) kgSbeq / kWh PE \\(9.988\\) MJ / kWh"},{"location":"tutorial/providers/","title":"Supported providers","text":""},{"location":"tutorial/providers/#list-of-all-providers","title":"List of all providers","text":"Provider name Extra for installation Guide Anthropic <code>anthropic</code> Guide for Anthropic  Cohere <code>cohere</code> Guide for Cohere  Google Gemini <code>google-generativeai</code> Guide for Google Gemini  Hugging Face Hub <code>huggingface-hub</code> Guide for Hugging Face Hub  LiteLLM <code>litellm</code> Guide for LiteLLM  Mistral AI <code>mistralai</code> Guide for Mistral AI  OpenAI <code>openai</code> Guide for OpenAI  Azure OpenAI <code>openai</code> Guide for Azure OpenAI"},{"location":"tutorial/providers/#chat-completions","title":"Chat Completions","text":"Provider Completions Completions (stream) Completions (async) Completions (async + stream) Anthropic Cohere Google Gemini HuggingFace Hub LiteLLM Mistral AI OpenAI Azure OpenAI <p>Partial support for Anthropic streams, see full documentation: Anthropic provider.</p>"},{"location":"tutorial/warnings_and_errors/","title":"Warnings and Errors","text":"<p>EcoLogits may encounter situations where the calculation of environmental impacts has high risk of inaccuracies or uncertainties (reported as warnings), or where the calculation fails due to certain reasons like misconfiguration (reported as errors).</p> <p>Warnings and errors are reported in the <code>ImpactsOutput</code> pydantic model within the <code>warnings</code> and <code>errors</code> fields respectively. Each warning or error contains a <code>code</code> (all listed below) and a <code>message</code> explaining the issue.</p> <p>Silent reporting of warnings and errors</p> <p>By default, warnings and errors are reported silently. This means you won't see any warning logged or exception raised. This approach ensures your program continues to execute and avoids spamming the log output, especially when executing many requests.</p> <p>Code example on how to determine if your request resulted in any warnings or errors and how to retrieve them.</p> <pre><code>from ecologits import EcoLogits\n\nEcoLogits.init()\n\nresponse = ...  # Request code goes here\n\nif response.impacts.has_warnings:\n    for w in response.impacts.warnings:\n        print(w)\n\nif response.impacts.has_errors:\n    for e in response.impacts.errors:\n        print(e)\n</code></pre>"},{"location":"tutorial/warnings_and_errors/#warnings","title":"Warnings","text":"<p>List of all the warnings that EcoLogits can report. </p>"},{"location":"tutorial/warnings_and_errors/#model-arch-not-released","title":"<code>model-arch-not-released</code>","text":"<p>This warning is reported when the model architecture is not disclosed by the provider. Thus, the estimation of environmental impacts is based on a assumption of the model architecture (e.g. dense or mixture of experts, number of parameters).</p>"},{"location":"tutorial/warnings_and_errors/#model-arch-multimodal","title":"<code>model-arch-multimodal</code>","text":"<p>This warning is reported when the model is multimodal. EcoLogits uses energy benchmarking data from open source LLMs that can only generate text. Models that can generate (or use as input) data from other modalities such as image, audio or video are currently not fully supported.</p>"},{"location":"tutorial/warnings_and_errors/#errors","title":"Errors","text":"<p>List of all the errors that EcoLogits can report.</p>"},{"location":"tutorial/warnings_and_errors/#model-not-registered","title":"<code>model-not-registered</code>","text":"<p>This error is reported when the selected model is not registered. This can happen when the model has been released recently or if you are using a custom model (such as fine-tuned models). In the first case you can try updating EcoLogits to the latest version, if the error persists, you can open up an issue .</p>"},{"location":"tutorial/warnings_and_errors/#zone-not-registered","title":"<code>zone-not-registered</code>","text":"<p>This error is reported when the selected geographical zone is not registered. This can happen if the configured zone does not exist or if the custom zone is not properly registered.</p>"},{"location":"tutorial/providers/anthropic/","title":"Anthropic","text":"<p>Lack of transparency</p> <p>Anthropic does not disclose any information about model architecture and inference infrastrucure. Thus, the environmental impacts are estimated with a very low precision.</p> <p>This guide focuses on the integration of  EcoLogits with the Anthropic official python client .</p> <p>Official links:</p> <ul> <li>Repository:  anthropics/anthropic-sdk-python</li> <li>Documentation:  docs.anthropic.com</li> </ul>"},{"location":"tutorial/providers/anthropic/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Anthropic client, please use the <code>anthropic</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[anthropic]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Anthropic's Python client.</p>"},{"location":"tutorial/providers/anthropic/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/anthropic/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from anthropic import Anthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Anthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nresponse = client.messages.create(\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    model=\"claude-3-haiku-20240307\",\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom anthropic import AsyncAnthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncAnthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.messages.create(\n        max_tokens=100,\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        model=\"claude-3-haiku-20240307\",\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/anthropic/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated in the last chunk for the entire request.</p> SyncAsync <pre><code>from anthropic import Anthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Anthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nwith client.messages.stream(\n    max_tokens=100,\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    model=\"claude-3-haiku-20240307\",\n) as stream:\n    for text in stream.text_stream:\n        pass\n    # Get estimated environmental impacts of the inference\n    print(stream.impacts)\n</code></pre> <pre><code>import asyncio\nfrom anthropic import AsyncAnthropic\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncAnthropic(api_key=\"&lt;ANTHROPIC_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    async with client.messages.stream(\n        max_tokens=100,\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        model=\"claude-3-haiku-20240307\",\n    ) as stream:\n        async for text in stream.text_stream:\n            pass\n        # Get estimated environmental impacts of the inference\n        print(stream.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/cohere/","title":"Cohere","text":"<p>Lack of transparency</p> <p>Majority of models released by Cohere are open-weights, but there is no information on the inference infrastructure. Thus, the environmental impacts are estimated with a lower precision.</p> <p>This guide focuses on the integration of  EcoLogits with the Cohere official python client .</p> <p>Official links:</p> <ul> <li>Repository:  mistralai/client-python</li> <li>Documentation:  docs.cohere.com</li> </ul>"},{"location":"tutorial/providers/cohere/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Cohere client, please use the <code>cohere</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[cohere]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Cohere's Python client.</p>"},{"location":"tutorial/providers/cohere/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/cohere/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from cohere import Client\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Client(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nresponse = client.chat(\n    message=\"Tell me a funny joke!\", \n    max_tokens=100\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom cohere import AsyncClient\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncClient(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat(\n        message=\"Tell me a funny joke!\", \n        max_tokens=100\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/cohere/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated in the last chunk for the entire request.</p> SyncAsync <pre><code>from cohere import Client\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Client(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nstream = client.chat_stream(\n    message=\"Tell me a funny joke!\", \n    max_tokens=100\n)\n\nfor chunk in stream:\n    if chunk.event_type == \"stream-end\":\n        # Get estimated environmental impacts of the inference\n        print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom cohere import AsyncClient\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncClient(api_key=\"&lt;COHERE_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    stream = await client.chat_stream(\n        message=\"Tell me a funny joke!\", \n        max_tokens=100\n    )\n\n    async for chunk in stream:\n        if chunk.event_type == \"stream-end\":\n            # Get estimated environmental impacts of the inference\n            print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/google/","title":"Google Gemini","text":"<p>Lack of transparency</p> <p>Google does not disclose any information about model architecture and inference infrastrucure. Thus, the environmental impacts are estimated with a very low precision.</p> <p>This guide focuses on the integration of  EcoLogits with the Google Gemini official python client .</p> <p>Official links:</p> <ul> <li>Repository:  google-gemini/generative-ai-python</li> <li>Documentation:  ai.google.dev</li> </ul>"},{"location":"tutorial/providers/google/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Google Gemini client, please use the <code>google-generativeai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[google-generativeai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Google Gemini Python client.</p>"},{"location":"tutorial/providers/google/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/google/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini\ngenai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\nresponse = model.generate_content(\"Write a story about a magic backpack.\")\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in async mode\nasync def main() -&gt; None:\n    genai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    response = await model.generate_content_async(\n        \"Write a story about a magic backpack.\"\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/google/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in streaming mode\ngenai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\nmodel = genai.GenerativeModel(\"gemini-1.5-flash\")\nstream = model.generate_content(\n    \"Write a story about a magic backpack.\", \n    stream=True\n)\n\n# Get cumulative estimated environmental impacts of the inference\nfor chunk in stream:\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nimport google.generativeai as genai\n\n# Initialize EcoLogits\nEcoLogits.init()\n\n# Ask something to Google Gemini in streaming and async mode\nasync def main() -&gt; None:\n    genai.configure(api_key=\"&lt;GOOGLE_API_KEY&gt;\")\n    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n    stream = await model.generate_content_async(\n        \"Write a story about a magic backpack.\", \n        stream=True\n    )\n\n    # Get cumulative estimated environmental impacts of the inference\n    async for chunk in stream:\n        print(chunk.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/huggingface_hub/","title":"Hugging Face Hub","text":"<p>Lack of transparency</p> <p>All models available on Hugging Face inference endpoints are open-weights, but there is not information on the inference infrastructure. Thus, the environmental impacts are estimated with a lower precision.</p> <p>This guide focuses on the integration of  EcoLogits with the Hugging Face Hub official python client .</p> <p>Official links:</p> <ul> <li>Repository:  huggingface/huggingface_hub</li> <li>Documentation:  huggingface.co</li> </ul>"},{"location":"tutorial/providers/huggingface_hub/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Hugging Face Hub client, please use the <code>huggingface-hub</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[huggingface-hub]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Hugging Face Hub's Python client.</p>"},{"location":"tutorial/providers/huggingface_hub/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/huggingface_hub/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom huggingface_hub import InferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\nresponse = client.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    max_tokens=15\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom huggingface_hub import AsyncInferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncInferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\n\nasync def main() -&gt; None:\n    response = await client.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        max_tokens=15\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/huggingface_hub/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom huggingface_hub import InferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = InferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\nstream = client.chat_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n    max_tokens=15,\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom huggingface_hub import AsyncInferenceClient\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncInferenceClient(model=\"HuggingFaceH4/zephyr-7b-beta\")\n\nasync def main() -&gt; None:\n    stream = await client.chat_completion(\n        messages=[{\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}],\n        max_tokens=15,\n        stream=True\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/litellm/","title":"LiteLLM","text":"<p>Lack of transparency</p> <p>Depending on the provider, the level of transparency varies. Thus, the environmental impacts can be estimated with a low precision.</p> <p>This guide focuses on the integration of  EcoLogits with the LiteLLM official Python client .</p> <p>Official links:</p> <ul> <li>Repository:  BerriAI/litellm</li> <li>Documentation:  litellm.vercel.app</li> </ul>"},{"location":"tutorial/providers/litellm/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with LiteLLM, please use the <code>litellm</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[litellm]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with LiteLLM's Python client.</p>"},{"location":"tutorial/providers/litellm/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/litellm/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data. Make sure you have the api key of the provider used in an .env file. Make sure you call the litellm generation function as \"litellm.completion\" and not just \"completion\".</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport litellm\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nresponse = litellm.completion(\n    model=\"gpt-4o-2024-05-13\",\n    messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nimport litellm\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nasync def main() -&gt; None:\n    response = await litellm.acompletion(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/litellm/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nimport litellm\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nstream = litellm.completion(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello World!\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nimport litellm\nfrom ecologits import EcoLogits\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nasync def main() -&gt; None:\n    stream = await litellm.acompletion(\n        model=\"gpt-3.5-turbo\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/mistralai/","title":"Mistral AI","text":"Deprecation of Mistral AI v0 <p>Mistral AI python client with version <code>&lt;1.0.0</code> will no longer be supported by EcoLogits. See official migration guide from v0 to v1 . </p> <p>Lack of transparency</p> <p>Some models released by Mistral AI are not open-weights, plus there is no information on the inference infrastructure. Thus, the environmental impacts are estimated with a lower precision.</p> <p>This guide focuses on the integration of  EcoLogits with the Mistral AI official python client .</p> <p>Official links:</p> <ul> <li>Repository:  mistralai/client-python</li> <li>Documentation:  docs.mistral.ai</li> </ul>"},{"location":"tutorial/providers/mistralai/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the Mistral AI client, please use the <code>mistralai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[mistralai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with Mistral AI's Python client.</p>"},{"location":"tutorial/providers/mistralai/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/mistralai/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom mistralai import Mistral\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Mistral(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nresponse = client.chat.complete(# (1)! \n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ],\n    model=\"mistral-tiny\"\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <ol> <li>Use <code>client.chat</code> for Mistral AI v0.</li> </ol> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom mistralai import Mistral\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Mistral(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat.complete_async(# (1)! \n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ],\n        model=\"mistral-tiny\"\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre> <ol> <li>Use <code>client.chat</code> for Mistral AI v0.</li> </ol>"},{"location":"tutorial/providers/mistralai/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom mistralai import Mistral\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Mistral(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nstream = client.chat.stream(# (1)! \n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ],\n    model=\"mistral-tiny\"\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.data.impacts) # (2)!  \n</code></pre> <ol> <li>Use <code>client.chat_stream</code> for Mistral AI v0.</li> <li>Use <code>chunk.impacts</code> for Mistral AI v0.</li> </ol> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom mistralai import Mistral\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = Mistral(api_key=\"&lt;MISTRAL_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat.stream_async(# (1)! \n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ],\n        model=\"mistral-tiny\"\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        if hasattr(chunk, \"impacts\"):\n            print(chunk.data.impacts) # (2)!  \n\n\nasyncio.run(main())\n</code></pre> <ol> <li>Use <code>client.chat_stream</code> for Mistral AI v0.</li> <li>Use <code>chunk.impacts</code> for Mistral AI v0.</li> </ol>"},{"location":"tutorial/providers/openai/","title":"OpenAI","text":"<p>Lack of transparency</p> <p>OpenAI does not disclose any information about model architecture and inference infrastrucure. Thus, the environmental impacts are estimated with a very low precision.</p> <p>This guide focuses on the integration of  EcoLogits with the OpenAI official python client .</p> <p>Official links:</p> <ul> <li>Repository:  openai/openai-python</li> <li>Documentation:  platform.openai.com</li> </ul>"},{"location":"tutorial/providers/openai/#installation","title":"Installation","text":"<p>To install EcoLogits along with all necessary dependencies for compatibility with the OpenAI client, please use the <code>openai</code> extra-dependency option as follows:</p> <pre><code>pip install ecologits[openai]\n</code></pre> <p>This installation command ensures that EcoLogits is set up with the specific libraries required to interface seamlessly with OpenAI's Python client.</p>"},{"location":"tutorial/providers/openai/#chat-completions","title":"Chat Completions","text":""},{"location":"tutorial/providers/openai/#example","title":"Example","text":"<p>Integrating EcoLogits with your applications does not alter the standard outputs from the API responses. Instead, it enriches them by adding the <code>Impacts</code> object, which contains detailed environmental impact data.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom openai import AsyncOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncOpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    response = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    # Get estimated environmental impacts of the inference\n    print(response.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/openai/#streaming-example","title":"Streaming example","text":"<p>In streaming mode, the impacts are calculated incrementally, which means you don't need to sum the impacts from each data chunk. Instead, the impact information in the last chunk reflects the total cumulative environmental impacts for the entire request.</p> SyncAsync <pre><code>from ecologits import EcoLogits\nfrom openai import OpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = OpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nstream = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello World!\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    # Get cumulative estimated environmental impacts of the inference\n    print(chunk.impacts)\n</code></pre> <pre><code>import asyncio\nfrom ecologits import EcoLogits\nfrom openai import AsyncOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AsyncOpenAI(api_key=\"&lt;OPENAI_API_KEY&gt;\")\n\nasync def main() -&gt; None:\n    stream = await client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n        ]\n    )\n\n    async for chunk in stream:\n        # Get cumulative estimated environmental impacts of the inference\n        print(chunk.impacts)\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"tutorial/providers/openai/#compatibility-with-azure-openai","title":"Compatibility with Azure OpenAI","text":"<p>EcoLogits is also compatible with Azure OpenAI .</p> <pre><code>import os\nfrom ecologits import EcoLogits\nfrom openai import AzureOpenAI\n\n# Initialize EcoLogits\nEcoLogits.init()\n\nclient = AzureOpenAI(\n    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n    api_version=\"2024-02-01\"\n)\n\n\nresponse = client.chat.completions.create(\n    model=\"gpt-35-turbo\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"Tell me a funny joke!\"}\n    ]\n)\n\n# Get estimated environmental impacts of the inference\nprint(response.impacts)\n</code></pre>"}]}