
@misc{luccioni_estimating_2022,
	title = {Estimating the {Carbon} {Footprint} of {BLOOM}, a {176B} {Parameter} {Language} {Model}},
	url = {http://arxiv.org/abs/2211.02001},
	doi = {10.48550/arXiv.2211.02001},
	abstract = {Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of{\textasciitilde}{\textbackslash}carboneq{\textasciitilde}if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Luccioni, Alexandra Sasha and Viguier, Sylvain and Ligozat, Anne-Laure},
	month = nov,
	year = {2022},
	note = {arXiv:2211.02001 [cs]},
	keywords = {Inference},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/F628NNAM/Luccioni et al. - 2022 - Estimating the Carbon Footprint of BLOOM, a 176B P.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/VANCGWYG/2211.html:text/html},
}

@article{de_vries_growing_2023,
	title = {The growing energy footprint of artificial intelligence},
	volume = {7},
	issn = {2542-4351},
	url = {https://www.sciencedirect.com/science/article/pii/S2542435123003653},
	doi = {10.1016/j.joule.2023.09.004},
	abstract = {Alex de Vries is a PhD candidate at the VU Amsterdam School of Business and Economics and the founder of Digiconomist, a research company dedicated to exposing the unintended consequences of digital trends. His research focuses on the environmental impact of emerging technologies and has played a major role in the global discussion regarding the sustainability of blockchain technology.},
	number = {10},
	urldate = {2024-07-17},
	journal = {Joule},
	author = {de Vries, Alex},
	month = oct,
	year = {2023},
	keywords = {Inference},
	pages = {2191--2194},
	file = {ScienceDirect Snapshot:/Users/perso/Zotero/storage/KRKW8RAL/S2542435123003653.html:text/html},
}

@inproceedings{luccioni_power_2024,
	title = {Power {Hungry} {Processing}: {Watts} {Driving} the {Cost} of {AI} {Deployment}?},
	shorttitle = {Power {Hungry} {Processing}},
	url = {http://arxiv.org/abs/2311.16863},
	doi = {10.1145/3630106.3658542},
	abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ``generality'' comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
	urldate = {2024-07-17},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
	month = jun,
	year = {2024},
	note = {arXiv:2311.16863 [cs]},
	keywords = {Inference},
	pages = {85--99},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/B9944C6A/Luccioni et al. - 2024 - Power Hungry Processing Watts Driving the Cost of.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/J23YEXW7/2311.html:text/html},
}

@article{desislavov_trends_2023,
	title = {Trends in {AI} inference energy consumption: {Beyond} the performance-vs-parameter laws of deep learning},
	volume = {38},
	issn = {2210-5379},
	shorttitle = {Trends in {AI} inference energy consumption},
	url = {https://www.sciencedirect.com/science/article/pii/S2210537923000124},
	doi = {10.1016/j.suscom.2023.100857},
	abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we analyse relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.},
	urldate = {2024-07-17},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Desislavov, Radosvet and Martínez-Plumed, Fernando and Hernández-Orallo, José},
	month = apr,
	year = {2023},
	keywords = {Inference},
	pages = {100857},
	file = {Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:/Users/perso/Zotero/storage/4EPVTXFM/Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:application/pdf},
}

@misc{patterson_carbon_2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {arXiv:2104.10350 [cs]},
	keywords = {Inference},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/6EZCUJAV/Patterson et al. - 2021 - Carbon Emissions and Large Neural Network Training.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/4UTMCBQA/2104.html:text/html},
}

@misc{lacoste_quantifying_2019,
	title = {Quantifying the {Carbon} {Emissions} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1910.09700},
	doi = {10.48550/arXiv.1910.09700},
	abstract = {From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
	month = nov,
	year = {2019},
	note = {arXiv:1910.09700 [cs]},
	keywords = {Training},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/78HKZGUS/Lacoste et al. - 2019 - Quantifying the Carbon Emissions of Machine Learni.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/ZX85PY8Z/1910.html:text/html},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Training},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/ZNZJI626/Touvron et al. - 2023 - LLaMA Open and Efficient Foundation Language Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/XT6N23VW/2302.html:text/html},
}

@misc{touvron_llama_2023-1,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Training},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/Q54U947Z/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/GPN3VHXD/2307.html:text/html},
}

@article{llama3modelcard_2024,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}


@inproceedings{luccioni_power_2024,
	title = {Power {Hungry} {Processing}: {Watts} {Driving} the {Cost} of {AI} {Deployment}?},
	shorttitle = {Power {Hungry} {Processing}},
	url = {http://arxiv.org/abs/2311.16863},
	doi = {10.1145/3630106.3658542},
	abstract = {Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of ``generality'' comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.},
	urldate = {2024-07-17},
	booktitle = {The 2024 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Luccioni, Alexandra Sasha and Jernite, Yacine and Strubell, Emma},
	month = jun,
	year = {2024},
	note = {arXiv:2311.16863 [cs]},
	keywords = {Inference},
	pages = {85--99},
	file = {arXiv Fulltext PDF:/Users/perso/Zotero/storage/B9944C6A/Luccioni et al. - 2024 - Power Hungry Processing Watts Driving the Cost of.pdf:application/pdf;arXiv.org Snapshot:/Users/perso/Zotero/storage/J23YEXW7/2311.html:text/html},
}

@article{ligozat_unraveling_2022,
	author = {Ligozat, Anne-Laure and Lefevre, Julien and Bugeau, Aurélie and Combaz, Jacques},
	title = {Unraveling the Hidden Environmental Impacts of AI Solutions for Environment Life Cycle Assessment of AI Solutions},
	journal = {Sustainability},
	volume = {14},
	year = {2022},
	number = {9},
	article-number = {5172},
	url = {https://www.mdpi.com/2071-1050/14/9/5172},
	issn = {2071-1050},
	abstract = {In the past ten years, artificial intelligence has encountered such dramatic progress that it is now seen as a tool of choice to solve environmental issues and, in the first place, greenhouse gas emissions (GHG). At the same time, the deep learning community began to realize that training models with more and more parameters require a lot of energy and, as a consequence, GHG emissions. To our knowledge, questioning the complete net environmental impacts of AI solutions for the environment (AI for Green) and not only GHG, has never been addressed directly. In this article, we propose to study the possible negative impacts of AI for Green. First, we review the different types of AI impacts; then, we present the different methodologies used to assess those impacts and show how to apply life cycle assessment to AI services. Finally, we discuss how to assess the environmental usefulness of a general AI service and point out the limitations of existing work in AI for Green.},
	doi = {10.3390/su14095172}
}

@software{codecarbon_2024,
  author       = {Benoit Courty and
                  Victor Schmidt and
                  Sasha Luccioni and
                  Goyal-Kamal and
                  MarionCoutarel and
                  Boris Feld and
                  Jérémy Lecourt and
                  LiamConnell and
                  Amine Saboni and
                  Inimaz and
                  supatomic and
                  Mathilde Léval and
                  Luis Blanche and
                  Alexis Cruveiller and
                  ouminasara and
                  Franklin Zhao and
                  Aditya Joshi and
                  Alexis Bogroff and
                  Hugues de Lavoreille and
                  Niko Laskaris and
                  Edoardo Abati and
                  Douglas Blank and
                  Ziyao Wang and
                  Armin Catovic and
                  Marc Alencon and
                  Michał Stęchły and
                  Christian Bauer and
                  Lucas-Otavio and
                  JPW and
                  MinervaBooks},
  title        = {mlco2/codecarbon: v2.4.1},
  month        = may,
  year         = 2024,
  publisher    = {Zenodo},
  version      = {v2.4.1},
  doi          = {10.5281/zenodo.11171501},
  url          = {https://doi.org/10.5281/zenodo.11171501}
}

@inproceedings{zeus_2023,
    title     = {Zeus: Understanding and Optimizing {GPU} Energy Consumption of {DNN} Training},
    author    = {Jie You and Jae-Won Chung and Mosharaf Chowdhury},
    booktitle = {USENIX NSDI},
    year      = {2023}
}

@misc{llm-perf-leaderboard,
  author = {Ilyas Moutawwakil, Régis Pierrard},
  title = {LLM-Perf Leaderboard},
  year = {2023},
  publisher = {Hugging Face},
  howpublished = "\url{https://huggingface.co/spaces/optimum/llm-perf-leaderboard}",
}

@software{optimum-benchmark,
  author = {Ilyas Moutawwakil, Régis Pierrard},
  publisher = {Hugging Face},
  year = {2023},
  title = {Optimum-Benchmark: A framework for benchmarking the performance of Transformers models with different hardwares, backends and optimizations.},
}

@inproceedings{boaviztapi,
  title = {{BoaviztAPI: a bottom-up model to assess the environmental impacts of cloud services}},
  author = {Simon, Thibault and Ekchajzer, David and Berthelot, Adrien and Fourboul, Eric and Rince, Samuel and Rouvoy, Romain},
  url = {https://hal.science/hal-04621947},
  booktitle = {{HotCarbon'24 - 3rd Workshop on Sustainable Computer Systems}},
  address = {Santa Cruz, United States},
  year = {2024},
  month = Jul,
  keywords = {Life Cycle Assessment ; Bottom-up ; Multicriteria ; Open-data ; Cloud ; Environmental Impact},
  pdf = {https://hal.science/hal-04621947v2/file/HotCarbon_hal.pdf},
  hal_id = {hal-04621947},
  hal_version = {v2},
}

@article{klopffer_life_1997,
  title        = {Life cycle assessment},
  volume       = {4},
  issn         = {1614-7499},
  OPTurl          = {https://doi.org/10.1007/BF02986351},
  doi          = {10.1007/BF02986351},
  abstract     = {The basic idea of {LCA} is that all environmental burdens connected with a product or service have to be assessed, back to the raw materials and down to waste removal. Therefore, the term “Life Cycle Assessment” is more precise than the German “Ökobilanz” or the French “écobilan”. This basic idea is undoubtedly true, and {LCA} is the only environmental assessment tool which avoids positive ratings for measurements which only consists in the shifting of burdens.},
  pages        = {223--228},
  number       = {4},
  journal      = {Environmental Science and Pollution Research},
  shortjournal = {Environ. Sci. \& Pollut. Res.},
  author       = {Klöpffer, Walter},
  urldate      = {2024-03-13},
  year         = {1997},
  langid       = {english},
  keywords     = {Club of Rome, Code of Practice, environmental assessment, goal definition and scoping, history, {LCA}, impact assessment, improvement assessment, interpretation assessment, inventory analysis, Inventory Table, {ISOLP}, {LCA}, Life Cycle Assessment, {SETAC}, {SPOLD}}
}

@misc{bordage_digital_2021,
  title  = {{Digital technologies in Europe: an environmental life cycle approach}},
  author = {Bordage, F. and de Montenay, L. and Benqassem, S. and Delmas-Orgelet, J. and Domon, F. and Prunel, D. and Vateau, C. and Lees Perasso, E.},
  year   = 2021,
  url    = {https://www.greenit.fr/wp-content/uploads/2021/12/EU-Study-LCA-7-DEC-EN.pdf},
  note   = {[Online; accessed 18. Mar. 2024]}
}

@book{forti_global_2020,
  title = {The Global E-waste Monitor 2020. Quantities, flows, and the circular economy potential},
  isbn = {978-92-808-9114-0},
  author    = {Forti, Vanessa and Baldé, Cornelis and Kuehr, Ruediger and Bel, Garam},
  year = {2020},
  publisher = {United Nations University (UNU)/United Nations Institute for Training and Research (UNITAR) – co-hosted SCYCLE Programme, International Telecommunication Union (ITU) \& International Solid Waste Association (ISWA)}
}

@techreport{ipcc_2013,
  key={IPCC:13},
  title={Climate Change 2013: The Physical Science Basis},
  author={Stocker, Thomas F and Qin, Dahe and Plattner, Gian-Kasper and Tignor, M and Allen, Simon K and Boschung, Judith and Nauels, Alexander and Xia, Yu and Bex, Vincent and Midgley, Pauline M},
  institution={Intergovernmental Panel on Climate Change, Working Group I Contribution to the IPCC Fifth Assessment Report (AR5)(Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA.)},
  year={2013},
  note={1535 pp.}
}

@article{van_oers_abiotic_2020,
  title		= {Abiotic resource depletion potentials ({ADPs}) for elements revisited—updating ultimate reserve estimates and introducing time series for production data},
  volume	= {25},
  issn		= {1614-7502},
  doi		= {10.1007/s11367-019-01683-x},
  pages		= {294--308},
  number	= {2},
  journaltitle	= {The International Journal of Life Cycle Assessment},
  journal	= {The International Journal of Life Cycle Assessment},
  shortjournal	= {Int J Life Cycle Assess},
  author	= {van Oers, Lauran and Guinée, Jeroen B. and Heijungs,
		  Reinout},
  urldate	= {2023-12-15},
  date		= {2020-02-01},
  year		= 2020,
  langid	= {english},
  keywords	= {{LCA}, Abiotic resource depletion, {ADP}, {LCIA}, Resource  use},
}

@article{frischknecht_cumulative_2015,
  title        = {Cumulative energy demand in {LCA}: the energy harvested approach},
  volume       = {20},
  issn         = {1614-7502},
  OPTurl          = {https://doi.org/10.1007/s11367-015-0897-4},
  doi          = {10.1007/s11367-015-0897-4},
  shorttitle   = {Cumulative energy demand in {LCA}},
  pages        = {957--969},
  number       = {7},
  journal      = {The International Journal of Life Cycle Assessment},
  shortjournal = {Int J Life Cycle Assess},
  author       = {Frischknecht, Rolf and Wyss, Franziska and Büsser Knöpfel, Sybille and Lützkendorf, Thomas and Balouktsi, Maria},
  urldate      = {2024-05-03},
  year         = {2015},
  langid       = {english},
  keywords     = {Characterisation factors, Cumulative energy demand, Energy harvested, Impact category indicator, Life cycle assessment, Life cycle impact assessment}
}

@article{boustead_lca_1996,
  title        = {{LCA} — How it came about: — The beginning in the U.K.},
  volume       = {1},
  issn         = {1614-7502},
  OPTurl          = {https://doi.org/10.1007/BF02978943},
  doi          = {10.1007/BF02978943},
  pages        = {147--150},
  number       = {3},
  journal      = {The International Journal of Life Cycle Assessment},
  shortjournal = {Int. J. {LCA}},
  author       = {Boustead, Ian},
  urldate      = {2021-12-07},
  year         = {1996},
  month        = {September},
  day          = {1},
  langid       = {english},
  keywords     = {{ACV} - {LCA}}
}

@article{hunt_lca_1996,
  title        = {{LCA} — How it came about: — Personal reflections on the origin and the development of {LCA} in the {USA}},
  volume       = {1},
  issn         = {0948-3349, 1614-7502},
  OPTurl          = {http://link.springer.com/10.1007/BF02978624},
  doi          = {10.1007/BF02978624},
  shorttitle   = {{LCA} — How it came about},
  abstract     = {This article is tbe personal reflectionsof the authors on the 25-year history of I.{CA} in the U.S.A.The original study was commissioned by The Coon-{ColaCompany} in 1969. Whilemost {LCAsin} the U.S. have been confidentialstudiesfor privatecompanies,important public studies are described. In the early years, the {LCAswere} generally commissioned by clients who were interested primarily in the solid waste aspects of total manufacturing and use systems,especiallyfor packaging products. The energy and other environmental infi{\textasciitilde}rmation was just a "bonus." In about 1975, tbe interest turned to energy. In 1988, the primary interest returned to solid waste, but was quickly replaced by a more balanced concern about the broad areas of resource use and environmental emissions.This broader interest has sparked the current debates in impact assessment.},
  pages        = {4--7},
  number       = {1},
  journal      = {The International Journal of Life Cycle Assessment},
  shortjournal = {Int. J. {LCA}},
  author       = {Hunt, Robert G. and Franklin, William E. and Hunt, R. G.},
  urldate      = {2021-11-25},
  date         = {1996-03},
  year         = {1996},
  langid       = {english},
  keywords     = {{ACV} - {LCA}}
}

@misc{ademe_base_empreinte,
  title   = {Base Empreinte®},
  author  = {ADEME},
  url     = {https://base-empreinte.ademe.fr/},
  note    = {[Online; accessed 8. Aug. 2024]}
}

@misc{uptimeinstitute_2024,
	author = {Jacqueline Davis, John O'Brien},
	title = {Large data centers are mostly more efficient, analysis confirms},
	howpublished = {\url{https://journal.uptimeinstitute.com/large-data-centers-are-mostly-more-efficient-analysis-confirms/}},
	year = {2024},
	note = {[Accessed 08-08-2024]},
}

@misc{uptimeinstitute_2022,
	author = {Jacqueline Davis, Daniel Bizo, Andy Lawrence, Dr. Owen Rogers, Max Smolaks, Lenny Simon, Douglas Donnellan},
	title = {Uptime Institute Global Data Center Survey Results 2022},
	howpublished = {\url{https://uptimeinstitute.com/resources/research-and-reports/uptime-institute-global-data-center-survey-results-2022}},
	year = {2022},
	note = {[Accessed 09-08-2024]},
}
